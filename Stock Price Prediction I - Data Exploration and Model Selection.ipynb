{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to compose the training and testing datasets from historical data and computed technical analysis indicators. I will first work with a specific stock, symbol AI.PA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's download historical data for the last 20 years from Yahoo! API, and read it in a DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100     8  100     8    0     0    235      0 --:--:-- --:--:-- --:--:--   235\n",
      "100  355k    0  355k    0     0  1233k      0 --:--:-- --:--:-- --:--:-- 1233k\n"
     ]
    }
   ],
   "source": [
    "!curl -L 'http://query1.finance.yahoo.com/v7/finance/download/AI.PA?period1=946857600&period2=1593820800&interval=1d&events=history' > historical_data_AI-PA.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from -r requirements.txt (line 1)) (0.24.2)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from -r requirements.txt (line 2)) (1.16.4)\n",
      "Requirement already satisfied: scikit-learn in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from -r requirements.txt (line 3)) (0.20.3)\n",
      "Collecting ta (from -r requirements.txt (line 4))\n",
      "  Downloading https://files.pythonhosted.org/packages/90/ec/e4f5aea8c7f0f55f92b52ffbafa389ea82f3a10d9cab2760e40af34c5b3f/ta-0.5.25.tar.gz\n",
      "Requirement already satisfied: python-dateutil>=2.5.0 in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from pandas->-r requirements.txt (line 1)) (2.7.3)\n",
      "Requirement already satisfied: pytz>=2011k in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from pandas->-r requirements.txt (line 1)) (2018.4)\n",
      "Requirement already satisfied: scipy>=0.13.3 in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from scikit-learn->-r requirements.txt (line 3)) (1.2.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from python-dateutil>=2.5.0->pandas->-r requirements.txt (line 1)) (1.11.0)\n",
      "Building wheels for collected packages: ta\n",
      "  Running setup.py bdist_wheel for ta ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/ec2-user/.cache/pip/wheels/2e/93/b7/cf649194508e53cee4145ffb949e9f26877a5a8dd12db9ed5b\n",
      "Successfully built ta\n",
      "Installing collected packages: ta\n",
      "Successfully installed ta-0.5.25\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 20.2b1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# install required python packages\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports go here\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ta\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer, mean_squared_error, mean_absolute_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.linear_model import ElasticNet\n",
    "import sagemaker\n",
    "import boto3\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('historical_data_AI-PA.csv', index_col=0, parse_dates=True, infer_datetime_format=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2000-01-03</th>\n",
       "      <td>34.854301</td>\n",
       "      <td>36.306599</td>\n",
       "      <td>34.771301</td>\n",
       "      <td>35.061798</td>\n",
       "      <td>10.226519</td>\n",
       "      <td>904282.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-04</th>\n",
       "      <td>35.061798</td>\n",
       "      <td>34.999500</td>\n",
       "      <td>32.613701</td>\n",
       "      <td>33.505798</td>\n",
       "      <td>9.772677</td>\n",
       "      <td>1381445.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-05</th>\n",
       "      <td>32.779701</td>\n",
       "      <td>33.402100</td>\n",
       "      <td>32.281700</td>\n",
       "      <td>33.194599</td>\n",
       "      <td>9.681908</td>\n",
       "      <td>853763.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-06</th>\n",
       "      <td>32.758900</td>\n",
       "      <td>36.223598</td>\n",
       "      <td>32.696701</td>\n",
       "      <td>35.580399</td>\n",
       "      <td>10.377778</td>\n",
       "      <td>1387137.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-07</th>\n",
       "      <td>35.580399</td>\n",
       "      <td>37.136398</td>\n",
       "      <td>34.958000</td>\n",
       "      <td>35.144798</td>\n",
       "      <td>10.250728</td>\n",
       "      <td>2198233.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Open       High        Low      Close  Adj Close     Volume\n",
       "Date                                                                        \n",
       "2000-01-03  34.854301  36.306599  34.771301  35.061798  10.226519   904282.0\n",
       "2000-01-04  35.061798  34.999500  32.613701  33.505798   9.772677  1381445.0\n",
       "2000-01-05  32.779701  33.402100  32.281700  33.194599   9.681908   853763.0\n",
       "2000-01-06  32.758900  36.223598  32.696701  35.580399  10.377778  1387137.0\n",
       "2000-01-07  35.580399  37.136398  34.958000  35.144798  10.250728  2198233.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-06-29</th>\n",
       "      <td>126.099998</td>\n",
       "      <td>127.750000</td>\n",
       "      <td>125.699997</td>\n",
       "      <td>127.500000</td>\n",
       "      <td>127.500000</td>\n",
       "      <td>966640.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-30</th>\n",
       "      <td>127.500000</td>\n",
       "      <td>128.399994</td>\n",
       "      <td>126.599998</td>\n",
       "      <td>128.399994</td>\n",
       "      <td>128.399994</td>\n",
       "      <td>1021045.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-01</th>\n",
       "      <td>128.300003</td>\n",
       "      <td>129.350006</td>\n",
       "      <td>127.050003</td>\n",
       "      <td>128.449997</td>\n",
       "      <td>128.449997</td>\n",
       "      <td>684533.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-02</th>\n",
       "      <td>129.600006</td>\n",
       "      <td>132.949997</td>\n",
       "      <td>128.800003</td>\n",
       "      <td>132.649994</td>\n",
       "      <td>132.649994</td>\n",
       "      <td>1304983.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-03</th>\n",
       "      <td>132.000000</td>\n",
       "      <td>132.899994</td>\n",
       "      <td>129.800003</td>\n",
       "      <td>130.350006</td>\n",
       "      <td>130.350006</td>\n",
       "      <td>681232.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Open        High         Low       Close   Adj Close  \\\n",
       "Date                                                                     \n",
       "2020-06-29  126.099998  127.750000  125.699997  127.500000  127.500000   \n",
       "2020-06-30  127.500000  128.399994  126.599998  128.399994  128.399994   \n",
       "2020-07-01  128.300003  129.350006  127.050003  128.449997  128.449997   \n",
       "2020-07-02  129.600006  132.949997  128.800003  132.649994  132.649994   \n",
       "2020-07-03  132.000000  132.899994  129.800003  130.350006  130.350006   \n",
       "\n",
       "               Volume  \n",
       "Date                   \n",
       "2020-06-29   966640.0  \n",
       "2020-06-30  1021045.0  \n",
       "2020-07-01   684533.0  \n",
       "2020-07-02  1304983.0  \n",
       "2020-07-03   681232.0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display descriptive stats about the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5267.000000</td>\n",
       "      <td>5267.000000</td>\n",
       "      <td>5267.000000</td>\n",
       "      <td>5267.000000</td>\n",
       "      <td>5267.000000</td>\n",
       "      <td>5.267000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>65.154315</td>\n",
       "      <td>65.766419</td>\n",
       "      <td>64.550407</td>\n",
       "      <td>65.181575</td>\n",
       "      <td>50.084645</td>\n",
       "      <td>1.257315e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>26.437988</td>\n",
       "      <td>26.582026</td>\n",
       "      <td>26.277434</td>\n",
       "      <td>26.441206</td>\n",
       "      <td>31.948313</td>\n",
       "      <td>7.275060e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>27.530800</td>\n",
       "      <td>28.087299</td>\n",
       "      <td>26.804600</td>\n",
       "      <td>26.970600</td>\n",
       "      <td>7.866548</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>38.716299</td>\n",
       "      <td>39.138500</td>\n",
       "      <td>38.380001</td>\n",
       "      <td>38.770450</td>\n",
       "      <td>18.792883</td>\n",
       "      <td>8.208425e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>60.484501</td>\n",
       "      <td>61.058498</td>\n",
       "      <td>59.993599</td>\n",
       "      <td>60.505001</td>\n",
       "      <td>42.880104</td>\n",
       "      <td>1.094817e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>86.474750</td>\n",
       "      <td>87.168801</td>\n",
       "      <td>85.711201</td>\n",
       "      <td>86.457001</td>\n",
       "      <td>76.446960</td>\n",
       "      <td>1.493460e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>140.500000</td>\n",
       "      <td>140.699997</td>\n",
       "      <td>139.800003</td>\n",
       "      <td>140.300003</td>\n",
       "      <td>137.140625</td>\n",
       "      <td>1.014686e+07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Open         High          Low        Close    Adj Close  \\\n",
       "count  5267.000000  5267.000000  5267.000000  5267.000000  5267.000000   \n",
       "mean     65.154315    65.766419    64.550407    65.181575    50.084645   \n",
       "std      26.437988    26.582026    26.277434    26.441206    31.948313   \n",
       "min      27.530800    28.087299    26.804600    26.970600     7.866548   \n",
       "25%      38.716299    39.138500    38.380001    38.770450    18.792883   \n",
       "50%      60.484501    61.058498    59.993599    60.505001    42.880104   \n",
       "75%      86.474750    87.168801    85.711201    86.457001    76.446960   \n",
       "max     140.500000   140.699997   139.800003   140.300003   137.140625   \n",
       "\n",
       "             Volume  \n",
       "count  5.267000e+03  \n",
       "mean   1.257315e+06  \n",
       "std    7.275060e+05  \n",
       "min    0.000000e+00  \n",
       "25%    8.208425e+05  \n",
       "50%    1.094817e+06  \n",
       "75%    1.493460e+06  \n",
       "max    1.014686e+07  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the distributions of our features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<matplotlib.axes._subplots.AxesSubplot object at 0x7f8197f42780>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x7f8197cc09e8>],\n",
       "       [<matplotlib.axes._subplots.AxesSubplot object at 0x7f8197c6dda0>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x7f8197c9d358>],\n",
       "       [<matplotlib.axes._subplots.AxesSubplot object at 0x7f8197c438d0>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x7f8197bebe48>]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Are there any NaN values in the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's add technical analysis indicators to our feature set.  \n",
    "I will use the 'ta' python package to compute them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/ta/trend.py:608: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dip[i] = 100 * (self._dip[i]/self._trs[i])\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/ta/trend.py:612: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  din[i] = 100 * (self._din[i]/self._trs[i])\n"
     ]
    }
   ],
   "source": [
    "df = ta.add_all_ta_features(df, open='Open', high='High', low='Low', close='Close', volume='Volume')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>volume_adi</th>\n",
       "      <th>volume_obv</th>\n",
       "      <th>volume_cmf</th>\n",
       "      <th>volume_fi</th>\n",
       "      <th>...</th>\n",
       "      <th>momentum_uo</th>\n",
       "      <th>momentum_stoch</th>\n",
       "      <th>momentum_stoch_signal</th>\n",
       "      <th>momentum_wr</th>\n",
       "      <th>momentum_ao</th>\n",
       "      <th>momentum_kama</th>\n",
       "      <th>momentum_roc</th>\n",
       "      <th>others_dr</th>\n",
       "      <th>others_dlr</th>\n",
       "      <th>others_cr</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2000-01-03</th>\n",
       "      <td>34.854301</td>\n",
       "      <td>36.306599</td>\n",
       "      <td>34.771301</td>\n",
       "      <td>35.061798</td>\n",
       "      <td>10.226519</td>\n",
       "      <td>904282.0</td>\n",
       "      <td>-5.620798e+05</td>\n",
       "      <td>904282.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-46.209035</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-04</th>\n",
       "      <td>35.061798</td>\n",
       "      <td>34.999500</td>\n",
       "      <td>32.613701</td>\n",
       "      <td>33.505798</td>\n",
       "      <td>9.772677</td>\n",
       "      <td>1381445.0</td>\n",
       "      <td>-9.104260e+05</td>\n",
       "      <td>-477163.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-4.437879</td>\n",
       "      <td>-4.539366</td>\n",
       "      <td>-4.437879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-05</th>\n",
       "      <td>32.779701</td>\n",
       "      <td>33.402100</td>\n",
       "      <td>32.281700</td>\n",
       "      <td>33.194599</td>\n",
       "      <td>9.681908</td>\n",
       "      <td>853763.0</td>\n",
       "      <td>-3.729013e+05</td>\n",
       "      <td>-1330926.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.928791</td>\n",
       "      <td>-0.933132</td>\n",
       "      <td>-5.325451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-06</th>\n",
       "      <td>32.758900</td>\n",
       "      <td>36.223598</td>\n",
       "      <td>32.696701</td>\n",
       "      <td>35.580399</td>\n",
       "      <td>10.377778</td>\n",
       "      <td>1387137.0</td>\n",
       "      <td>5.082923e+05</td>\n",
       "      <td>56211.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.187314</td>\n",
       "      <td>6.940771</td>\n",
       "      <td>1.479106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-07</th>\n",
       "      <td>35.580399</td>\n",
       "      <td>37.136398</td>\n",
       "      <td>34.958000</td>\n",
       "      <td>35.144798</td>\n",
       "      <td>10.250728</td>\n",
       "      <td>2198233.0</td>\n",
       "      <td>-1.312943e+06</td>\n",
       "      <td>-2142022.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.224272</td>\n",
       "      <td>-1.231828</td>\n",
       "      <td>0.236725</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 78 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Open       High        Low      Close  Adj Close     Volume  \\\n",
       "Date                                                                           \n",
       "2000-01-03  34.854301  36.306599  34.771301  35.061798  10.226519   904282.0   \n",
       "2000-01-04  35.061798  34.999500  32.613701  33.505798   9.772677  1381445.0   \n",
       "2000-01-05  32.779701  33.402100  32.281700  33.194599   9.681908   853763.0   \n",
       "2000-01-06  32.758900  36.223598  32.696701  35.580399  10.377778  1387137.0   \n",
       "2000-01-07  35.580399  37.136398  34.958000  35.144798  10.250728  2198233.0   \n",
       "\n",
       "              volume_adi  volume_obv  volume_cmf  volume_fi  ...  momentum_uo  \\\n",
       "Date                                                         ...                \n",
       "2000-01-03 -5.620798e+05    904282.0         NaN        NaN  ...          NaN   \n",
       "2000-01-04 -9.104260e+05   -477163.0         NaN        NaN  ...          NaN   \n",
       "2000-01-05 -3.729013e+05  -1330926.0         NaN        NaN  ...          NaN   \n",
       "2000-01-06  5.082923e+05     56211.0         NaN        NaN  ...          NaN   \n",
       "2000-01-07 -1.312943e+06  -2142022.0         NaN        NaN  ...          NaN   \n",
       "\n",
       "            momentum_stoch  momentum_stoch_signal  momentum_wr  momentum_ao  \\\n",
       "Date                                                                          \n",
       "2000-01-03             NaN                    NaN          NaN          NaN   \n",
       "2000-01-04             NaN                    NaN          NaN          NaN   \n",
       "2000-01-05             NaN                    NaN          NaN          NaN   \n",
       "2000-01-06             NaN                    NaN          NaN          NaN   \n",
       "2000-01-07             NaN                    NaN          NaN          NaN   \n",
       "\n",
       "            momentum_kama  momentum_roc  others_dr  others_dlr  others_cr  \n",
       "Date                                                                       \n",
       "2000-01-03            NaN           NaN -46.209035         NaN   0.000000  \n",
       "2000-01-04            NaN           NaN  -4.437879   -4.539366  -4.437879  \n",
       "2000-01-05            NaN           NaN  -0.928791   -0.933132  -5.325451  \n",
       "2000-01-06            NaN           NaN   7.187314    6.940771   1.479106  \n",
       "2000-01-07            NaN           NaN  -1.224272   -1.231828   0.236725  \n",
       "\n",
       "[5 rows x 78 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume', 'volume_adi',\n",
       "       'volume_obv', 'volume_cmf', 'volume_fi', 'momentum_mfi', 'volume_em',\n",
       "       'volume_sma_em', 'volume_vpt', 'volume_nvi', 'volume_vwap',\n",
       "       'volatility_atr', 'volatility_bbm', 'volatility_bbh', 'volatility_bbl',\n",
       "       'volatility_bbw', 'volatility_bbp', 'volatility_bbhi',\n",
       "       'volatility_bbli', 'volatility_kcc', 'volatility_kch', 'volatility_kcl',\n",
       "       'volatility_kcw', 'volatility_kcp', 'volatility_kchi',\n",
       "       'volatility_kcli', 'volatility_dcl', 'volatility_dch', 'trend_macd',\n",
       "       'trend_macd_signal', 'trend_macd_diff', 'trend_sma_fast',\n",
       "       'trend_sma_slow', 'trend_ema_fast', 'trend_ema_slow', 'trend_adx',\n",
       "       'trend_adx_pos', 'trend_adx_neg', 'trend_vortex_ind_pos',\n",
       "       'trend_vortex_ind_neg', 'trend_vortex_ind_diff', 'trend_trix',\n",
       "       'trend_mass_index', 'trend_cci', 'trend_dpo', 'trend_kst',\n",
       "       'trend_kst_sig', 'trend_kst_diff', 'trend_ichimoku_conv',\n",
       "       'trend_ichimoku_base', 'trend_ichimoku_a', 'trend_ichimoku_b',\n",
       "       'trend_visual_ichimoku_a', 'trend_visual_ichimoku_b', 'trend_aroon_up',\n",
       "       'trend_aroon_down', 'trend_aroon_ind', 'trend_psar_up',\n",
       "       'trend_psar_down', 'trend_psar_up_indicator',\n",
       "       'trend_psar_down_indicator', 'momentum_rsi', 'momentum_tsi',\n",
       "       'momentum_uo', 'momentum_stoch', 'momentum_stoch_signal', 'momentum_wr',\n",
       "       'momentum_ao', 'momentum_kama', 'momentum_roc', 'others_dr',\n",
       "       'others_dlr', 'others_cr'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The dataset now has much more features, but also NaN values. I will then perform interpolation of the dataset, so that they get replaced by neutral values, i.e. that will not influence the algorithm during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33869"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.interpolate(axis=0, limit_direction='both', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I will now add historical data for CAC40 and SBF120, the indices that AI.PA stock relates to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100     8  100     8    0     0    222      0 --:--:-- --:--:-- --:--:--   222\n",
      "100  404k    0  404k    0     0  1974k      0 --:--:-- --:--:-- --:--:-- 1974k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100     8  100     8    0     0    250      0 --:--:-- --:--:-- --:--:--   250\n",
      "100  339k    0  339k    0     0  1816k      0 --:--:-- --:--:-- --:--:-- 1816k\n"
     ]
    }
   ],
   "source": [
    "!curl -L 'http://query1.finance.yahoo.com/v7/finance/download/^FCHI?period1=946857600&period2=1593820800&interval=1d&events=history' > historical_data_CAC40.csv\n",
    "!curl -L 'http://query1.finance.yahoo.com/v7/finance/download/^SBF120?period1=946857600&period2=1593820800&interval=1d&events=history' > historical_data_SBF120.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prefix columns with index name\n",
    "\n",
    "cac40_df = pd.read_csv('historical_data_CAC40.csv', index_col=0, parse_dates=True, infer_datetime_format=True)\n",
    "sbf120_df = pd.read_csv('historical_data_SBF120.csv', index_col=0, parse_dates=True, infer_datetime_format=True)\n",
    "\n",
    "# Drop 'Volume' and 'Adj Close' features, meaningless regarding market indices\n",
    "cac40_df.drop(['Volume', 'Adj Close'], axis=1, inplace=True)\n",
    "sbf120_df.drop(['Volume', 'Adj Close'], axis=1, inplace=True)\n",
    "\n",
    "prefixed_cac_cols = list()\n",
    "prefixed_sbf_cols = list()\n",
    "for cac_col, sbf_col in zip(cac40_df.columns, sbf120_df.columns):\n",
    "    prefixed_cac_cols.append('cac40_' + cac_col)\n",
    "    prefixed_sbf_cols.append('sbf120_' + sbf_col)\n",
    "\n",
    "cac40_df.columns = prefixed_cac_cols\n",
    "sbf120_df.columns = prefixed_sbf_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             cac40_Open   cac40_High    cac40_Low  cac40_Close\n",
      "Date                                                          \n",
      "2000-01-03  6024.379883  6102.120117  5901.770020  5917.370117\n",
      "2000-01-04  5922.229980  5925.069824  5657.200195  5672.020020\n",
      "2000-01-05  5521.830078  5589.500000  5461.589844  5479.700195\n",
      "2000-01-06  5485.930176  5530.259766  5388.850098  5450.109863\n",
      "2000-01-07  5423.879883  5561.689941  5423.879883  5539.609863\n",
      "\n",
      "            sbf120_Open  sbf120_High   sbf120_Low  sbf120_Close\n",
      "Date                                                           \n",
      "2000-01-03  4035.110107  4035.110107  4035.110107   4035.110107\n",
      "2000-01-04  3873.149902  3873.149902  3873.149902   3873.149902\n",
      "2000-01-05  3743.870117  3743.870117  3743.870117   3743.870117\n",
      "2000-01-06  3728.080078  3728.080078  3728.080078   3728.080078\n",
      "2000-01-07  3794.070068  3794.070068  3794.070068   3794.070068\n"
     ]
    }
   ],
   "source": [
    "# Check everything went as expected\n",
    "\n",
    "print(cac40_df.head())\n",
    "print('')\n",
    "print(sbf120_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpolate both dataframes to fill NaNs\n",
    "cac40_df.interpolate(axis=0, limit_direction='both', inplace=True)\n",
    "sbf120_df.interpolate(axis=0, limit_direction='both', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add These features to our stock price dataset\n",
    "df = pd.concat([df, cac40_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df, sbf120_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>volume_adi</th>\n",
       "      <th>volume_obv</th>\n",
       "      <th>volume_cmf</th>\n",
       "      <th>volume_fi</th>\n",
       "      <th>...</th>\n",
       "      <th>others_dlr</th>\n",
       "      <th>others_cr</th>\n",
       "      <th>cac40_Open</th>\n",
       "      <th>cac40_High</th>\n",
       "      <th>cac40_Low</th>\n",
       "      <th>cac40_Close</th>\n",
       "      <th>sbf120_Open</th>\n",
       "      <th>sbf120_High</th>\n",
       "      <th>sbf120_Low</th>\n",
       "      <th>sbf120_Close</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2000-01-03</th>\n",
       "      <td>34.854301</td>\n",
       "      <td>36.306599</td>\n",
       "      <td>34.771301</td>\n",
       "      <td>35.061798</td>\n",
       "      <td>10.226519</td>\n",
       "      <td>904282.0</td>\n",
       "      <td>-5.620798e+05</td>\n",
       "      <td>904282.0</td>\n",
       "      <td>-0.402545</td>\n",
       "      <td>-833509.084988</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.539366</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6024.379883</td>\n",
       "      <td>6102.120117</td>\n",
       "      <td>5901.770020</td>\n",
       "      <td>5917.370117</td>\n",
       "      <td>4035.110107</td>\n",
       "      <td>4035.110107</td>\n",
       "      <td>4035.110107</td>\n",
       "      <td>4035.110107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-04</th>\n",
       "      <td>35.061798</td>\n",
       "      <td>34.999500</td>\n",
       "      <td>32.613701</td>\n",
       "      <td>33.505798</td>\n",
       "      <td>9.772677</td>\n",
       "      <td>1381445.0</td>\n",
       "      <td>-9.104260e+05</td>\n",
       "      <td>-477163.0</td>\n",
       "      <td>-0.402545</td>\n",
       "      <td>-833509.084988</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.539366</td>\n",
       "      <td>-4.437879</td>\n",
       "      <td>5922.229980</td>\n",
       "      <td>5925.069824</td>\n",
       "      <td>5657.200195</td>\n",
       "      <td>5672.020020</td>\n",
       "      <td>3873.149902</td>\n",
       "      <td>3873.149902</td>\n",
       "      <td>3873.149902</td>\n",
       "      <td>3873.149902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-05</th>\n",
       "      <td>32.779701</td>\n",
       "      <td>33.402100</td>\n",
       "      <td>32.281700</td>\n",
       "      <td>33.194599</td>\n",
       "      <td>9.681908</td>\n",
       "      <td>853763.0</td>\n",
       "      <td>-3.729013e+05</td>\n",
       "      <td>-1330926.0</td>\n",
       "      <td>-0.402545</td>\n",
       "      <td>-833509.084988</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.933132</td>\n",
       "      <td>-5.325451</td>\n",
       "      <td>5521.830078</td>\n",
       "      <td>5589.500000</td>\n",
       "      <td>5461.589844</td>\n",
       "      <td>5479.700195</td>\n",
       "      <td>3743.870117</td>\n",
       "      <td>3743.870117</td>\n",
       "      <td>3743.870117</td>\n",
       "      <td>3743.870117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-06</th>\n",
       "      <td>32.758900</td>\n",
       "      <td>36.223598</td>\n",
       "      <td>32.696701</td>\n",
       "      <td>35.580399</td>\n",
       "      <td>10.377778</td>\n",
       "      <td>1387137.0</td>\n",
       "      <td>5.082923e+05</td>\n",
       "      <td>56211.0</td>\n",
       "      <td>-0.402545</td>\n",
       "      <td>-833509.084988</td>\n",
       "      <td>...</td>\n",
       "      <td>6.940771</td>\n",
       "      <td>1.479106</td>\n",
       "      <td>5485.930176</td>\n",
       "      <td>5530.259766</td>\n",
       "      <td>5388.850098</td>\n",
       "      <td>5450.109863</td>\n",
       "      <td>3728.080078</td>\n",
       "      <td>3728.080078</td>\n",
       "      <td>3728.080078</td>\n",
       "      <td>3728.080078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-07</th>\n",
       "      <td>35.580399</td>\n",
       "      <td>37.136398</td>\n",
       "      <td>34.958000</td>\n",
       "      <td>35.144798</td>\n",
       "      <td>10.250728</td>\n",
       "      <td>2198233.0</td>\n",
       "      <td>-1.312943e+06</td>\n",
       "      <td>-2142022.0</td>\n",
       "      <td>-0.402545</td>\n",
       "      <td>-833509.084988</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.231828</td>\n",
       "      <td>0.236725</td>\n",
       "      <td>5423.879883</td>\n",
       "      <td>5561.689941</td>\n",
       "      <td>5423.879883</td>\n",
       "      <td>5539.609863</td>\n",
       "      <td>3794.070068</td>\n",
       "      <td>3794.070068</td>\n",
       "      <td>3794.070068</td>\n",
       "      <td>3794.070068</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 86 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Open       High        Low      Close  Adj Close     Volume  \\\n",
       "Date                                                                           \n",
       "2000-01-03  34.854301  36.306599  34.771301  35.061798  10.226519   904282.0   \n",
       "2000-01-04  35.061798  34.999500  32.613701  33.505798   9.772677  1381445.0   \n",
       "2000-01-05  32.779701  33.402100  32.281700  33.194599   9.681908   853763.0   \n",
       "2000-01-06  32.758900  36.223598  32.696701  35.580399  10.377778  1387137.0   \n",
       "2000-01-07  35.580399  37.136398  34.958000  35.144798  10.250728  2198233.0   \n",
       "\n",
       "              volume_adi  volume_obv  volume_cmf      volume_fi  ...  \\\n",
       "Date                                                             ...   \n",
       "2000-01-03 -5.620798e+05    904282.0   -0.402545 -833509.084988  ...   \n",
       "2000-01-04 -9.104260e+05   -477163.0   -0.402545 -833509.084988  ...   \n",
       "2000-01-05 -3.729013e+05  -1330926.0   -0.402545 -833509.084988  ...   \n",
       "2000-01-06  5.082923e+05     56211.0   -0.402545 -833509.084988  ...   \n",
       "2000-01-07 -1.312943e+06  -2142022.0   -0.402545 -833509.084988  ...   \n",
       "\n",
       "            others_dlr  others_cr   cac40_Open   cac40_High    cac40_Low  \\\n",
       "Date                                                                       \n",
       "2000-01-03   -4.539366   0.000000  6024.379883  6102.120117  5901.770020   \n",
       "2000-01-04   -4.539366  -4.437879  5922.229980  5925.069824  5657.200195   \n",
       "2000-01-05   -0.933132  -5.325451  5521.830078  5589.500000  5461.589844   \n",
       "2000-01-06    6.940771   1.479106  5485.930176  5530.259766  5388.850098   \n",
       "2000-01-07   -1.231828   0.236725  5423.879883  5561.689941  5423.879883   \n",
       "\n",
       "            cac40_Close  sbf120_Open  sbf120_High   sbf120_Low  sbf120_Close  \n",
       "Date                                                                          \n",
       "2000-01-03  5917.370117  4035.110107  4035.110107  4035.110107   4035.110107  \n",
       "2000-01-04  5672.020020  3873.149902  3873.149902  3873.149902   3873.149902  \n",
       "2000-01-05  5479.700195  3743.870117  3743.870117  3743.870117   3743.870117  \n",
       "2000-01-06  5450.109863  3728.080078  3728.080078  3728.080078   3728.080078  \n",
       "2000-01-07  5539.609863  3794.070068  3794.070068  3794.070068   3794.070068  \n",
       "\n",
       "[5 rows x 86 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The value I am trying to predict is Ajdusted Close for day d + 1 to 7, from stock characteristics of day d. This means I have to shift the Adjusted Close column by -1 to -7, and drop the last 7 rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjclose_df = pd.DataFrame()\n",
    "adjclose_cols = list()\n",
    "for i in range(1, 8):\n",
    "    colname = 'AdjClose_D+' + str(i)\n",
    "    adjclose_df[colname] = df['Adj Close'].shift(periods=-i)\n",
    "    adjclose_cols.append(colname)\n",
    "adjclose_df.columns = adjclose_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AdjClose_D+1</th>\n",
       "      <th>AdjClose_D+2</th>\n",
       "      <th>AdjClose_D+3</th>\n",
       "      <th>AdjClose_D+4</th>\n",
       "      <th>AdjClose_D+5</th>\n",
       "      <th>AdjClose_D+6</th>\n",
       "      <th>AdjClose_D+7</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-06-25</th>\n",
       "      <td>126.099998</td>\n",
       "      <td>127.500000</td>\n",
       "      <td>128.399994</td>\n",
       "      <td>128.449997</td>\n",
       "      <td>132.649994</td>\n",
       "      <td>130.350006</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-26</th>\n",
       "      <td>127.500000</td>\n",
       "      <td>128.399994</td>\n",
       "      <td>128.449997</td>\n",
       "      <td>132.649994</td>\n",
       "      <td>130.350006</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-29</th>\n",
       "      <td>128.399994</td>\n",
       "      <td>128.449997</td>\n",
       "      <td>132.649994</td>\n",
       "      <td>130.350006</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-30</th>\n",
       "      <td>128.449997</td>\n",
       "      <td>132.649994</td>\n",
       "      <td>130.350006</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-01</th>\n",
       "      <td>132.649994</td>\n",
       "      <td>130.350006</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-02</th>\n",
       "      <td>130.350006</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-03</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            AdjClose_D+1  AdjClose_D+2  AdjClose_D+3  AdjClose_D+4  \\\n",
       "Date                                                                 \n",
       "2020-06-25    126.099998    127.500000    128.399994    128.449997   \n",
       "2020-06-26    127.500000    128.399994    128.449997    132.649994   \n",
       "2020-06-29    128.399994    128.449997    132.649994    130.350006   \n",
       "2020-06-30    128.449997    132.649994    130.350006           NaN   \n",
       "2020-07-01    132.649994    130.350006           NaN           NaN   \n",
       "2020-07-02    130.350006           NaN           NaN           NaN   \n",
       "2020-07-03           NaN           NaN           NaN           NaN   \n",
       "\n",
       "            AdjClose_D+5  AdjClose_D+6  AdjClose_D+7  \n",
       "Date                                                  \n",
       "2020-06-25    132.649994    130.350006           NaN  \n",
       "2020-06-26    130.350006           NaN           NaN  \n",
       "2020-06-29           NaN           NaN           NaN  \n",
       "2020-06-30           NaN           NaN           NaN  \n",
       "2020-07-01           NaN           NaN           NaN  \n",
       "2020-07-02           NaN           NaN           NaN  \n",
       "2020-07-03           NaN           NaN           NaN  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adjclose_df.tail(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjclose_df.drop(adjclose_df.tail(i).index,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(df.tail(i).index,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AdjClose_D+1</th>\n",
       "      <th>AdjClose_D+2</th>\n",
       "      <th>AdjClose_D+3</th>\n",
       "      <th>AdjClose_D+4</th>\n",
       "      <th>AdjClose_D+5</th>\n",
       "      <th>AdjClose_D+6</th>\n",
       "      <th>AdjClose_D+7</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-06-16</th>\n",
       "      <td>127.400002</td>\n",
       "      <td>126.550003</td>\n",
       "      <td>128.100006</td>\n",
       "      <td>127.349998</td>\n",
       "      <td>129.050003</td>\n",
       "      <td>126.199997</td>\n",
       "      <td>126.849998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-17</th>\n",
       "      <td>126.550003</td>\n",
       "      <td>128.100006</td>\n",
       "      <td>127.349998</td>\n",
       "      <td>129.050003</td>\n",
       "      <td>126.199997</td>\n",
       "      <td>126.849998</td>\n",
       "      <td>126.099998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-18</th>\n",
       "      <td>128.100006</td>\n",
       "      <td>127.349998</td>\n",
       "      <td>129.050003</td>\n",
       "      <td>126.199997</td>\n",
       "      <td>126.849998</td>\n",
       "      <td>126.099998</td>\n",
       "      <td>127.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-19</th>\n",
       "      <td>127.349998</td>\n",
       "      <td>129.050003</td>\n",
       "      <td>126.199997</td>\n",
       "      <td>126.849998</td>\n",
       "      <td>126.099998</td>\n",
       "      <td>127.500000</td>\n",
       "      <td>128.399994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-22</th>\n",
       "      <td>129.050003</td>\n",
       "      <td>126.199997</td>\n",
       "      <td>126.849998</td>\n",
       "      <td>126.099998</td>\n",
       "      <td>127.500000</td>\n",
       "      <td>128.399994</td>\n",
       "      <td>128.449997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-23</th>\n",
       "      <td>126.199997</td>\n",
       "      <td>126.849998</td>\n",
       "      <td>126.099998</td>\n",
       "      <td>127.500000</td>\n",
       "      <td>128.399994</td>\n",
       "      <td>128.449997</td>\n",
       "      <td>132.649994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-24</th>\n",
       "      <td>126.849998</td>\n",
       "      <td>126.099998</td>\n",
       "      <td>127.500000</td>\n",
       "      <td>128.399994</td>\n",
       "      <td>128.449997</td>\n",
       "      <td>132.649994</td>\n",
       "      <td>130.350006</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            AdjClose_D+1  AdjClose_D+2  AdjClose_D+3  AdjClose_D+4  \\\n",
       "Date                                                                 \n",
       "2020-06-16    127.400002    126.550003    128.100006    127.349998   \n",
       "2020-06-17    126.550003    128.100006    127.349998    129.050003   \n",
       "2020-06-18    128.100006    127.349998    129.050003    126.199997   \n",
       "2020-06-19    127.349998    129.050003    126.199997    126.849998   \n",
       "2020-06-22    129.050003    126.199997    126.849998    126.099998   \n",
       "2020-06-23    126.199997    126.849998    126.099998    127.500000   \n",
       "2020-06-24    126.849998    126.099998    127.500000    128.399994   \n",
       "\n",
       "            AdjClose_D+5  AdjClose_D+6  AdjClose_D+7  \n",
       "Date                                                  \n",
       "2020-06-16    129.050003    126.199997    126.849998  \n",
       "2020-06-17    126.199997    126.849998    126.099998  \n",
       "2020-06-18    126.849998    126.099998    127.500000  \n",
       "2020-06-19    126.099998    127.500000    128.399994  \n",
       "2020-06-22    127.500000    128.399994    128.449997  \n",
       "2020-06-23    128.399994    128.449997    132.649994  \n",
       "2020-06-24    128.449997    132.649994    130.350006  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adjclose_df.tail(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AdjClose_D+1</th>\n",
       "      <th>AdjClose_D+2</th>\n",
       "      <th>AdjClose_D+3</th>\n",
       "      <th>AdjClose_D+4</th>\n",
       "      <th>AdjClose_D+5</th>\n",
       "      <th>AdjClose_D+6</th>\n",
       "      <th>AdjClose_D+7</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2000-01-03</th>\n",
       "      <td>9.772677</td>\n",
       "      <td>9.681908</td>\n",
       "      <td>10.377778</td>\n",
       "      <td>10.250728</td>\n",
       "      <td>9.893692</td>\n",
       "      <td>9.802925</td>\n",
       "      <td>9.916677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-04</th>\n",
       "      <td>9.681908</td>\n",
       "      <td>10.377778</td>\n",
       "      <td>10.250728</td>\n",
       "      <td>9.893692</td>\n",
       "      <td>9.802925</td>\n",
       "      <td>9.916677</td>\n",
       "      <td>10.147853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-05</th>\n",
       "      <td>10.377778</td>\n",
       "      <td>10.250728</td>\n",
       "      <td>9.893692</td>\n",
       "      <td>9.802925</td>\n",
       "      <td>9.916677</td>\n",
       "      <td>10.147853</td>\n",
       "      <td>10.371740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-06</th>\n",
       "      <td>10.250728</td>\n",
       "      <td>9.893692</td>\n",
       "      <td>9.802925</td>\n",
       "      <td>9.916677</td>\n",
       "      <td>10.147853</td>\n",
       "      <td>10.371740</td>\n",
       "      <td>10.026813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-07</th>\n",
       "      <td>9.893692</td>\n",
       "      <td>9.802925</td>\n",
       "      <td>9.916677</td>\n",
       "      <td>10.147853</td>\n",
       "      <td>10.371740</td>\n",
       "      <td>10.026813</td>\n",
       "      <td>9.923937</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            AdjClose_D+1  AdjClose_D+2  AdjClose_D+3  AdjClose_D+4  \\\n",
       "Date                                                                 \n",
       "2000-01-03      9.772677      9.681908     10.377778     10.250728   \n",
       "2000-01-04      9.681908     10.377778     10.250728      9.893692   \n",
       "2000-01-05     10.377778     10.250728      9.893692      9.802925   \n",
       "2000-01-06     10.250728      9.893692      9.802925      9.916677   \n",
       "2000-01-07      9.893692      9.802925      9.916677     10.147853   \n",
       "\n",
       "            AdjClose_D+5  AdjClose_D+6  AdjClose_D+7  \n",
       "Date                                                  \n",
       "2000-01-03      9.893692      9.802925      9.916677  \n",
       "2000-01-04      9.802925      9.916677     10.147853  \n",
       "2000-01-05      9.916677     10.147853     10.371740  \n",
       "2000-01-06     10.147853     10.371740     10.026813  \n",
       "2000-01-07     10.371740     10.026813      9.923937  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adjclose_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5265, 7)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adjclose_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>volume_adi</th>\n",
       "      <th>volume_obv</th>\n",
       "      <th>volume_cmf</th>\n",
       "      <th>volume_fi</th>\n",
       "      <th>...</th>\n",
       "      <th>others_dlr</th>\n",
       "      <th>others_cr</th>\n",
       "      <th>cac40_Open</th>\n",
       "      <th>cac40_High</th>\n",
       "      <th>cac40_Low</th>\n",
       "      <th>cac40_Close</th>\n",
       "      <th>sbf120_Open</th>\n",
       "      <th>sbf120_High</th>\n",
       "      <th>sbf120_Low</th>\n",
       "      <th>sbf120_Close</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-06-18</th>\n",
       "      <td>127.150002</td>\n",
       "      <td>128.199997</td>\n",
       "      <td>125.449997</td>\n",
       "      <td>126.550003</td>\n",
       "      <td>126.550003</td>\n",
       "      <td>886472.0</td>\n",
       "      <td>2.255536e+08</td>\n",
       "      <td>246930822.0</td>\n",
       "      <td>0.187488</td>\n",
       "      <td>409308.773296</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.669425</td>\n",
       "      <td>260.934151</td>\n",
       "      <td>4978.299805</td>\n",
       "      <td>5017.180176</td>\n",
       "      <td>4908.600098</td>\n",
       "      <td>4958.750000</td>\n",
       "      <td>3976.079515</td>\n",
       "      <td>3981.240028</td>\n",
       "      <td>3921.819399</td>\n",
       "      <td>3940.668816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-19</th>\n",
       "      <td>127.750000</td>\n",
       "      <td>129.350006</td>\n",
       "      <td>127.199997</td>\n",
       "      <td>128.100006</td>\n",
       "      <td>128.100006</td>\n",
       "      <td>2035713.0</td>\n",
       "      <td>2.252222e+08</td>\n",
       "      <td>248966535.0</td>\n",
       "      <td>0.147959</td>\n",
       "      <td>801601.985274</td>\n",
       "      <td>...</td>\n",
       "      <td>1.217375</td>\n",
       "      <td>265.354926</td>\n",
       "      <td>4997.529785</td>\n",
       "      <td>5040.470215</td>\n",
       "      <td>4979.450195</td>\n",
       "      <td>4979.450195</td>\n",
       "      <td>3976.365011</td>\n",
       "      <td>3981.530020</td>\n",
       "      <td>3922.057630</td>\n",
       "      <td>3940.923466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-22</th>\n",
       "      <td>127.400002</td>\n",
       "      <td>129.149994</td>\n",
       "      <td>127.000000</td>\n",
       "      <td>127.349998</td>\n",
       "      <td>127.349998</td>\n",
       "      <td>738802.0</td>\n",
       "      <td>2.247240e+08</td>\n",
       "      <td>248227733.0</td>\n",
       "      <td>0.105716</td>\n",
       "      <td>607929.214461</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.587207</td>\n",
       "      <td>263.215823</td>\n",
       "      <td>4928.009766</td>\n",
       "      <td>5006.399902</td>\n",
       "      <td>4902.060059</td>\n",
       "      <td>4948.700195</td>\n",
       "      <td>3976.650507</td>\n",
       "      <td>3981.820011</td>\n",
       "      <td>3922.295861</td>\n",
       "      <td>3941.178117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-23</th>\n",
       "      <td>128.250000</td>\n",
       "      <td>129.500000</td>\n",
       "      <td>127.800003</td>\n",
       "      <td>129.050003</td>\n",
       "      <td>129.050003</td>\n",
       "      <td>984631.0</td>\n",
       "      <td>2.251873e+08</td>\n",
       "      <td>249212364.0</td>\n",
       "      <td>0.161753</td>\n",
       "      <td>760207.558560</td>\n",
       "      <td>...</td>\n",
       "      <td>1.326076</td>\n",
       "      <td>268.064419</td>\n",
       "      <td>4972.879883</td>\n",
       "      <td>5046.310059</td>\n",
       "      <td>4962.600098</td>\n",
       "      <td>5017.680176</td>\n",
       "      <td>3976.936003</td>\n",
       "      <td>3982.110002</td>\n",
       "      <td>3922.534092</td>\n",
       "      <td>3941.432767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-24</th>\n",
       "      <td>128.149994</td>\n",
       "      <td>128.300003</td>\n",
       "      <td>126.199997</td>\n",
       "      <td>126.199997</td>\n",
       "      <td>126.199997</td>\n",
       "      <td>861152.0</td>\n",
       "      <td>2.243262e+08</td>\n",
       "      <td>248351212.0</td>\n",
       "      <td>0.147496</td>\n",
       "      <td>300993.854921</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.233202</td>\n",
       "      <td>259.935897</td>\n",
       "      <td>4985.629883</td>\n",
       "      <td>5004.040039</td>\n",
       "      <td>4871.359863</td>\n",
       "      <td>4871.359863</td>\n",
       "      <td>3977.221499</td>\n",
       "      <td>3982.399993</td>\n",
       "      <td>3922.772323</td>\n",
       "      <td>3941.687418</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 86 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Open        High         Low       Close   Adj Close  \\\n",
       "Date                                                                     \n",
       "2020-06-18  127.150002  128.199997  125.449997  126.550003  126.550003   \n",
       "2020-06-19  127.750000  129.350006  127.199997  128.100006  128.100006   \n",
       "2020-06-22  127.400002  129.149994  127.000000  127.349998  127.349998   \n",
       "2020-06-23  128.250000  129.500000  127.800003  129.050003  129.050003   \n",
       "2020-06-24  128.149994  128.300003  126.199997  126.199997  126.199997   \n",
       "\n",
       "               Volume    volume_adi   volume_obv  volume_cmf      volume_fi  \\\n",
       "Date                                                                          \n",
       "2020-06-18   886472.0  2.255536e+08  246930822.0    0.187488  409308.773296   \n",
       "2020-06-19  2035713.0  2.252222e+08  248966535.0    0.147959  801601.985274   \n",
       "2020-06-22   738802.0  2.247240e+08  248227733.0    0.105716  607929.214461   \n",
       "2020-06-23   984631.0  2.251873e+08  249212364.0    0.161753  760207.558560   \n",
       "2020-06-24   861152.0  2.243262e+08  248351212.0    0.147496  300993.854921   \n",
       "\n",
       "            ...  others_dlr   others_cr   cac40_Open   cac40_High  \\\n",
       "Date        ...                                                     \n",
       "2020-06-18  ...   -0.669425  260.934151  4978.299805  5017.180176   \n",
       "2020-06-19  ...    1.217375  265.354926  4997.529785  5040.470215   \n",
       "2020-06-22  ...   -0.587207  263.215823  4928.009766  5006.399902   \n",
       "2020-06-23  ...    1.326076  268.064419  4972.879883  5046.310059   \n",
       "2020-06-24  ...   -2.233202  259.935897  4985.629883  5004.040039   \n",
       "\n",
       "              cac40_Low  cac40_Close  sbf120_Open  sbf120_High   sbf120_Low  \\\n",
       "Date                                                                          \n",
       "2020-06-18  4908.600098  4958.750000  3976.079515  3981.240028  3921.819399   \n",
       "2020-06-19  4979.450195  4979.450195  3976.365011  3981.530020  3922.057630   \n",
       "2020-06-22  4902.060059  4948.700195  3976.650507  3981.820011  3922.295861   \n",
       "2020-06-23  4962.600098  5017.680176  3976.936003  3982.110002  3922.534092   \n",
       "2020-06-24  4871.359863  4871.359863  3977.221499  3982.399993  3922.772323   \n",
       "\n",
       "            sbf120_Close  \n",
       "Date                      \n",
       "2020-06-18   3940.668816  \n",
       "2020-06-19   3940.923466  \n",
       "2020-06-22   3941.178117  \n",
       "2020-06-23   3941.432767  \n",
       "2020-06-24   3941.687418  \n",
       "\n",
       "[5 rows x 86 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>volume_adi</th>\n",
       "      <th>volume_obv</th>\n",
       "      <th>volume_cmf</th>\n",
       "      <th>volume_fi</th>\n",
       "      <th>...</th>\n",
       "      <th>others_dlr</th>\n",
       "      <th>others_cr</th>\n",
       "      <th>cac40_Open</th>\n",
       "      <th>cac40_High</th>\n",
       "      <th>cac40_Low</th>\n",
       "      <th>cac40_Close</th>\n",
       "      <th>sbf120_Open</th>\n",
       "      <th>sbf120_High</th>\n",
       "      <th>sbf120_Low</th>\n",
       "      <th>sbf120_Close</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2000-01-03</th>\n",
       "      <td>34.854301</td>\n",
       "      <td>36.306599</td>\n",
       "      <td>34.771301</td>\n",
       "      <td>35.061798</td>\n",
       "      <td>10.226519</td>\n",
       "      <td>904282.0</td>\n",
       "      <td>-5.620798e+05</td>\n",
       "      <td>904282.0</td>\n",
       "      <td>-0.402545</td>\n",
       "      <td>-833509.084988</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.539366</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6024.379883</td>\n",
       "      <td>6102.120117</td>\n",
       "      <td>5901.770020</td>\n",
       "      <td>5917.370117</td>\n",
       "      <td>4035.110107</td>\n",
       "      <td>4035.110107</td>\n",
       "      <td>4035.110107</td>\n",
       "      <td>4035.110107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-04</th>\n",
       "      <td>35.061798</td>\n",
       "      <td>34.999500</td>\n",
       "      <td>32.613701</td>\n",
       "      <td>33.505798</td>\n",
       "      <td>9.772677</td>\n",
       "      <td>1381445.0</td>\n",
       "      <td>-9.104260e+05</td>\n",
       "      <td>-477163.0</td>\n",
       "      <td>-0.402545</td>\n",
       "      <td>-833509.084988</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.539366</td>\n",
       "      <td>-4.437879</td>\n",
       "      <td>5922.229980</td>\n",
       "      <td>5925.069824</td>\n",
       "      <td>5657.200195</td>\n",
       "      <td>5672.020020</td>\n",
       "      <td>3873.149902</td>\n",
       "      <td>3873.149902</td>\n",
       "      <td>3873.149902</td>\n",
       "      <td>3873.149902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-05</th>\n",
       "      <td>32.779701</td>\n",
       "      <td>33.402100</td>\n",
       "      <td>32.281700</td>\n",
       "      <td>33.194599</td>\n",
       "      <td>9.681908</td>\n",
       "      <td>853763.0</td>\n",
       "      <td>-3.729013e+05</td>\n",
       "      <td>-1330926.0</td>\n",
       "      <td>-0.402545</td>\n",
       "      <td>-833509.084988</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.933132</td>\n",
       "      <td>-5.325451</td>\n",
       "      <td>5521.830078</td>\n",
       "      <td>5589.500000</td>\n",
       "      <td>5461.589844</td>\n",
       "      <td>5479.700195</td>\n",
       "      <td>3743.870117</td>\n",
       "      <td>3743.870117</td>\n",
       "      <td>3743.870117</td>\n",
       "      <td>3743.870117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-06</th>\n",
       "      <td>32.758900</td>\n",
       "      <td>36.223598</td>\n",
       "      <td>32.696701</td>\n",
       "      <td>35.580399</td>\n",
       "      <td>10.377778</td>\n",
       "      <td>1387137.0</td>\n",
       "      <td>5.082923e+05</td>\n",
       "      <td>56211.0</td>\n",
       "      <td>-0.402545</td>\n",
       "      <td>-833509.084988</td>\n",
       "      <td>...</td>\n",
       "      <td>6.940771</td>\n",
       "      <td>1.479106</td>\n",
       "      <td>5485.930176</td>\n",
       "      <td>5530.259766</td>\n",
       "      <td>5388.850098</td>\n",
       "      <td>5450.109863</td>\n",
       "      <td>3728.080078</td>\n",
       "      <td>3728.080078</td>\n",
       "      <td>3728.080078</td>\n",
       "      <td>3728.080078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-07</th>\n",
       "      <td>35.580399</td>\n",
       "      <td>37.136398</td>\n",
       "      <td>34.958000</td>\n",
       "      <td>35.144798</td>\n",
       "      <td>10.250728</td>\n",
       "      <td>2198233.0</td>\n",
       "      <td>-1.312943e+06</td>\n",
       "      <td>-2142022.0</td>\n",
       "      <td>-0.402545</td>\n",
       "      <td>-833509.084988</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.231828</td>\n",
       "      <td>0.236725</td>\n",
       "      <td>5423.879883</td>\n",
       "      <td>5561.689941</td>\n",
       "      <td>5423.879883</td>\n",
       "      <td>5539.609863</td>\n",
       "      <td>3794.070068</td>\n",
       "      <td>3794.070068</td>\n",
       "      <td>3794.070068</td>\n",
       "      <td>3794.070068</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 86 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Open       High        Low      Close  Adj Close     Volume  \\\n",
       "Date                                                                           \n",
       "2000-01-03  34.854301  36.306599  34.771301  35.061798  10.226519   904282.0   \n",
       "2000-01-04  35.061798  34.999500  32.613701  33.505798   9.772677  1381445.0   \n",
       "2000-01-05  32.779701  33.402100  32.281700  33.194599   9.681908   853763.0   \n",
       "2000-01-06  32.758900  36.223598  32.696701  35.580399  10.377778  1387137.0   \n",
       "2000-01-07  35.580399  37.136398  34.958000  35.144798  10.250728  2198233.0   \n",
       "\n",
       "              volume_adi  volume_obv  volume_cmf      volume_fi  ...  \\\n",
       "Date                                                             ...   \n",
       "2000-01-03 -5.620798e+05    904282.0   -0.402545 -833509.084988  ...   \n",
       "2000-01-04 -9.104260e+05   -477163.0   -0.402545 -833509.084988  ...   \n",
       "2000-01-05 -3.729013e+05  -1330926.0   -0.402545 -833509.084988  ...   \n",
       "2000-01-06  5.082923e+05     56211.0   -0.402545 -833509.084988  ...   \n",
       "2000-01-07 -1.312943e+06  -2142022.0   -0.402545 -833509.084988  ...   \n",
       "\n",
       "            others_dlr  others_cr   cac40_Open   cac40_High    cac40_Low  \\\n",
       "Date                                                                       \n",
       "2000-01-03   -4.539366   0.000000  6024.379883  6102.120117  5901.770020   \n",
       "2000-01-04   -4.539366  -4.437879  5922.229980  5925.069824  5657.200195   \n",
       "2000-01-05   -0.933132  -5.325451  5521.830078  5589.500000  5461.589844   \n",
       "2000-01-06    6.940771   1.479106  5485.930176  5530.259766  5388.850098   \n",
       "2000-01-07   -1.231828   0.236725  5423.879883  5561.689941  5423.879883   \n",
       "\n",
       "            cac40_Close  sbf120_Open  sbf120_High   sbf120_Low  sbf120_Close  \n",
       "Date                                                                          \n",
       "2000-01-03  5917.370117  4035.110107  4035.110107  4035.110107   4035.110107  \n",
       "2000-01-04  5672.020020  3873.149902  3873.149902  3873.149902   3873.149902  \n",
       "2000-01-05  5479.700195  3743.870117  3743.870117  3743.870117   3743.870117  \n",
       "2000-01-06  5450.109863  3728.080078  3728.080078  3728.080078   3728.080078  \n",
       "2000-01-07  5539.609863  3794.070068  3794.070068  3794.070068   3794.070068  \n",
       "\n",
       "[5 rows x 86 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5265, 86)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I will then train 7 models, each predicting a different Adjusted Close value for D + 1 up to 7:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The dataset has very different value ranges, so I have to normalize it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaler = MinMaxScaler().fit(df.values)\n",
    "y_scaler = MinMaxScaler().fit(adjclose_df.values)\n",
    "\n",
    "X_scaled = X_scaler.transform(df.values)\n",
    "y_scaled = y_scaler.transform(adjclose_df.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split training and testing sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the last 10% data goes to the testing set\n",
    "train_size = int(len(X_scaled) * 0.90)\n",
    "train_X, test_X = X_scaled[0:train_size], X_scaled[train_size:len(X_scaled)]\n",
    "train_y, test_y = y_scaled[0:train_size], y_scaled[train_size:len(y_scaled)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set size: 89.99%\n",
      "testing set size: 10.01%\n"
     ]
    }
   ],
   "source": [
    "# Check if split is correct\n",
    "print(\"training set size: {:.2f}%\".format(len(train_X)/len(X_scaled) * 100))\n",
    "print(\"testing set size: {:.2f}%\".format(len(test_X)/len(X_scaled) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Investigate by training models\n",
    "I will then train several models including the DummyRegressor benchmark model, but also LinearRegressor and RandomForestRegressor, to get a better grasp of what model would be best suited for this use-case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function returns the Root Mean Squared Error, normalized by Standard-Deviation\n",
    "def stdev_root_mean_squared_error(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred)) / np.std(y_true)\n",
    "\n",
    "# This one returns the Mean Absolute Percentage Error, normalized by the true values\n",
    "# and expressed as a percentage\n",
    "def mean_asbsolute_percentage_error(y_true, y_pred):\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "def get_metrics_list(y_true, y_pred, decimals=3):\n",
    "    mse = round(mean_squared_error(y_true, y_pred), decimals)\n",
    "    sd_rmse = round(stdev_root_mean_squared_error(y_true, y_pred), decimals)\n",
    "    mae = round(mean_absolute_error(y_true, y_pred), decimals)\n",
    "    mape = round(mean_asbsolute_percentage_error(y_true, y_pred), decimals)\n",
    "    return [mse, sd_rmse, mae, mape]\n",
    "\n",
    "# Function that trains 7 models, each predicting adjusted close for day + 1 to 7\n",
    "def train_eval(model_list, train_X, train_y, test_X, test_y):\n",
    "    preds_list = list()\n",
    "    index_names = [str('d+{}'.format(i)) for i in range(1, 8)]\n",
    "    metrics_df = pd.DataFrame(columns=['MSE', 'SD-RMSE', 'MAE', 'MAPE'], index=index_names)\n",
    "    for i in range(7):\n",
    "        model_list[i].fit(train_X, train_y[:,i])\n",
    "        preds_list.append(model_list[i].predict(test_X))\n",
    "        metrics_df.loc['d+{}'.format(i + 1)] = get_metrics_list(test_y[:,i], preds_list[i])\n",
    "    print(metrics_df)\n",
    "    return preds_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       MSE SD-RMSE    MAE    MAPE\n",
      "d+1  0.242   5.106  0.482  62.904\n",
      "d+2  0.242   5.101  0.482  62.907\n",
      "d+3  0.242   5.095  0.483  62.909\n",
      "d+4  0.243   5.087  0.483  62.912\n",
      "d+5  0.243    5.08  0.483  62.916\n",
      "d+6  0.244   5.067  0.484  62.919\n",
      "d+7  0.244   5.057  0.484  62.923\n",
      "CPU times: user 15.8 ms, sys: 0 ns, total: 15.8 ms\n",
      "Wall time: 16.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "dummy_models_list = list()\n",
    "for i in range(7):\n",
    "    dummy_models_list.append(DummyRegressor())\n",
    "\n",
    "dummy_preds_list = train_eval(dummy_models_list, train_X, train_y, test_X, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       MSE SD-RMSE    MAE   MAPE\n",
      "d+1      0   0.126  0.008  1.057\n",
      "d+2      0   0.165  0.011  1.412\n",
      "d+3      0     0.2  0.013  1.673\n",
      "d+4  0.001   0.235  0.015  1.976\n",
      "d+5  0.001   0.266  0.017  2.234\n",
      "d+6  0.001   0.296  0.019  2.504\n",
      "d+7  0.001   0.321   0.02  2.703\n",
      "CPU times: user 354 ms, sys: 150 ms, total: 504 ms\n",
      "Wall time: 258 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "linear_models_list = list()\n",
    "for i in range(7):\n",
    "    linear_models_list.append(LinearRegression())\n",
    "\n",
    "linear_preds_list = train_eval(linear_models_list, train_X, train_y, test_X, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       MSE SD-RMSE    MAE    MAPE\n",
      "d+1  0.021   1.494  0.107  12.803\n",
      "d+2  0.022   1.544  0.112  13.419\n",
      "d+3  0.022    1.55  0.114  13.668\n",
      "d+4  0.024   1.615  0.121  14.512\n",
      "d+5  0.025   1.628  0.123  14.721\n",
      "d+6  0.025   1.624  0.123  14.828\n",
      "d+7  0.026   1.649  0.127  15.244\n",
      "CPU times: user 2min 22s, sys: 115 ms, total: 2min 22s\n",
      "Wall time: 2min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "rf_models_list = list()\n",
    "for i in range(7):\n",
    "    rf_models_list.append(RandomForestRegressor(n_estimators=100, random_state=42))\n",
    "\n",
    "rf_preds_list = train_eval(rf_models_list, train_X, train_y, test_X, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the dataset is relatively small (approx. 4000 records) and contains a lot of features (approx. 90), I would have expected LinearRegression model to perform poorly, and RandomForestRegressor to shine, but the linear model obtained suprisingly good evaluation metrics, versus quite poor accuracy for the random forests!\n",
    "\n",
    "Let's investigate further by training:\n",
    "- a simple model working with distances like K-Nearest-Neighbors\n",
    "- Kernel Ridge Regressor, another simple model\n",
    "- a more complex model like Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       MSE SD-RMSE    MAE    MAPE\n",
      "d+1  0.021   1.521  0.118  14.462\n",
      "d+2  0.022   1.544  0.121  14.744\n",
      "d+3  0.023   1.555  0.122  14.877\n",
      "d+4  0.023   1.567  0.123   15.04\n",
      "d+5  0.024   1.584  0.125  15.207\n",
      "d+6  0.024   1.591  0.125  15.265\n",
      "d+7  0.024   1.601  0.127  15.393\n",
      "CPU times: user 1.79 s, sys: 8.02 ms, total: 1.8 s\n",
      "Wall time: 1.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "knn_models_list = list()\n",
    "for i in range(7):\n",
    "    knn_models_list.append(KNeighborsRegressor())\n",
    "\n",
    "knn_preds_list = train_eval(knn_models_list, train_X, train_y, test_X, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN does quite poorly at predicting the values I am interested in, most probably because of the large number of features the dataset has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       MSE SD-RMSE    MAE    MAPE\n",
      "d+1  0.018   1.393  0.129  16.691\n",
      "d+2  0.017   1.368  0.126  16.328\n",
      "d+3  0.016   1.326  0.122  15.748\n",
      "d+4  0.015   1.266  0.116  14.959\n",
      "d+5  0.015   1.256  0.115  14.844\n",
      "d+6  0.014   1.232  0.113  14.569\n",
      "d+7  0.014   1.208  0.111  14.212\n",
      "CPU times: user 248 ms, sys: 4 ms, total: 252 ms\n",
      "Wall time: 252 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "svr_models_list = list()\n",
    "for i in range(7):\n",
    "    svr_models_list.append(SVR(gamma='auto'))\n",
    "\n",
    "svr_preds_list = train_eval(svr_models_list, train_X, train_y, test_X, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       MSE SD-RMSE    MAE   MAPE\n",
      "d+1      0   0.159  0.012  1.537\n",
      "d+2      0   0.194  0.014  1.863\n",
      "d+3      0   0.229  0.017  2.185\n",
      "d+4  0.001    0.26  0.019  2.432\n",
      "d+5  0.001   0.291  0.021  2.653\n",
      "d+6  0.001   0.317  0.022  2.858\n",
      "d+7  0.001   0.339  0.024  3.041\n",
      "CPU times: user 19.6 s, sys: 3.13 s, total: 22.7 s\n",
      "Wall time: 16.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "kr_models_list = list()\n",
    "for i in range(7):\n",
    "    kr_models_list.append(KernelRidge())\n",
    "\n",
    "kr_preds_list = train_eval(kr_models_list, train_X, train_y, test_X, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is interesting to note that a complex model like Support Vector Regressor fails to obtain good predictions even though it is supposed to work well with a large feature set, and that only simple models like Kernel Ridge and Linear Regression work well with the dataset.\n",
    "\n",
    "#### DeepAR\n",
    "\n",
    "The last algorithm I will train before moving to dimensionality reduction is DeepAR. It is included in Amazon SageMaker, and is supposed to work very well with time-oriented data such as the present market dataset. It is implemented with a Recurrent Neural Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SageMaker and S3 variables\n",
    "sagemaker_session = sagemaker.Session()\n",
    "s3_bucket = sagemaker.Session().default_bucket()\n",
    "s3_prefix = 'deepar-stock-pred'\n",
    "role = sagemaker.get_execution_role()\n",
    "region = sagemaker_session.boto_region_name\n",
    "s3_output_path = \"s3://{}/{}/output\".format(s3_bucket, s3_prefix)\n",
    "image_name = sagemaker.amazon.amazon_estimator.get_image_uri(region, \"forecasting-deepar\", \"latest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the test set into validation and testing set for DeepAR\n",
    "# the first 50% of the testing set goes to the validation set\n",
    "valid_size = int(len(test_X) * 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert datasets to DeepAR JSON format and write them to local files\n",
    "\n",
    "# Scale target values\n",
    "deepar_target_scaler = MinMaxScaler()\n",
    "adjclose_array = df['Adj Close'].values.reshape(-1, 1)\n",
    "deepar_target_scaler.fit(adjclose_array)\n",
    "target_list = deepar_target_scaler.transform(adjclose_array).reshape(1, -1)[0].tolist()\n",
    "\n",
    "# Scale feature set\n",
    "dyn_feat = df.drop('Adj Close', axis=1).values\n",
    "scaled_dyn_feat = MinMaxScaler().fit_transform(dyn_feat)\n",
    "\n",
    "# Create the train and valid feature lists\n",
    "train_feat = scaled_dyn_feat[:train_size]\n",
    "train_feat_list = np.swapaxes(train_feat, 0, 1).tolist()\n",
    "\n",
    "valid_feat = scaled_dyn_feat[train_size : train_size+valid_size]\n",
    "valid_feat_list = np.swapaxes(valid_feat, 0, 1).tolist()\n",
    "\n",
    "# Build JSON queries for training\n",
    "train_dict = {\"start\": str(df.index[0]), \"target\": target_list[:train_size], \"dynamic_feat\": train_feat_list}\n",
    "train_json = json.dumps(train_dict)\n",
    "\n",
    "valid_dict = {\"start\": str(df.index[train_size]), \"target\": target_list[train_size : train_size+valid_size], \"dynamic_feat\": valid_feat_list}\n",
    "valid_json = json.dumps(valid_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "s3.Object(bucket_name='sagemaker-us-west-2-378467645007', key='deepar-stock-pred/data/valid.json')"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Upload dataset to S3 to make it available to SageMaker\n",
    "s3_data_path = \"{}/data\".format(s3_prefix)\n",
    "s3 = boto3.resource('s3')\n",
    "bucket = s3.Bucket(s3_bucket)\n",
    "train_channel = s3_data_path + \"/train.json\"\n",
    "valid_channel = s3_data_path + \"/valid.json\"\n",
    "bucket.put_object(Key=train_channel, Body=train_json)\n",
    "bucket.put_object(Key=valid_channel, Body=valid_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"prediction_length\": \"7\",\n",
    "    \"context_length\": \"7\",\n",
    "    \"time_freq\": \"D\",\n",
    "    \"epochs\": \"200\",\n",
    "    \"early_stopping_patience\": \"40\",\n",
    "    \"num_layers\": \"2\",  \n",
    "    \"num_cells\": \"40\",\n",
    "    \"mini_batch_size\": \"128\",\n",
    "    \"learning_rate\": \"1e-3\",\n",
    "    \"dropout_rate\": \"0.1\", \n",
    "    \"likelihood\": \"gaussian\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-07-04 15:39:32 Starting - Starting the training job...\n",
      "2020-07-04 15:39:33 Starting - Launching requested ML instances.........\n",
      "2020-07-04 15:41:04 Starting - Preparing the instances for training......\n",
      "2020-07-04 15:42:30 Downloading - Downloading input data\n",
      "2020-07-04 15:42:30 Training - Downloading the training image.....\u001b[34mArguments: train\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:43:13 INFO 140466583250752] Reading default configuration from /opt/amazon/lib/python2.7/site-packages/algorithm/resources/default-input.json: {u'num_dynamic_feat': u'auto', u'dropout_rate': u'0.10', u'mini_batch_size': u'128', u'test_quantiles': u'[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]', u'_tuning_objective_metric': u'', u'_num_gpus': u'auto', u'num_eval_samples': u'100', u'learning_rate': u'0.001', u'num_cells': u'40', u'num_layers': u'2', u'embedding_dimension': u'10', u'_kvstore': u'auto', u'_num_kv_servers': u'auto', u'cardinality': u'auto', u'likelihood': u'student-t', u'early_stopping_patience': u''}\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:43:13 INFO 140466583250752] Reading provided configuration from /opt/ml/input/config/hyperparameters.json: {u'dropout_rate': u'0.1', u'learning_rate': u'1e-3', u'num_cells': u'40', u'prediction_length': u'7', u'epochs': u'200', u'time_freq': u'D', u'context_length': u'7', u'num_layers': u'2', u'mini_batch_size': u'128', u'likelihood': u'gaussian', u'early_stopping_patience': u'40'}\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:43:13 INFO 140466583250752] Final configuration: {u'dropout_rate': u'0.1', u'test_quantiles': u'[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]', u'_tuning_objective_metric': u'', u'num_eval_samples': u'100', u'learning_rate': u'1e-3', u'num_layers': u'2', u'epochs': u'200', u'embedding_dimension': u'10', u'num_cells': u'40', u'_num_kv_servers': u'auto', u'mini_batch_size': u'128', u'likelihood': u'gaussian', u'num_dynamic_feat': u'auto', u'cardinality': u'auto', u'_num_gpus': u'auto', u'prediction_length': u'7', u'time_freq': u'D', u'context_length': u'7', u'_kvstore': u'auto', u'early_stopping_patience': u'40'}\u001b[0m\n",
      "\u001b[34mProcess 1 is a worker.\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:43:13 INFO 140466583250752] Detected entry point for worker worker\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:43:13 INFO 140466583250752] Using early stopping with patience 40\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:43:13 INFO 140466583250752] [cardinality=auto] `cat` field was NOT found in the file `/opt/ml/input/data/train/train.json` and will NOT be used for training.\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:43:14 INFO 140466583250752] [num_dynamic_feat=auto] `dynamic_feat` field was found in the file `/opt/ml/input/data/train/train.json` and will be used for training.\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:43:14 INFO 140466583250752] [num_dynamic_feat=auto] Inferred value of num_dynamic_feat=85 from dataset.\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:43:14 INFO 140466583250752] Training set statistics:\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:43:14 INFO 140466583250752] Real time series\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:43:14 INFO 140466583250752] number of time series: 1\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:43:14 INFO 140466583250752] number of observations: 4738\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:43:14 INFO 140466583250752] mean target length: 4738\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:43:14 INFO 140466583250752] min/mean/max target: 0.0/0.277265375603/0.686148226261\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:43:14 INFO 140466583250752] mean abs(target): 0.277265375603\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:43:14 INFO 140466583250752] contains missing values: no\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:43:14 INFO 140466583250752] Small number of time series. Doing 1280 passes over dataset with prob 1.0 per epoch.\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:43:14 INFO 140466583250752] Test set statistics:\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:43:14 INFO 140466583250752] Real time series\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:43:14 INFO 140466583250752] number of time series: 1\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:43:14 INFO 140466583250752] number of observations: 263\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:43:14 INFO 140466583250752] mean target length: 263\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:43:14 INFO 140466583250752] min/mean/max target: 0.627243101597/0.675164632471/0.752329289913\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:43:14 INFO 140466583250752] mean abs(target): 0.675164632471\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:43:14 INFO 140466583250752] contains missing values: no\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:43:14 INFO 140466583250752] nvidia-smi took: 0.0754489898682 secs to identify 1 gpus\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:43:14 INFO 140466583250752] Number of GPUs being used: 1\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:43:14 INFO 140466583250752] Create Store: local\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"get_graph.time\": {\"count\": 1, \"max\": 48.586130142211914, \"sum\": 48.586130142211914, \"min\": 48.586130142211914}}, \"EndTime\": 1593877394.194031, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593877394.144492}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:43:14 INFO 140466583250752] Number of GPUs being used: 1\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"initialize.time\": {\"count\": 1, \"max\": 3019.300937652588, \"sum\": 3019.300937652588, \"min\": 3019.300937652588}}, \"EndTime\": 1593877397.163924, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593877394.194113}\n",
      "\u001b[0m\n",
      "\n",
      "2020-07-04 15:43:11 Training - Training image download completed. Training in progress.\u001b[34m[07/04/2020 15:44:05 INFO 140466583250752] Epoch[0] Batch[0] avg_epoch_loss=-0.147913\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:44:05 INFO 140466583250752] #quality_metric: host=algo-1, epoch=0, batch=0 train loss <loss>=-0.147912666202\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:44:06 INFO 140466583250752] Epoch[0] Batch[5] avg_epoch_loss=-0.967370\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:44:06 INFO 140466583250752] #quality_metric: host=algo-1, epoch=0, batch=5 train loss <loss>=-0.967369653285\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:44:06 INFO 140466583250752] Epoch[0] Batch [5]#011Speed: 1797.14 samples/sec#011loss=-0.967370\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:44:06 INFO 140466583250752] processed a total of 1244 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"epochs\": {\"count\": 1, \"max\": 200, \"sum\": 200.0, \"min\": 200}, \"update.time\": {\"count\": 1, \"max\": 49508.12792778015, \"sum\": 49508.12792778015, \"min\": 49508.12792778015}}, \"EndTime\": 1593877446.672228, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593877397.163996}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:44:06 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=25.127122737 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:44:06 INFO 140466583250752] #progress_metric: host=algo-1, completed 0 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:44:06 INFO 140466583250752] #quality_metric: host=algo-1, epoch=0, train loss <loss>=-1.05648683161\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:44:06 INFO 140466583250752] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:44:06 INFO 140466583250752] Saved checkpoint to \"/opt/ml/model/state_3acea9c1-114f-4556-bd2a-085115e4f3d8-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 8.832931518554688, \"sum\": 8.832931518554688, \"min\": 8.832931518554688}}, \"EndTime\": 1593877446.681813, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593877446.672311}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:44:56 INFO 140466583250752] Epoch[1] Batch[0] avg_epoch_loss=-1.512530\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:44:56 INFO 140466583250752] #quality_metric: host=algo-1, epoch=1, batch=0 train loss <loss>=-1.51252985001\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:44:56 INFO 140466583250752] Epoch[1] Batch[5] avg_epoch_loss=-1.590717\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:44:56 INFO 140466583250752] #quality_metric: host=algo-1, epoch=1, batch=5 train loss <loss>=-1.59071662029\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:44:56 INFO 140466583250752] Epoch[1] Batch [5]#011Speed: 1815.07 samples/sec#011loss=-1.590717\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:44:57 INFO 140466583250752] Epoch[1] Batch[10] avg_epoch_loss=-1.808334\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:44:57 INFO 140466583250752] #quality_metric: host=algo-1, epoch=1, batch=10 train loss <loss>=-2.06947579384\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:44:57 INFO 140466583250752] Epoch[1] Batch [10]#011Speed: 1113.44 samples/sec#011loss=-2.069476\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:44:57 INFO 140466583250752] processed a total of 1298 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 50568.05205345154, \"sum\": 50568.05205345154, \"min\": 50568.05205345154}}, \"EndTime\": 1593877497.250003, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593877446.681886}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:44:57 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=25.6683119773 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:44:57 INFO 140466583250752] #progress_metric: host=algo-1, completed 1 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:44:57 INFO 140466583250752] #quality_metric: host=algo-1, epoch=1, train loss <loss>=-1.80833442645\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:44:57 INFO 140466583250752] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:44:57 INFO 140466583250752] Saved checkpoint to \"/opt/ml/model/state_039d7462-141d-4e0d-8b77-ccee9b3460ee-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 8.48388671875, \"sum\": 8.48388671875, \"min\": 8.48388671875}}, \"EndTime\": 1593877497.259172, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593877497.250096}\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[07/04/2020 15:45:46 INFO 140466583250752] Epoch[2] Batch[0] avg_epoch_loss=-2.384125\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:45:46 INFO 140466583250752] #quality_metric: host=algo-1, epoch=2, batch=0 train loss <loss>=-2.38412475586\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:45:46 INFO 140466583250752] Epoch[2] Batch[5] avg_epoch_loss=-2.447776\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:45:46 INFO 140466583250752] #quality_metric: host=algo-1, epoch=2, batch=5 train loss <loss>=-2.44777584076\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:45:46 INFO 140466583250752] Epoch[2] Batch [5]#011Speed: 1797.34 samples/sec#011loss=-2.447776\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:45:47 INFO 140466583250752] Epoch[2] Batch[10] avg_epoch_loss=-2.563244\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:45:47 INFO 140466583250752] #quality_metric: host=algo-1, epoch=2, batch=10 train loss <loss>=-2.7018055439\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:45:47 INFO 140466583250752] Epoch[2] Batch [10]#011Speed: 1219.95 samples/sec#011loss=-2.701806\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:45:47 INFO 140466583250752] processed a total of 1332 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 50239.95780944824, \"sum\": 50239.95780944824, \"min\": 50239.95780944824}}, \"EndTime\": 1593877547.499286, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593877497.259262}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:45:47 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=26.5126886469 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:45:47 INFO 140466583250752] #progress_metric: host=algo-1, completed 1 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:45:47 INFO 140466583250752] #quality_metric: host=algo-1, epoch=2, train loss <loss>=-2.56324388764\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:45:47 INFO 140466583250752] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:45:47 INFO 140466583250752] Saved checkpoint to \"/opt/ml/model/state_d5132a32-f7e1-4478-9e67-64c2e6b99fa8-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 7.822990417480469, \"sum\": 7.822990417480469, \"min\": 7.822990417480469}}, \"EndTime\": 1593877547.507805, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593877547.49938}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:46:36 INFO 140466583250752] Epoch[3] Batch[0] avg_epoch_loss=-2.933417\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:46:36 INFO 140466583250752] #quality_metric: host=algo-1, epoch=3, batch=0 train loss <loss>=-2.93341684341\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:46:36 INFO 140466583250752] Epoch[3] Batch[5] avg_epoch_loss=-2.715771\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:46:36 INFO 140466583250752] #quality_metric: host=algo-1, epoch=3, batch=5 train loss <loss>=-2.71577103933\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:46:36 INFO 140466583250752] Epoch[3] Batch [5]#011Speed: 1800.25 samples/sec#011loss=-2.715771\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:46:37 INFO 140466583250752] processed a total of 1262 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 49845.45111656189, \"sum\": 49845.45111656189, \"min\": 49845.45111656189}}, \"EndTime\": 1593877597.353406, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593877547.507891}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:46:37 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=25.3181916862 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:46:37 INFO 140466583250752] #progress_metric: host=algo-1, completed 2 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:46:37 INFO 140466583250752] #quality_metric: host=algo-1, epoch=3, train loss <loss>=-2.84023149014\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:46:37 INFO 140466583250752] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:46:37 INFO 140466583250752] Saved checkpoint to \"/opt/ml/model/state_271c30a7-2f6a-47d7-b282-a0704fe27ced-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 7.9860687255859375, \"sum\": 7.9860687255859375, \"min\": 7.9860687255859375}}, \"EndTime\": 1593877597.362131, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593877597.353494}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:47:26 INFO 140466583250752] Epoch[4] Batch[0] avg_epoch_loss=-3.079201\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:47:26 INFO 140466583250752] #quality_metric: host=algo-1, epoch=4, batch=0 train loss <loss>=-3.07920074463\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:47:26 INFO 140466583250752] Epoch[4] Batch[5] avg_epoch_loss=-3.089570\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:47:26 INFO 140466583250752] #quality_metric: host=algo-1, epoch=4, batch=5 train loss <loss>=-3.08957008521\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:47:26 INFO 140466583250752] Epoch[4] Batch [5]#011Speed: 1837.49 samples/sec#011loss=-3.089570\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:47:27 INFO 140466583250752] processed a total of 1263 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 49942.45100021362, \"sum\": 49942.45100021362, \"min\": 49942.45100021362}}, \"EndTime\": 1593877647.304713, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593877597.362205}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:47:27 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=25.2890606561 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:47:27 INFO 140466583250752] #progress_metric: host=algo-1, completed 2 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:47:27 INFO 140466583250752] #quality_metric: host=algo-1, epoch=4, train loss <loss>=-3.15256943703\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:47:27 INFO 140466583250752] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:47:27 INFO 140466583250752] Saved checkpoint to \"/opt/ml/model/state_9894f127-7333-47aa-a272-7b1e129092ec-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 8.049964904785156, \"sum\": 8.049964904785156, \"min\": 8.049964904785156}}, \"EndTime\": 1593877647.313477, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593877647.304774}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:48:16 INFO 140466583250752] Epoch[5] Batch[0] avg_epoch_loss=-3.392435\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:48:16 INFO 140466583250752] #quality_metric: host=algo-1, epoch=5, batch=0 train loss <loss>=-3.39243507385\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:48:16 INFO 140466583250752] Epoch[5] Batch[5] avg_epoch_loss=-3.386856\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:48:16 INFO 140466583250752] #quality_metric: host=algo-1, epoch=5, batch=5 train loss <loss>=-3.38685627778\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:48:16 INFO 140466583250752] Epoch[5] Batch [5]#011Speed: 1812.25 samples/sec#011loss=-3.386856\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:48:17 INFO 140466583250752] Epoch[5] Batch[10] avg_epoch_loss=-3.283360\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:48:17 INFO 140466583250752] #quality_metric: host=algo-1, epoch=5, batch=10 train loss <loss>=-3.15916361809\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:48:17 INFO 140466583250752] Epoch[5] Batch [10]#011Speed: 1125.76 samples/sec#011loss=-3.159164\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:48:17 INFO 140466583250752] processed a total of 1305 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 50251.76286697388, \"sum\": 50251.76286697388, \"min\": 50251.76286697388}}, \"EndTime\": 1593877697.56537, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593877647.313543}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:48:17 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=25.9691786922 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:48:17 INFO 140466583250752] #progress_metric: host=algo-1, completed 3 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:48:17 INFO 140466583250752] #quality_metric: host=algo-1, epoch=5, train loss <loss>=-3.28335961429\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:48:17 INFO 140466583250752] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:48:17 INFO 140466583250752] Saved checkpoint to \"/opt/ml/model/state_c9086fa7-071d-4090-96aa-dd996b45682c-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 8.28409194946289, \"sum\": 8.28409194946289, \"min\": 8.28409194946289}}, \"EndTime\": 1593877697.574269, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593877697.565447}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:49:06 INFO 140466583250752] Epoch[6] Batch[0] avg_epoch_loss=-3.123408\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:49:06 INFO 140466583250752] #quality_metric: host=algo-1, epoch=6, batch=0 train loss <loss>=-3.12340760231\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:49:07 INFO 140466583250752] Epoch[6] Batch[5] avg_epoch_loss=-3.315492\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:49:07 INFO 140466583250752] #quality_metric: host=algo-1, epoch=6, batch=5 train loss <loss>=-3.31549171607\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:49:07 INFO 140466583250752] Epoch[6] Batch [5]#011Speed: 1823.69 samples/sec#011loss=-3.315492\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:49:07 INFO 140466583250752] Epoch[6] Batch[10] avg_epoch_loss=-3.413715\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:49:07 INFO 140466583250752] #quality_metric: host=algo-1, epoch=6, batch=10 train loss <loss>=-3.53158373833\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:49:07 INFO 140466583250752] Epoch[6] Batch [10]#011Speed: 1138.17 samples/sec#011loss=-3.531584\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:49:07 INFO 140466583250752] processed a total of 1305 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 50072.84188270569, \"sum\": 50072.84188270569, \"min\": 50072.84188270569}}, \"EndTime\": 1593877747.647244, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593877697.574338}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:49:07 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=26.0619693081 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:49:07 INFO 140466583250752] #progress_metric: host=algo-1, completed 3 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:49:07 INFO 140466583250752] #quality_metric: host=algo-1, epoch=6, train loss <loss>=-3.41371536255\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:49:07 INFO 140466583250752] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:49:07 INFO 140466583250752] Saved checkpoint to \"/opt/ml/model/state_e6d427a1-ff7f-438e-a084-d837386e7272-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 7.997035980224609, \"sum\": 7.997035980224609, \"min\": 7.997035980224609}}, \"EndTime\": 1593877747.655932, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593877747.647326}\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[07/04/2020 15:49:56 INFO 140466583250752] Epoch[7] Batch[0] avg_epoch_loss=-3.598943\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:49:56 INFO 140466583250752] #quality_metric: host=algo-1, epoch=7, batch=0 train loss <loss>=-3.59894275665\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:49:57 INFO 140466583250752] Epoch[7] Batch[5] avg_epoch_loss=-3.533870\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:49:57 INFO 140466583250752] #quality_metric: host=algo-1, epoch=7, batch=5 train loss <loss>=-3.5338704586\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:49:57 INFO 140466583250752] Epoch[7] Batch [5]#011Speed: 1833.60 samples/sec#011loss=-3.533870\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:49:57 INFO 140466583250752] processed a total of 1246 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 49802.136182785034, \"sum\": 49802.136182785034, \"min\": 49802.136182785034}}, \"EndTime\": 1593877797.458199, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593877747.655999}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:49:57 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=25.0189509512 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:49:57 INFO 140466583250752] #progress_metric: host=algo-1, completed 4 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:49:57 INFO 140466583250752] #quality_metric: host=algo-1, epoch=7, train loss <loss>=-3.52354736328\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:49:57 INFO 140466583250752] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:49:57 INFO 140466583250752] Saved checkpoint to \"/opt/ml/model/state_21e02745-18d9-40ad-a49c-35fe8e3db7db-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 7.803201675415039, \"sum\": 7.803201675415039, \"min\": 7.803201675415039}}, \"EndTime\": 1593877797.466659, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593877797.458271}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:50:46 INFO 140466583250752] Epoch[8] Batch[0] avg_epoch_loss=-3.050382\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:50:46 INFO 140466583250752] #quality_metric: host=algo-1, epoch=8, batch=0 train loss <loss>=-3.0503821373\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:50:47 INFO 140466583250752] Epoch[8] Batch[5] avg_epoch_loss=-3.411710\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:50:47 INFO 140466583250752] #quality_metric: host=algo-1, epoch=8, batch=5 train loss <loss>=-3.41171042124\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:50:47 INFO 140466583250752] Epoch[8] Batch [5]#011Speed: 1814.36 samples/sec#011loss=-3.411710\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:50:47 INFO 140466583250752] processed a total of 1237 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 50160.94183921814, \"sum\": 50160.94183921814, \"min\": 50160.94183921814}}, \"EndTime\": 1593877847.627731, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593877797.466728}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:50:47 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=24.6605604162 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:50:47 INFO 140466583250752] #progress_metric: host=algo-1, completed 4 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:50:47 INFO 140466583250752] #quality_metric: host=algo-1, epoch=8, train loss <loss>=-3.45372514725\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:50:47 INFO 140466583250752] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:51:37 INFO 140466583250752] Epoch[9] Batch[0] avg_epoch_loss=-3.332080\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:51:37 INFO 140466583250752] #quality_metric: host=algo-1, epoch=9, batch=0 train loss <loss>=-3.33208036423\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:51:37 INFO 140466583250752] Epoch[9] Batch[5] avg_epoch_loss=-3.417775\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:51:37 INFO 140466583250752] #quality_metric: host=algo-1, epoch=9, batch=5 train loss <loss>=-3.41777483622\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:51:37 INFO 140466583250752] Epoch[9] Batch [5]#011Speed: 1828.86 samples/sec#011loss=-3.417775\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:51:37 INFO 140466583250752] processed a total of 1242 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 50232.760190963745, \"sum\": 50232.760190963745, \"min\": 50232.760190963745}}, \"EndTime\": 1593877897.861191, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593877847.627817}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:51:37 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=24.7248336585 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:51:37 INFO 140466583250752] #progress_metric: host=algo-1, completed 5 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:51:37 INFO 140466583250752] #quality_metric: host=algo-1, epoch=9, train loss <loss>=-3.48588352203\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:51:37 INFO 140466583250752] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:52:27 INFO 140466583250752] Epoch[10] Batch[0] avg_epoch_loss=-3.682214\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:52:27 INFO 140466583250752] #quality_metric: host=algo-1, epoch=10, batch=0 train loss <loss>=-3.68221402168\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:52:27 INFO 140466583250752] Epoch[10] Batch[5] avg_epoch_loss=-3.580545\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:52:27 INFO 140466583250752] #quality_metric: host=algo-1, epoch=10, batch=5 train loss <loss>=-3.58054459095\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:52:27 INFO 140466583250752] Epoch[10] Batch [5]#011Speed: 1839.78 samples/sec#011loss=-3.580545\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:52:27 INFO 140466583250752] processed a total of 1241 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 49933.526039123535, \"sum\": 49933.526039123535, \"min\": 49933.526039123535}}, \"EndTime\": 1593877947.795304, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593877897.861286}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:52:27 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=24.8529839306 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:52:27 INFO 140466583250752] #progress_metric: host=algo-1, completed 5 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:52:27 INFO 140466583250752] #quality_metric: host=algo-1, epoch=10, train loss <loss>=-3.58882753849\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:52:27 INFO 140466583250752] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:52:27 INFO 140466583250752] Saved checkpoint to \"/opt/ml/model/state_3a1ee532-0a0c-4b9c-adbd-bf11b25b6b01-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 7.732868194580078, \"sum\": 7.732868194580078, \"min\": 7.732868194580078}}, \"EndTime\": 1593877947.803801, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593877947.795377}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:54:57 INFO 140466583250752] Epoch[13] Batch[0] avg_epoch_loss=-3.660936\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:54:57 INFO 140466583250752] #quality_metric: host=algo-1, epoch=13, batch=0 train loss <loss>=-3.66093587875\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:54:57 INFO 140466583250752] Epoch[13] Batch[5] avg_epoch_loss=-3.473712\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:54:57 INFO 140466583250752] #quality_metric: host=algo-1, epoch=13, batch=5 train loss <loss>=-3.47371244431\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:54:57 INFO 140466583250752] Epoch[13] Batch [5]#011Speed: 1833.45 samples/sec#011loss=-3.473712\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:54:58 INFO 140466583250752] Epoch[13] Batch[10] avg_epoch_loss=-3.532273\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:54:58 INFO 140466583250752] #quality_metric: host=algo-1, epoch=13, batch=10 train loss <loss>=-3.60254559517\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:54:58 INFO 140466583250752] Epoch[13] Batch [10]#011Speed: 1260.96 samples/sec#011loss=-3.602546\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:54:58 INFO 140466583250752] processed a total of 1362 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 50685.35280227661, \"sum\": 50685.35280227661, \"min\": 50685.35280227661}}, \"EndTime\": 1593878098.317304, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593878047.631898}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:54:58 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=26.8716053328 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:54:58 INFO 140466583250752] #progress_metric: host=algo-1, completed 7 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:54:58 INFO 140466583250752] #quality_metric: host=algo-1, epoch=13, train loss <loss>=-3.53227296743\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:54:58 INFO 140466583250752] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:55:47 INFO 140466583250752] Epoch[14] Batch[0] avg_epoch_loss=-3.568001\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:55:47 INFO 140466583250752] #quality_metric: host=algo-1, epoch=14, batch=0 train loss <loss>=-3.56800127029\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:55:48 INFO 140466583250752] Epoch[14] Batch[5] avg_epoch_loss=-3.622276\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:55:48 INFO 140466583250752] #quality_metric: host=algo-1, epoch=14, batch=5 train loss <loss>=-3.62227626642\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:55:48 INFO 140466583250752] Epoch[14] Batch [5]#011Speed: 1818.09 samples/sec#011loss=-3.622276\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:55:48 INFO 140466583250752] Epoch[14] Batch[10] avg_epoch_loss=-3.647399\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:55:48 INFO 140466583250752] #quality_metric: host=algo-1, epoch=14, batch=10 train loss <loss>=-3.67754635811\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:55:48 INFO 140466583250752] Epoch[14] Batch [10]#011Speed: 1116.71 samples/sec#011loss=-3.677546\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:55:48 INFO 140466583250752] processed a total of 1299 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 50511.833906173706, \"sum\": 50511.833906173706, \"min\": 50511.833906173706}}, \"EndTime\": 1593878148.829718, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593878098.317387}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:55:48 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=25.7166806447 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:55:48 INFO 140466583250752] #progress_metric: host=algo-1, completed 7 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:55:48 INFO 140466583250752] #quality_metric: host=algo-1, epoch=14, train loss <loss>=-3.64739903537\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:55:48 INFO 140466583250752] loss did not improve\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[07/04/2020 15:56:38 INFO 140466583250752] Epoch[15] Batch[0] avg_epoch_loss=-3.654280\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:56:38 INFO 140466583250752] #quality_metric: host=algo-1, epoch=15, batch=0 train loss <loss>=-3.65428042412\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:56:38 INFO 140466583250752] Epoch[15] Batch[5] avg_epoch_loss=-3.755961\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:56:38 INFO 140466583250752] #quality_metric: host=algo-1, epoch=15, batch=5 train loss <loss>=-3.75596090158\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:56:38 INFO 140466583250752] Epoch[15] Batch [5]#011Speed: 1823.56 samples/sec#011loss=-3.755961\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:56:39 INFO 140466583250752] processed a total of 1260 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 50366.43409729004, \"sum\": 50366.43409729004, \"min\": 50366.43409729004}}, \"EndTime\": 1593878199.196779, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593878148.829805}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:56:39 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=25.0166067311 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:56:39 INFO 140466583250752] #progress_metric: host=algo-1, completed 8 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:56:39 INFO 140466583250752] #quality_metric: host=algo-1, epoch=15, train loss <loss>=-3.76893806458\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:56:39 INFO 140466583250752] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:56:39 INFO 140466583250752] Saved checkpoint to \"/opt/ml/model/state_99cf728a-34e8-4a8b-b8bd-d0537246cccc-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 8.012056350708008, \"sum\": 8.012056350708008, \"min\": 8.012056350708008}}, \"EndTime\": 1593878199.205427, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593878199.196849}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:57:30 INFO 140466583250752] Epoch[16] Batch[0] avg_epoch_loss=-3.978217\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:57:30 INFO 140466583250752] #quality_metric: host=algo-1, epoch=16, batch=0 train loss <loss>=-3.97821736336\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:57:31 INFO 140466583250752] Epoch[16] Batch[5] avg_epoch_loss=-3.832343\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:57:31 INFO 140466583250752] #quality_metric: host=algo-1, epoch=16, batch=5 train loss <loss>=-3.83234314124\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:57:31 INFO 140466583250752] Epoch[16] Batch [5]#011Speed: 1756.48 samples/sec#011loss=-3.832343\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:57:31 INFO 140466583250752] Epoch[16] Batch[10] avg_epoch_loss=-3.798602\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:57:31 INFO 140466583250752] #quality_metric: host=algo-1, epoch=16, batch=10 train loss <loss>=-3.75811166763\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:57:31 INFO 140466583250752] Epoch[16] Batch [10]#011Speed: 1269.10 samples/sec#011loss=-3.758112\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:57:31 INFO 140466583250752] processed a total of 1350 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52614.72797393799, \"sum\": 52614.72797393799, \"min\": 52614.72797393799}}, \"EndTime\": 1593878251.820295, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593878199.205501}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:57:31 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=25.6581520514 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:57:31 INFO 140466583250752] #progress_metric: host=algo-1, completed 8 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:57:31 INFO 140466583250752] #quality_metric: host=algo-1, epoch=16, train loss <loss>=-3.79860156233\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:57:31 INFO 140466583250752] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:57:31 INFO 140466583250752] Saved checkpoint to \"/opt/ml/model/state_c1b0e61e-b553-4b8d-8a5d-05d1fa4cd6d2-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 8.206844329833984, \"sum\": 8.206844329833984, \"min\": 8.206844329833984}}, \"EndTime\": 1593878251.829152, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593878251.820383}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:58:23 INFO 140466583250752] Epoch[17] Batch[0] avg_epoch_loss=-3.473650\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:58:23 INFO 140466583250752] #quality_metric: host=algo-1, epoch=17, batch=0 train loss <loss>=-3.47364997864\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:58:23 INFO 140466583250752] Epoch[17] Batch[5] avg_epoch_loss=-3.787825\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:58:23 INFO 140466583250752] #quality_metric: host=algo-1, epoch=17, batch=5 train loss <loss>=-3.78782518705\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:58:23 INFO 140466583250752] Epoch[17] Batch [5]#011Speed: 1810.73 samples/sec#011loss=-3.787825\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:58:23 INFO 140466583250752] Epoch[17] Batch[10] avg_epoch_loss=-3.868074\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:58:23 INFO 140466583250752] #quality_metric: host=algo-1, epoch=17, batch=10 train loss <loss>=-3.96437215805\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:58:23 INFO 140466583250752] Epoch[17] Batch [10]#011Speed: 1171.30 samples/sec#011loss=-3.964372\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:58:23 INFO 140466583250752] processed a total of 1311 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52164.007902145386, \"sum\": 52164.007902145386, \"min\": 52164.007902145386}}, \"EndTime\": 1593878303.993298, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593878251.829226}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:58:23 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=25.1322057353 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:58:23 INFO 140466583250752] #progress_metric: host=algo-1, completed 9 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:58:23 INFO 140466583250752] #quality_metric: host=algo-1, epoch=17, train loss <loss>=-3.86807381023\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:58:23 INFO 140466583250752] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:58:24 INFO 140466583250752] Saved checkpoint to \"/opt/ml/model/state_75ab4d46-e87c-42fb-885c-0773ad7c72bc-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 7.8449249267578125, \"sum\": 7.8449249267578125, \"min\": 7.8449249267578125}}, \"EndTime\": 1593878304.001842, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593878303.993393}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:59:15 INFO 140466583250752] Epoch[18] Batch[0] avg_epoch_loss=-3.956352\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:59:15 INFO 140466583250752] #quality_metric: host=algo-1, epoch=18, batch=0 train loss <loss>=-3.95635223389\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:59:15 INFO 140466583250752] Epoch[18] Batch[5] avg_epoch_loss=-3.880514\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:59:15 INFO 140466583250752] #quality_metric: host=algo-1, epoch=18, batch=5 train loss <loss>=-3.88051398595\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:59:15 INFO 140466583250752] Epoch[18] Batch [5]#011Speed: 1826.29 samples/sec#011loss=-3.880514\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:59:16 INFO 140466583250752] Epoch[18] Batch[10] avg_epoch_loss=-3.942839\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:59:16 INFO 140466583250752] #quality_metric: host=algo-1, epoch=18, batch=10 train loss <loss>=-4.01762824059\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:59:16 INFO 140466583250752] Epoch[18] Batch [10]#011Speed: 1111.66 samples/sec#011loss=-4.017628\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:59:16 INFO 140466583250752] processed a total of 1289 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52418.28393936157, \"sum\": 52418.28393936157, \"min\": 52418.28393936157}}, \"EndTime\": 1593878356.420266, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593878304.001914}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:59:16 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=24.5905966054 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:59:16 INFO 140466583250752] #progress_metric: host=algo-1, completed 9 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:59:16 INFO 140466583250752] #quality_metric: host=algo-1, epoch=18, train loss <loss>=-3.94283864715\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:59:16 INFO 140466583250752] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[07/04/2020 15:59:16 INFO 140466583250752] Saved checkpoint to \"/opt/ml/model/state_55fe93fd-8c27-4f69-8f55-a7d7230c58b3-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 8.273124694824219, \"sum\": 8.273124694824219, \"min\": 8.273124694824219}}, \"EndTime\": 1593878356.42922, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593878356.420351}\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[07/04/2020 16:00:08 INFO 140466583250752] Epoch[19] Batch[0] avg_epoch_loss=-3.449018\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:00:08 INFO 140466583250752] #quality_metric: host=algo-1, epoch=19, batch=0 train loss <loss>=-3.44901847839\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:00:08 INFO 140466583250752] Epoch[19] Batch[5] avg_epoch_loss=-3.583866\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:00:08 INFO 140466583250752] #quality_metric: host=algo-1, epoch=19, batch=5 train loss <loss>=-3.58386600018\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:00:08 INFO 140466583250752] Epoch[19] Batch [5]#011Speed: 1824.76 samples/sec#011loss=-3.583866\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:00:08 INFO 140466583250752] processed a total of 1255 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52459.25283432007, \"sum\": 52459.25283432007, \"min\": 52459.25283432007}}, \"EndTime\": 1593878408.888612, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593878356.429296}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:00:08 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=23.9232717222 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:00:08 INFO 140466583250752] #progress_metric: host=algo-1, completed 10 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:00:08 INFO 140466583250752] #quality_metric: host=algo-1, epoch=19, train loss <loss>=-3.56122760773\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:00:08 INFO 140466583250752] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:01:00 INFO 140466583250752] Epoch[20] Batch[0] avg_epoch_loss=-3.873794\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:01:00 INFO 140466583250752] #quality_metric: host=algo-1, epoch=20, batch=0 train loss <loss>=-3.87379384041\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:01:00 INFO 140466583250752] Epoch[20] Batch[5] avg_epoch_loss=-3.765912\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:01:00 INFO 140466583250752] #quality_metric: host=algo-1, epoch=20, batch=5 train loss <loss>=-3.76591189702\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:01:00 INFO 140466583250752] Epoch[20] Batch [5]#011Speed: 1809.99 samples/sec#011loss=-3.765912\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:01:01 INFO 140466583250752] Epoch[20] Batch[10] avg_epoch_loss=-3.723780\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:01:01 INFO 140466583250752] #quality_metric: host=algo-1, epoch=20, batch=10 train loss <loss>=-3.67322278023\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:01:01 INFO 140466583250752] Epoch[20] Batch [10]#011Speed: 1190.73 samples/sec#011loss=-3.673223\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:01:01 INFO 140466583250752] processed a total of 1323 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52389.556884765625, \"sum\": 52389.556884765625, \"min\": 52389.556884765625}}, \"EndTime\": 1593878461.278828, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593878408.888698}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:01:01 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=25.253071817 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:01:01 INFO 140466583250752] #progress_metric: host=algo-1, completed 10 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:01:01 INFO 140466583250752] #quality_metric: host=algo-1, epoch=20, train loss <loss>=-3.7237804803\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:01:01 INFO 140466583250752] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:01:53 INFO 140466583250752] Epoch[21] Batch[0] avg_epoch_loss=-3.908938\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:01:53 INFO 140466583250752] #quality_metric: host=algo-1, epoch=21, batch=0 train loss <loss>=-3.90893793106\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:01:53 INFO 140466583250752] Epoch[21] Batch[5] avg_epoch_loss=-3.806250\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:01:53 INFO 140466583250752] #quality_metric: host=algo-1, epoch=21, batch=5 train loss <loss>=-3.80624961853\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:01:53 INFO 140466583250752] Epoch[21] Batch [5]#011Speed: 1813.32 samples/sec#011loss=-3.806250\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:01:53 INFO 140466583250752] Epoch[21] Batch[10] avg_epoch_loss=-3.816371\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:01:53 INFO 140466583250752] #quality_metric: host=algo-1, epoch=21, batch=10 train loss <loss>=-3.82851653099\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:01:53 INFO 140466583250752] Epoch[21] Batch [10]#011Speed: 1198.30 samples/sec#011loss=-3.828517\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:01:53 INFO 140466583250752] processed a total of 1322 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52665.20619392395, \"sum\": 52665.20619392395, \"min\": 52665.20619392395}}, \"EndTime\": 1593878513.944574, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593878461.278901}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:01:53 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=25.1019038437 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:01:53 INFO 140466583250752] #progress_metric: host=algo-1, completed 11 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:01:53 INFO 140466583250752] #quality_metric: host=algo-1, epoch=21, train loss <loss>=-3.81637094238\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:01:53 INFO 140466583250752] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:02:45 INFO 140466583250752] Epoch[22] Batch[0] avg_epoch_loss=-3.685392\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:02:45 INFO 140466583250752] #quality_metric: host=algo-1, epoch=22, batch=0 train loss <loss>=-3.68539237976\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:02:45 INFO 140466583250752] Epoch[22] Batch[5] avg_epoch_loss=-3.782125\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:02:45 INFO 140466583250752] #quality_metric: host=algo-1, epoch=22, batch=5 train loss <loss>=-3.78212475777\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:02:45 INFO 140466583250752] Epoch[22] Batch [5]#011Speed: 1771.65 samples/sec#011loss=-3.782125\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:02:46 INFO 140466583250752] processed a total of 1241 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52342.857122421265, \"sum\": 52342.857122421265, \"min\": 52342.857122421265}}, \"EndTime\": 1593878566.288003, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593878513.944653}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:02:46 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=23.7090096319 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:02:46 INFO 140466583250752] #progress_metric: host=algo-1, completed 11 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:02:46 INFO 140466583250752] #quality_metric: host=algo-1, epoch=22, train loss <loss>=-3.79122247696\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:02:46 INFO 140466583250752] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:03:37 INFO 140466583250752] Epoch[23] Batch[0] avg_epoch_loss=-3.877869\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:03:37 INFO 140466583250752] #quality_metric: host=algo-1, epoch=23, batch=0 train loss <loss>=-3.87786912918\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:03:38 INFO 140466583250752] Epoch[23] Batch[5] avg_epoch_loss=-3.914544\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:03:38 INFO 140466583250752] #quality_metric: host=algo-1, epoch=23, batch=5 train loss <loss>=-3.91454414527\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:03:38 INFO 140466583250752] Epoch[23] Batch [5]#011Speed: 1756.41 samples/sec#011loss=-3.914544\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:03:38 INFO 140466583250752] processed a total of 1274 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52419.530153274536, \"sum\": 52419.530153274536, \"min\": 52419.530153274536}}, \"EndTime\": 1593878618.708182, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593878566.288076}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:03:38 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=24.303861788 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:03:38 INFO 140466583250752] #progress_metric: host=algo-1, completed 12 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:03:38 INFO 140466583250752] #quality_metric: host=algo-1, epoch=23, train loss <loss>=-3.94615833759\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:03:38 INFO 140466583250752] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:03:38 INFO 140466583250752] Saved checkpoint to \"/opt/ml/model/state_30ed4d13-0115-4694-9dbc-aff9cf168b22-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 7.8411102294921875, \"sum\": 7.8411102294921875, \"min\": 7.8411102294921875}}, \"EndTime\": 1593878618.716672, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593878618.70826}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:04:30 INFO 140466583250752] Epoch[24] Batch[0] avg_epoch_loss=-4.031426\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:04:30 INFO 140466583250752] #quality_metric: host=algo-1, epoch=24, batch=0 train loss <loss>=-4.03142642975\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:04:30 INFO 140466583250752] Epoch[24] Batch[5] avg_epoch_loss=-3.918330\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:04:30 INFO 140466583250752] #quality_metric: host=algo-1, epoch=24, batch=5 train loss <loss>=-3.91832975547\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:04:30 INFO 140466583250752] Epoch[24] Batch [5]#011Speed: 1815.80 samples/sec#011loss=-3.918330\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:04:31 INFO 140466583250752] Epoch[24] Batch[10] avg_epoch_loss=-4.031841\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:04:31 INFO 140466583250752] #quality_metric: host=algo-1, epoch=24, batch=10 train loss <loss>=-4.16805386543\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:04:31 INFO 140466583250752] Epoch[24] Batch [10]#011Speed: 1106.01 samples/sec#011loss=-4.168054\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:04:31 INFO 140466583250752] processed a total of 1295 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52545.07112503052, \"sum\": 52545.07112503052, \"min\": 52545.07112503052}}, \"EndTime\": 1593878671.261877, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593878618.716744}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:04:31 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=24.6454589052 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:04:31 INFO 140466583250752] #progress_metric: host=algo-1, completed 12 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:04:31 INFO 140466583250752] #quality_metric: host=algo-1, epoch=24, train loss <loss>=-4.03184071454\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:04:31 INFO 140466583250752] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:04:31 INFO 140466583250752] Saved checkpoint to \"/opt/ml/model/state_cfd18fe6-e52b-4756-b082-d79aafb3f3da-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 8.049964904785156, \"sum\": 8.049964904785156, \"min\": 8.049964904785156}}, \"EndTime\": 1593878671.270595, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593878671.261944}\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[07/04/2020 16:05:22 INFO 140466583250752] Epoch[25] Batch[0] avg_epoch_loss=-3.360052\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:05:22 INFO 140466583250752] #quality_metric: host=algo-1, epoch=25, batch=0 train loss <loss>=-3.36005163193\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:05:23 INFO 140466583250752] Epoch[25] Batch[5] avg_epoch_loss=-3.580284\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:05:23 INFO 140466583250752] #quality_metric: host=algo-1, epoch=25, batch=5 train loss <loss>=-3.58028403918\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:05:23 INFO 140466583250752] Epoch[25] Batch [5]#011Speed: 1742.03 samples/sec#011loss=-3.580284\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:05:23 INFO 140466583250752] Epoch[25] Batch[10] avg_epoch_loss=-3.586229\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:05:23 INFO 140466583250752] #quality_metric: host=algo-1, epoch=25, batch=10 train loss <loss>=-3.59336228371\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:05:23 INFO 140466583250752] Epoch[25] Batch [10]#011Speed: 1124.55 samples/sec#011loss=-3.593362\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:05:23 INFO 140466583250752] processed a total of 1304 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52503.796100616455, \"sum\": 52503.796100616455, \"min\": 52503.796100616455}}, \"EndTime\": 1593878723.774535, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593878671.270672}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:05:23 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=24.8362379422 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:05:23 INFO 140466583250752] #progress_metric: host=algo-1, completed 13 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:05:23 INFO 140466583250752] #quality_metric: host=algo-1, epoch=25, train loss <loss>=-3.58622869578\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:05:23 INFO 140466583250752] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:06:15 INFO 140466583250752] Epoch[26] Batch[0] avg_epoch_loss=-3.952490\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:06:15 INFO 140466583250752] #quality_metric: host=algo-1, epoch=26, batch=0 train loss <loss>=-3.95248961449\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:06:15 INFO 140466583250752] Epoch[26] Batch[5] avg_epoch_loss=-3.964736\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:06:15 INFO 140466583250752] #quality_metric: host=algo-1, epoch=26, batch=5 train loss <loss>=-3.96473622322\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:06:15 INFO 140466583250752] Epoch[26] Batch [5]#011Speed: 1828.95 samples/sec#011loss=-3.964736\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:06:16 INFO 140466583250752] processed a total of 1272 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52477.813959121704, \"sum\": 52477.813959121704, \"min\": 52477.813959121704}}, \"EndTime\": 1593878776.252953, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593878723.774622}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:06:16 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=24.2387581571 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:06:16 INFO 140466583250752] #progress_metric: host=algo-1, completed 13 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:06:16 INFO 140466583250752] #quality_metric: host=algo-1, epoch=26, train loss <loss>=-3.95512981415\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:06:16 INFO 140466583250752] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:07:07 INFO 140466583250752] Epoch[27] Batch[0] avg_epoch_loss=-3.965613\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:07:07 INFO 140466583250752] #quality_metric: host=algo-1, epoch=27, batch=0 train loss <loss>=-3.96561288834\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:07:08 INFO 140466583250752] Epoch[27] Batch[5] avg_epoch_loss=-3.979415\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:07:08 INFO 140466583250752] #quality_metric: host=algo-1, epoch=27, batch=5 train loss <loss>=-3.97941486041\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:07:08 INFO 140466583250752] Epoch[27] Batch [5]#011Speed: 1811.85 samples/sec#011loss=-3.979415\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:07:08 INFO 140466583250752] Epoch[27] Batch[10] avg_epoch_loss=-3.956911\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:07:08 INFO 140466583250752] #quality_metric: host=algo-1, epoch=27, batch=10 train loss <loss>=-3.92990589142\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:07:08 INFO 140466583250752] Epoch[27] Batch [10]#011Speed: 1082.35 samples/sec#011loss=-3.929906\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:07:08 INFO 140466583250752] processed a total of 1284 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52435.075998306274, \"sum\": 52435.075998306274, \"min\": 52435.075998306274}}, \"EndTime\": 1593878828.688654, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593878776.25303}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:07:08 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=24.4873625996 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:07:08 INFO 140466583250752] #progress_metric: host=algo-1, completed 14 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:07:08 INFO 140466583250752] #quality_metric: host=algo-1, epoch=27, train loss <loss>=-3.95691078359\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:07:08 INFO 140466583250752] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:07:59 INFO 140466583250752] Epoch[28] Batch[0] avg_epoch_loss=-3.959818\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:07:59 INFO 140466583250752] #quality_metric: host=algo-1, epoch=28, batch=0 train loss <loss>=-3.95981812477\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:08:00 INFO 140466583250752] Epoch[28] Batch[5] avg_epoch_loss=-4.038840\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:08:00 INFO 140466583250752] #quality_metric: host=algo-1, epoch=28, batch=5 train loss <loss>=-4.03883985678\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:08:00 INFO 140466583250752] Epoch[28] Batch [5]#011Speed: 1802.79 samples/sec#011loss=-4.038840\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:08:00 INFO 140466583250752] processed a total of 1252 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 51993.776082992554, \"sum\": 51993.776082992554, \"min\": 51993.776082992554}}, \"EndTime\": 1593878880.683067, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593878828.688744}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:08:00 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=24.0797464635 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:08:00 INFO 140466583250752] #progress_metric: host=algo-1, completed 14 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:08:00 INFO 140466583250752] #quality_metric: host=algo-1, epoch=28, train loss <loss>=-4.02580561638\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:08:00 INFO 140466583250752] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:08:52 INFO 140466583250752] Epoch[29] Batch[0] avg_epoch_loss=-4.091868\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:08:52 INFO 140466583250752] #quality_metric: host=algo-1, epoch=29, batch=0 train loss <loss>=-4.09186840057\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:08:52 INFO 140466583250752] Epoch[29] Batch[5] avg_epoch_loss=-4.052060\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:08:52 INFO 140466583250752] #quality_metric: host=algo-1, epoch=29, batch=5 train loss <loss>=-4.05206024647\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:08:52 INFO 140466583250752] Epoch[29] Batch [5]#011Speed: 1816.58 samples/sec#011loss=-4.052060\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:08:53 INFO 140466583250752] Epoch[29] Batch[10] avg_epoch_loss=-4.035985\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:08:53 INFO 140466583250752] #quality_metric: host=algo-1, epoch=29, batch=10 train loss <loss>=-4.01669406891\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:08:53 INFO 140466583250752] Epoch[29] Batch [10]#011Speed: 1164.00 samples/sec#011loss=-4.016694\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:08:53 INFO 140466583250752] processed a total of 1318 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52350.86488723755, \"sum\": 52350.86488723755, \"min\": 52350.86488723755}}, \"EndTime\": 1593878933.034566, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593878880.683151}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:08:53 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=25.1762239963 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:08:53 INFO 140466583250752] #progress_metric: host=algo-1, completed 15 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:08:53 INFO 140466583250752] #quality_metric: host=algo-1, epoch=29, train loss <loss>=-4.03598471121\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:08:53 INFO 140466583250752] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:08:53 INFO 140466583250752] Saved checkpoint to \"/opt/ml/model/state_547d135b-d54b-4058-b9f8-b65d5be4cfc3-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 8.08095932006836, \"sum\": 8.08095932006836, \"min\": 8.08095932006836}}, \"EndTime\": 1593878933.043267, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593878933.034646}\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[07/04/2020 16:09:44 INFO 140466583250752] Epoch[30] Batch[0] avg_epoch_loss=-2.460935\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:09:44 INFO 140466583250752] #quality_metric: host=algo-1, epoch=30, batch=0 train loss <loss>=-2.4609348774\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:09:45 INFO 140466583250752] Epoch[30] Batch[5] avg_epoch_loss=-3.525690\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:09:45 INFO 140466583250752] #quality_metric: host=algo-1, epoch=30, batch=5 train loss <loss>=-3.5256896019\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:09:45 INFO 140466583250752] Epoch[30] Batch [5]#011Speed: 1815.92 samples/sec#011loss=-3.525690\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:09:45 INFO 140466583250752] processed a total of 1259 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52417.09804534912, \"sum\": 52417.09804534912, \"min\": 52417.09804534912}}, \"EndTime\": 1593878985.460502, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593878933.043335}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:09:45 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=24.0188175217 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:09:45 INFO 140466583250752] #progress_metric: host=algo-1, completed 15 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:09:45 INFO 140466583250752] #quality_metric: host=algo-1, epoch=30, train loss <loss>=-3.71282477379\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:09:45 INFO 140466583250752] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:10:37 INFO 140466583250752] Epoch[31] Batch[0] avg_epoch_loss=-4.075134\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:10:37 INFO 140466583250752] #quality_metric: host=algo-1, epoch=31, batch=0 train loss <loss>=-4.07513427734\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:10:37 INFO 140466583250752] Epoch[31] Batch[5] avg_epoch_loss=-3.941448\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:10:37 INFO 140466583250752] #quality_metric: host=algo-1, epoch=31, batch=5 train loss <loss>=-3.94144761562\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:10:37 INFO 140466583250752] Epoch[31] Batch [5]#011Speed: 1822.79 samples/sec#011loss=-3.941448\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:10:38 INFO 140466583250752] processed a total of 1280 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52598.296880722046, \"sum\": 52598.296880722046, \"min\": 52598.296880722046}}, \"EndTime\": 1593879038.059433, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593878985.460595}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:10:38 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=24.3353317898 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:10:38 INFO 140466583250752] #progress_metric: host=algo-1, completed 16 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:10:38 INFO 140466583250752] #quality_metric: host=algo-1, epoch=31, train loss <loss>=-3.9531078577\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:10:38 INFO 140466583250752] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:11:29 INFO 140466583250752] Epoch[32] Batch[0] avg_epoch_loss=-3.942036\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:11:29 INFO 140466583250752] #quality_metric: host=algo-1, epoch=32, batch=0 train loss <loss>=-3.94203615189\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:11:30 INFO 140466583250752] Epoch[32] Batch[5] avg_epoch_loss=-3.976393\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:11:30 INFO 140466583250752] #quality_metric: host=algo-1, epoch=32, batch=5 train loss <loss>=-3.97639334202\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:11:30 INFO 140466583250752] Epoch[32] Batch [5]#011Speed: 1724.03 samples/sec#011loss=-3.976393\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:11:30 INFO 140466583250752] processed a total of 1260 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52448.06694984436, \"sum\": 52448.06694984436, \"min\": 52448.06694984436}}, \"EndTime\": 1593879090.508162, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593879038.059512}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:11:30 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=24.023707543 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:11:30 INFO 140466583250752] #progress_metric: host=algo-1, completed 16 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:11:30 INFO 140466583250752] #quality_metric: host=algo-1, epoch=32, train loss <loss>=-3.97517826557\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:11:30 INFO 140466583250752] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:12:22 INFO 140466583250752] Epoch[33] Batch[0] avg_epoch_loss=-3.893829\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:12:22 INFO 140466583250752] #quality_metric: host=algo-1, epoch=33, batch=0 train loss <loss>=-3.89382886887\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:12:22 INFO 140466583250752] Epoch[33] Batch[5] avg_epoch_loss=-3.949903\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:12:22 INFO 140466583250752] #quality_metric: host=algo-1, epoch=33, batch=5 train loss <loss>=-3.94990324974\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:12:22 INFO 140466583250752] Epoch[33] Batch [5]#011Speed: 1809.37 samples/sec#011loss=-3.949903\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:12:22 INFO 140466583250752] processed a total of 1280 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52432.62791633606, \"sum\": 52432.62791633606, \"min\": 52432.62791633606}}, \"EndTime\": 1593879142.941427, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593879090.508243}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:12:22 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=24.4122144023 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:12:22 INFO 140466583250752] #progress_metric: host=algo-1, completed 17 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:12:22 INFO 140466583250752] #quality_metric: host=algo-1, epoch=33, train loss <loss>=-3.96481497288\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:12:22 INFO 140466583250752] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:13:14 INFO 140466583250752] Epoch[34] Batch[0] avg_epoch_loss=-4.117383\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:13:14 INFO 140466583250752] #quality_metric: host=algo-1, epoch=34, batch=0 train loss <loss>=-4.1173825264\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:13:14 INFO 140466583250752] Epoch[34] Batch[5] avg_epoch_loss=-4.022223\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:13:14 INFO 140466583250752] #quality_metric: host=algo-1, epoch=34, batch=5 train loss <loss>=-4.02222307523\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:13:14 INFO 140466583250752] Epoch[34] Batch [5]#011Speed: 1811.58 samples/sec#011loss=-4.022223\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:13:15 INFO 140466583250752] Epoch[34] Batch[10] avg_epoch_loss=-4.039051\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:13:15 INFO 140466583250752] #quality_metric: host=algo-1, epoch=34, batch=10 train loss <loss>=-4.0592455864\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:13:15 INFO 140466583250752] Epoch[34] Batch [10]#011Speed: 1173.72 samples/sec#011loss=-4.059246\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:13:15 INFO 140466583250752] processed a total of 1312 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52206.230878829956, \"sum\": 52206.230878829956, \"min\": 52206.230878829956}}, \"EndTime\": 1593879195.148308, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593879142.941524}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:13:15 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=25.1310376254 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:13:15 INFO 140466583250752] #progress_metric: host=algo-1, completed 17 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:13:15 INFO 140466583250752] #quality_metric: host=algo-1, epoch=34, train loss <loss>=-4.0390514894\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:13:15 INFO 140466583250752] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:13:15 INFO 140466583250752] Saved checkpoint to \"/opt/ml/model/state_1bb2940f-ca1c-4098-9ce3-004b34c3fe93-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 7.895946502685547, \"sum\": 7.895946502685547, \"min\": 7.895946502685547}}, \"EndTime\": 1593879195.156839, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593879195.148393}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:14:06 INFO 140466583250752] Epoch[35] Batch[0] avg_epoch_loss=-3.877030\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:14:06 INFO 140466583250752] #quality_metric: host=algo-1, epoch=35, batch=0 train loss <loss>=-3.87702989578\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:14:07 INFO 140466583250752] Epoch[35] Batch[5] avg_epoch_loss=-4.045493\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:14:07 INFO 140466583250752] #quality_metric: host=algo-1, epoch=35, batch=5 train loss <loss>=-4.04549320539\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:14:07 INFO 140466583250752] Epoch[35] Batch [5]#011Speed: 1810.75 samples/sec#011loss=-4.045493\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:14:07 INFO 140466583250752] Epoch[35] Batch[10] avg_epoch_loss=-4.126385\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:14:07 INFO 140466583250752] #quality_metric: host=algo-1, epoch=35, batch=10 train loss <loss>=-4.22345581055\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:14:07 INFO 140466583250752] Epoch[35] Batch [10]#011Speed: 1233.17 samples/sec#011loss=-4.223456\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:14:07 INFO 140466583250752] processed a total of 1349 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52661.49806976318, \"sum\": 52661.49806976318, \"min\": 52661.49806976318}}, \"EndTime\": 1593879247.818472, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593879195.156909}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:14:07 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=25.6163762957 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:14:07 INFO 140466583250752] #progress_metric: host=algo-1, completed 18 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:14:07 INFO 140466583250752] #quality_metric: host=algo-1, epoch=35, train loss <loss>=-4.12638529864\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:14:07 INFO 140466583250752] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:14:07 INFO 140466583250752] Saved checkpoint to \"/opt/ml/model/state_0f283185-8b5d-4a87-87a1-468683442ea5-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 10.040998458862305, \"sum\": 10.040998458862305, \"min\": 10.040998458862305}}, \"EndTime\": 1593879247.829173, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593879247.818558}\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[07/04/2020 16:14:59 INFO 140466583250752] Epoch[36] Batch[0] avg_epoch_loss=-3.986794\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:14:59 INFO 140466583250752] #quality_metric: host=algo-1, epoch=36, batch=0 train loss <loss>=-3.98679351807\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:14:59 INFO 140466583250752] Epoch[36] Batch[5] avg_epoch_loss=-4.061351\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:14:59 INFO 140466583250752] #quality_metric: host=algo-1, epoch=36, batch=5 train loss <loss>=-4.06135058403\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:14:59 INFO 140466583250752] Epoch[36] Batch [5]#011Speed: 1810.99 samples/sec#011loss=-4.061351\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:15:00 INFO 140466583250752] Epoch[36] Batch[10] avg_epoch_loss=-4.043699\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:15:00 INFO 140466583250752] #quality_metric: host=algo-1, epoch=36, batch=10 train loss <loss>=-4.02251696587\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:15:00 INFO 140466583250752] Epoch[36] Batch [10]#011Speed: 1077.51 samples/sec#011loss=-4.022517\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:15:00 INFO 140466583250752] processed a total of 1281 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52633.40902328491, \"sum\": 52633.40902328491, \"min\": 52633.40902328491}}, \"EndTime\": 1593879300.462718, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593879247.829241}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:15:00 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=24.3380949659 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:15:00 INFO 140466583250752] #progress_metric: host=algo-1, completed 18 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:15:00 INFO 140466583250752] #quality_metric: host=algo-1, epoch=36, train loss <loss>=-4.04369893941\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:15:00 INFO 140466583250752] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:15:52 INFO 140466583250752] Epoch[37] Batch[0] avg_epoch_loss=-4.222580\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:15:52 INFO 140466583250752] #quality_metric: host=algo-1, epoch=37, batch=0 train loss <loss>=-4.22257995605\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:15:52 INFO 140466583250752] Epoch[37] Batch[5] avg_epoch_loss=-4.105637\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:15:52 INFO 140466583250752] #quality_metric: host=algo-1, epoch=37, batch=5 train loss <loss>=-4.10563739141\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:15:52 INFO 140466583250752] Epoch[37] Batch [5]#011Speed: 1794.30 samples/sec#011loss=-4.105637\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:15:53 INFO 140466583250752] Epoch[37] Batch[10] avg_epoch_loss=-4.038610\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:15:53 INFO 140466583250752] #quality_metric: host=algo-1, epoch=37, batch=10 train loss <loss>=-3.95817613602\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:15:53 INFO 140466583250752] Epoch[37] Batch [10]#011Speed: 1091.95 samples/sec#011loss=-3.958176\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:15:53 INFO 140466583250752] processed a total of 1285 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52588.53077888489, \"sum\": 52588.53077888489, \"min\": 52588.53077888489}}, \"EndTime\": 1593879353.051807, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593879300.462804}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:15:53 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=24.4349178384 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:15:53 INFO 140466583250752] #progress_metric: host=algo-1, completed 19 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:15:53 INFO 140466583250752] #quality_metric: host=algo-1, epoch=37, train loss <loss>=-4.03860954805\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:15:53 INFO 140466583250752] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:16:44 INFO 140466583250752] Epoch[38] Batch[0] avg_epoch_loss=-3.074817\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:16:44 INFO 140466583250752] #quality_metric: host=algo-1, epoch=38, batch=0 train loss <loss>=-3.0748167038\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:16:45 INFO 140466583250752] Epoch[38] Batch[5] avg_epoch_loss=-3.767660\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:16:45 INFO 140466583250752] #quality_metric: host=algo-1, epoch=38, batch=5 train loss <loss>=-3.76766010125\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:16:45 INFO 140466583250752] Epoch[38] Batch [5]#011Speed: 1804.40 samples/sec#011loss=-3.767660\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:16:45 INFO 140466583250752] Epoch[38] Batch[10] avg_epoch_loss=-3.898333\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:16:45 INFO 140466583250752] #quality_metric: host=algo-1, epoch=38, batch=10 train loss <loss>=-4.05514039993\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:16:45 INFO 140466583250752] Epoch[38] Batch [10]#011Speed: 1104.53 samples/sec#011loss=-4.055140\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:16:45 INFO 140466583250752] processed a total of 1291 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52818.06802749634, \"sum\": 52818.06802749634, \"min\": 52818.06802749634}}, \"EndTime\": 1593879405.870497, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593879353.05191}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:16:45 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=24.4423310023 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:16:45 INFO 140466583250752] #progress_metric: host=algo-1, completed 19 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:16:45 INFO 140466583250752] #quality_metric: host=algo-1, epoch=38, train loss <loss>=-3.89833296429\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:16:45 INFO 140466583250752] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:17:37 INFO 140466583250752] Epoch[39] Batch[0] avg_epoch_loss=-3.904380\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:17:37 INFO 140466583250752] #quality_metric: host=algo-1, epoch=39, batch=0 train loss <loss>=-3.90437960625\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:17:37 INFO 140466583250752] Epoch[39] Batch[5] avg_epoch_loss=-3.985599\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:17:37 INFO 140466583250752] #quality_metric: host=algo-1, epoch=39, batch=5 train loss <loss>=-3.98559860388\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:17:37 INFO 140466583250752] Epoch[39] Batch [5]#011Speed: 1822.80 samples/sec#011loss=-3.985599\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:17:38 INFO 140466583250752] processed a total of 1262 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52463.099002838135, \"sum\": 52463.099002838135, \"min\": 52463.099002838135}}, \"EndTime\": 1593879458.334199, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593879405.870587}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:17:38 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=24.0549519835 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:17:38 INFO 140466583250752] #progress_metric: host=algo-1, completed 20 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:17:38 INFO 140466583250752] #quality_metric: host=algo-1, epoch=39, train loss <loss>=-4.03030135632\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:17:38 INFO 140466583250752] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:18:30 INFO 140466583250752] Epoch[40] Batch[0] avg_epoch_loss=-4.140520\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:18:30 INFO 140466583250752] #quality_metric: host=algo-1, epoch=40, batch=0 train loss <loss>=-4.14051961899\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:18:30 INFO 140466583250752] Epoch[40] Batch[5] avg_epoch_loss=-4.062860\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:18:30 INFO 140466583250752] #quality_metric: host=algo-1, epoch=40, batch=5 train loss <loss>=-4.06285993258\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:18:30 INFO 140466583250752] Epoch[40] Batch [5]#011Speed: 1806.42 samples/sec#011loss=-4.062860\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:18:30 INFO 140466583250752] processed a total of 1266 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52449.74613189697, \"sum\": 52449.74613189697, \"min\": 52449.74613189697}}, \"EndTime\": 1593879510.784534, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593879458.334271}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:18:30 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=24.13734083 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:18:30 INFO 140466583250752] #progress_metric: host=algo-1, completed 20 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:18:30 INFO 140466583250752] #quality_metric: host=algo-1, epoch=40, train loss <loss>=-4.08615937233\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:18:30 INFO 140466583250752] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:19:22 INFO 140466583250752] Epoch[41] Batch[0] avg_epoch_loss=-4.125322\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:19:22 INFO 140466583250752] #quality_metric: host=algo-1, epoch=41, batch=0 train loss <loss>=-4.12532186508\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:19:22 INFO 140466583250752] Epoch[41] Batch[5] avg_epoch_loss=-4.139711\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:19:22 INFO 140466583250752] #quality_metric: host=algo-1, epoch=41, batch=5 train loss <loss>=-4.13971145948\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:19:22 INFO 140466583250752] Epoch[41] Batch [5]#011Speed: 1820.69 samples/sec#011loss=-4.139711\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:19:23 INFO 140466583250752] Epoch[41] Batch[10] avg_epoch_loss=-4.134819\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:19:23 INFO 140466583250752] #quality_metric: host=algo-1, epoch=41, batch=10 train loss <loss>=-4.1289484024\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:19:23 INFO 140466583250752] Epoch[41] Batch [10]#011Speed: 1245.53 samples/sec#011loss=-4.128948\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:19:23 INFO 140466583250752] processed a total of 1341 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52570.693016052246, \"sum\": 52570.693016052246, \"min\": 52570.693016052246}}, \"EndTime\": 1593879563.355757, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593879510.784602}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:19:23 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=25.5084290227 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:19:23 INFO 140466583250752] #progress_metric: host=algo-1, completed 21 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:19:23 INFO 140466583250752] #quality_metric: host=algo-1, epoch=41, train loss <loss>=-4.13481916081\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:19:23 INFO 140466583250752] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:19:23 INFO 140466583250752] Saved checkpoint to \"/opt/ml/model/state_4810435f-1bf6-4235-a0c5-ca3453bcbfcd-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 7.649898529052734, \"sum\": 7.649898529052734, \"min\": 7.649898529052734}}, \"EndTime\": 1593879563.364146, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593879563.355852}\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[07/04/2020 16:20:14 INFO 140466583250752] Epoch[42] Batch[0] avg_epoch_loss=-4.093235\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:20:14 INFO 140466583250752] #quality_metric: host=algo-1, epoch=42, batch=0 train loss <loss>=-4.09323501587\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:20:15 INFO 140466583250752] Epoch[42] Batch[5] avg_epoch_loss=-4.048449\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:20:15 INFO 140466583250752] #quality_metric: host=algo-1, epoch=42, batch=5 train loss <loss>=-4.04844923814\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:20:15 INFO 140466583250752] Epoch[42] Batch [5]#011Speed: 1819.61 samples/sec#011loss=-4.048449\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:20:15 INFO 140466583250752] processed a total of 1262 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52190.75393676758, \"sum\": 52190.75393676758, \"min\": 52190.75393676758}}, \"EndTime\": 1593879615.55504, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593879563.364218}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:20:15 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=24.1804740142 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:20:15 INFO 140466583250752] #progress_metric: host=algo-1, completed 21 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:20:15 INFO 140466583250752] #quality_metric: host=algo-1, epoch=42, train loss <loss>=-4.04717929363\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:20:15 INFO 140466583250752] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:21:07 INFO 140466583250752] Epoch[43] Batch[0] avg_epoch_loss=-4.112134\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:21:07 INFO 140466583250752] #quality_metric: host=algo-1, epoch=43, batch=0 train loss <loss>=-4.1121339798\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:21:07 INFO 140466583250752] Epoch[43] Batch[5] avg_epoch_loss=-4.080609\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:21:07 INFO 140466583250752] #quality_metric: host=algo-1, epoch=43, batch=5 train loss <loss>=-4.08060940107\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:21:07 INFO 140466583250752] Epoch[43] Batch [5]#011Speed: 1812.83 samples/sec#011loss=-4.080609\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:21:08 INFO 140466583250752] Epoch[43] Batch[10] avg_epoch_loss=-4.021301\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:21:08 INFO 140466583250752] #quality_metric: host=algo-1, epoch=43, batch=10 train loss <loss>=-3.95013012886\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:21:08 INFO 140466583250752] Epoch[43] Batch [10]#011Speed: 1164.53 samples/sec#011loss=-3.950130\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:21:08 INFO 140466583250752] processed a total of 1326 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52763.700008392334, \"sum\": 52763.700008392334, \"min\": 52763.700008392334}}, \"EndTime\": 1593879668.319337, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593879615.555114}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:21:08 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=25.1308491357 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:21:08 INFO 140466583250752] #progress_metric: host=algo-1, completed 22 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:21:08 INFO 140466583250752] #quality_metric: host=algo-1, epoch=43, train loss <loss>=-4.02130064097\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:21:08 INFO 140466583250752] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:21:59 INFO 140466583250752] Epoch[44] Batch[0] avg_epoch_loss=-4.216635\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:21:59 INFO 140466583250752] #quality_metric: host=algo-1, epoch=44, batch=0 train loss <loss>=-4.21663475037\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:21:59 INFO 140466583250752] Epoch[44] Batch[5] avg_epoch_loss=-4.130840\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:21:59 INFO 140466583250752] #quality_metric: host=algo-1, epoch=44, batch=5 train loss <loss>=-4.13083978494\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:21:59 INFO 140466583250752] Epoch[44] Batch [5]#011Speed: 1818.74 samples/sec#011loss=-4.130840\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:22:00 INFO 140466583250752] processed a total of 1209 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52065.27280807495, \"sum\": 52065.27280807495, \"min\": 52065.27280807495}}, \"EndTime\": 1593879720.385185, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593879668.31943}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:22:00 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=23.2207967115 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:22:00 INFO 140466583250752] #progress_metric: host=algo-1, completed 22 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:22:00 INFO 140466583250752] #quality_metric: host=algo-1, epoch=44, train loss <loss>=-4.16489322186\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:22:00 INFO 140466583250752] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:22:00 INFO 140466583250752] Saved checkpoint to \"/opt/ml/model/state_50c19649-e4c1-478f-a265-f02554c08eaa-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 8.28409194946289, \"sum\": 8.28409194946289, \"min\": 8.28409194946289}}, \"EndTime\": 1593879720.39414, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593879720.385267}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:22:52 INFO 140466583250752] Epoch[45] Batch[0] avg_epoch_loss=-4.136598\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:22:52 INFO 140466583250752] #quality_metric: host=algo-1, epoch=45, batch=0 train loss <loss>=-4.13659763336\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:22:52 INFO 140466583250752] Epoch[45] Batch[5] avg_epoch_loss=-4.011434\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:22:52 INFO 140466583250752] #quality_metric: host=algo-1, epoch=45, batch=5 train loss <loss>=-4.01143423716\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:22:52 INFO 140466583250752] Epoch[45] Batch [5]#011Speed: 1732.97 samples/sec#011loss=-4.011434\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:22:53 INFO 140466583250752] processed a total of 1258 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52633.36515426636, \"sum\": 52633.36515426636, \"min\": 52633.36515426636}}, \"EndTime\": 1593879773.02764, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593879720.394217}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:22:53 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=23.9011480263 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:22:53 INFO 140466583250752] #progress_metric: host=algo-1, completed 23 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:22:53 INFO 140466583250752] #quality_metric: host=algo-1, epoch=45, train loss <loss>=-4.03246672153\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:22:53 INFO 140466583250752] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:23:44 INFO 140466583250752] Epoch[46] Batch[0] avg_epoch_loss=-4.052464\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:23:44 INFO 140466583250752] #quality_metric: host=algo-1, epoch=46, batch=0 train loss <loss>=-4.05246448517\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:23:44 INFO 140466583250752] Epoch[46] Batch[5] avg_epoch_loss=-4.128660\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:23:44 INFO 140466583250752] #quality_metric: host=algo-1, epoch=46, batch=5 train loss <loss>=-4.12865980466\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:23:44 INFO 140466583250752] Epoch[46] Batch [5]#011Speed: 1808.01 samples/sec#011loss=-4.128660\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:23:45 INFO 140466583250752] Epoch[46] Batch[10] avg_epoch_loss=-4.086107\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:23:45 INFO 140466583250752] #quality_metric: host=algo-1, epoch=46, batch=10 train loss <loss>=-4.03504414558\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:23:45 INFO 140466583250752] Epoch[46] Batch [10]#011Speed: 1115.14 samples/sec#011loss=-4.035044\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:23:45 INFO 140466583250752] processed a total of 1292 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52340.66915512085, \"sum\": 52340.66915512085, \"min\": 52340.66915512085}}, \"EndTime\": 1593879825.368948, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593879773.0277}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:23:45 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=24.6843804925 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:23:45 INFO 140466583250752] #progress_metric: host=algo-1, completed 23 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:23:45 INFO 140466583250752] #quality_metric: host=algo-1, epoch=46, train loss <loss>=-4.08610723235\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:23:45 INFO 140466583250752] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:24:36 INFO 140466583250752] Epoch[47] Batch[0] avg_epoch_loss=-3.281770\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:24:36 INFO 140466583250752] #quality_metric: host=algo-1, epoch=47, batch=0 train loss <loss>=-3.2817697525\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:24:37 INFO 140466583250752] Epoch[47] Batch[5] avg_epoch_loss=-3.602956\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:24:37 INFO 140466583250752] #quality_metric: host=algo-1, epoch=47, batch=5 train loss <loss>=-3.60295569897\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:24:37 INFO 140466583250752] Epoch[47] Batch [5]#011Speed: 1828.55 samples/sec#011loss=-3.602956\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:24:37 INFO 140466583250752] processed a total of 1236 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52129.76789474487, \"sum\": 52129.76789474487, \"min\": 52129.76789474487}}, \"EndTime\": 1593879877.499325, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593879825.369032}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:24:37 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=23.7100045164 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:24:37 INFO 140466583250752] #progress_metric: host=algo-1, completed 24 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:24:37 INFO 140466583250752] #quality_metric: host=algo-1, epoch=47, train loss <loss>=-3.67335112095\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:24:37 INFO 140466583250752] loss did not improve\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[07/04/2020 16:25:28 INFO 140466583250752] Epoch[48] Batch[0] avg_epoch_loss=-3.633671\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:25:28 INFO 140466583250752] #quality_metric: host=algo-1, epoch=48, batch=0 train loss <loss>=-3.63367128372\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:25:28 INFO 140466583250752] Epoch[48] Batch[5] avg_epoch_loss=-3.784148\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:25:28 INFO 140466583250752] #quality_metric: host=algo-1, epoch=48, batch=5 train loss <loss>=-3.78414813677\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:25:28 INFO 140466583250752] Epoch[48] Batch [5]#011Speed: 1823.99 samples/sec#011loss=-3.784148\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:25:29 INFO 140466583250752] Epoch[48] Batch[10] avg_epoch_loss=-3.883523\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:25:29 INFO 140466583250752] #quality_metric: host=algo-1, epoch=48, batch=10 train loss <loss>=-4.0027736187\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:25:29 INFO 140466583250752] Epoch[48] Batch [10]#011Speed: 1215.72 samples/sec#011loss=-4.002774\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:25:29 INFO 140466583250752] processed a total of 1323 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 51721.561908721924, \"sum\": 51721.561908721924, \"min\": 51721.561908721924}}, \"EndTime\": 1593879929.221567, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593879877.499406}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:25:29 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=25.5792219282 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:25:29 INFO 140466583250752] #progress_metric: host=algo-1, completed 24 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:25:29 INFO 140466583250752] #quality_metric: host=algo-1, epoch=48, train loss <loss>=-3.88352335583\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:25:29 INFO 140466583250752] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:26:20 INFO 140466583250752] Epoch[49] Batch[0] avg_epoch_loss=-3.971770\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:26:20 INFO 140466583250752] #quality_metric: host=algo-1, epoch=49, batch=0 train loss <loss>=-3.9717695713\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:26:20 INFO 140466583250752] Epoch[49] Batch[5] avg_epoch_loss=-4.044073\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:26:20 INFO 140466583250752] #quality_metric: host=algo-1, epoch=49, batch=5 train loss <loss>=-4.04407286644\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:26:20 INFO 140466583250752] Epoch[49] Batch [5]#011Speed: 1815.07 samples/sec#011loss=-4.044073\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:26:20 INFO 140466583250752] processed a total of 1248 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 51589.40291404724, \"sum\": 51589.40291404724, \"min\": 51589.40291404724}}, \"EndTime\": 1593879980.811589, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593879929.221638}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:26:20 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=24.1909709046 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:26:20 INFO 140466583250752] #progress_metric: host=algo-1, completed 25 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:26:20 INFO 140466583250752] #quality_metric: host=algo-1, epoch=49, train loss <loss>=-4.03125214577\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:26:20 INFO 140466583250752] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:27:11 INFO 140466583250752] Epoch[50] Batch[0] avg_epoch_loss=-4.081848\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:27:11 INFO 140466583250752] #quality_metric: host=algo-1, epoch=50, batch=0 train loss <loss>=-4.08184814453\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:27:11 INFO 140466583250752] Epoch[50] Batch[5] avg_epoch_loss=-4.125499\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:27:11 INFO 140466583250752] #quality_metric: host=algo-1, epoch=50, batch=5 train loss <loss>=-4.12549869219\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:27:11 INFO 140466583250752] Epoch[50] Batch [5]#011Speed: 1771.40 samples/sec#011loss=-4.125499\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:27:12 INFO 140466583250752] Epoch[50] Batch[10] avg_epoch_loss=-4.140420\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:27:12 INFO 140466583250752] #quality_metric: host=algo-1, epoch=50, batch=10 train loss <loss>=-4.15832471848\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:27:12 INFO 140466583250752] Epoch[50] Batch [10]#011Speed: 1201.55 samples/sec#011loss=-4.158325\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:27:12 INFO 140466583250752] processed a total of 1327 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 51654.382944107056, \"sum\": 51654.382944107056, \"min\": 51654.382944107056}}, \"EndTime\": 1593880032.466587, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593879980.811649}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:27:12 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=25.6899135299 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:27:12 INFO 140466583250752] #progress_metric: host=algo-1, completed 25 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:27:12 INFO 140466583250752] #quality_metric: host=algo-1, epoch=50, train loss <loss>=-4.14041961323\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:27:12 INFO 140466583250752] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:28:03 INFO 140466583250752] Epoch[51] Batch[0] avg_epoch_loss=-4.089140\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:28:03 INFO 140466583250752] #quality_metric: host=algo-1, epoch=51, batch=0 train loss <loss>=-4.08913993835\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:28:03 INFO 140466583250752] Epoch[51] Batch[5] avg_epoch_loss=-4.163401\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:28:03 INFO 140466583250752] #quality_metric: host=algo-1, epoch=51, batch=5 train loss <loss>=-4.16340065002\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:28:03 INFO 140466583250752] Epoch[51] Batch [5]#011Speed: 1819.35 samples/sec#011loss=-4.163401\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:28:03 INFO 140466583250752] processed a total of 1276 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 51276.899099349976, \"sum\": 51276.899099349976, \"min\": 51276.899099349976}}, \"EndTime\": 1593880083.744121, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593880032.466677}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:28:03 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=24.8844458202 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:28:03 INFO 140466583250752] #progress_metric: host=algo-1, completed 26 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:28:03 INFO 140466583250752] #quality_metric: host=algo-1, epoch=51, train loss <loss>=-4.14431219101\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:28:03 INFO 140466583250752] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:28:54 INFO 140466583250752] Epoch[52] Batch[0] avg_epoch_loss=-4.165007\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:28:54 INFO 140466583250752] #quality_metric: host=algo-1, epoch=52, batch=0 train loss <loss>=-4.16500711441\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:28:55 INFO 140466583250752] Epoch[52] Batch[5] avg_epoch_loss=-4.070966\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:28:55 INFO 140466583250752] #quality_metric: host=algo-1, epoch=52, batch=5 train loss <loss>=-4.07096556822\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:28:55 INFO 140466583250752] Epoch[52] Batch [5]#011Speed: 1827.62 samples/sec#011loss=-4.070966\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:28:55 INFO 140466583250752] Epoch[52] Batch[10] avg_epoch_loss=-4.203084\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:28:55 INFO 140466583250752] #quality_metric: host=algo-1, epoch=52, batch=10 train loss <loss>=-4.36162576675\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:28:55 INFO 140466583250752] Epoch[52] Batch [10]#011Speed: 1137.11 samples/sec#011loss=-4.361626\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:28:55 INFO 140466583250752] processed a total of 1309 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 51926.90300941467, \"sum\": 51926.90300941467, \"min\": 51926.90300941467}}, \"EndTime\": 1593880135.671708, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593880083.744194}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:28:55 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=25.2084526296 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:28:55 INFO 140466583250752] #progress_metric: host=algo-1, completed 26 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:28:55 INFO 140466583250752] #quality_metric: host=algo-1, epoch=52, train loss <loss>=-4.20308384028\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:28:55 INFO 140466583250752] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:28:55 INFO 140466583250752] Saved checkpoint to \"/opt/ml/model/state_6fc18f7a-7acd-490b-9d43-11b847d94433-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 8.153915405273438, \"sum\": 8.153915405273438, \"min\": 8.153915405273438}}, \"EndTime\": 1593880135.680512, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593880135.671791}\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[07/04/2020 16:29:46 INFO 140466583250752] Epoch[53] Batch[0] avg_epoch_loss=-4.262716\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:29:46 INFO 140466583250752] #quality_metric: host=algo-1, epoch=53, batch=0 train loss <loss>=-4.26271629333\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:29:47 INFO 140466583250752] Epoch[53] Batch[5] avg_epoch_loss=-4.162700\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:29:47 INFO 140466583250752] #quality_metric: host=algo-1, epoch=53, batch=5 train loss <loss>=-4.16269961993\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:29:47 INFO 140466583250752] Epoch[53] Batch [5]#011Speed: 1759.34 samples/sec#011loss=-4.162700\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:29:47 INFO 140466583250752] processed a total of 1257 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 51779.4029712677, \"sum\": 51779.4029712677, \"min\": 51779.4029712677}}, \"EndTime\": 1593880187.460052, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593880135.680584}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:29:47 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=24.2760005823 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:29:47 INFO 140466583250752] #progress_metric: host=algo-1, completed 27 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:29:47 INFO 140466583250752] #quality_metric: host=algo-1, epoch=53, train loss <loss>=-4.16983714104\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:29:47 INFO 140466583250752] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:30:38 INFO 140466583250752] Epoch[54] Batch[0] avg_epoch_loss=-4.169528\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:30:38 INFO 140466583250752] #quality_metric: host=algo-1, epoch=54, batch=0 train loss <loss>=-4.16952753067\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:30:39 INFO 140466583250752] Epoch[54] Batch[5] avg_epoch_loss=-4.202832\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:30:39 INFO 140466583250752] #quality_metric: host=algo-1, epoch=54, batch=5 train loss <loss>=-4.2028324604\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:30:39 INFO 140466583250752] Epoch[54] Batch [5]#011Speed: 1821.97 samples/sec#011loss=-4.202832\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:30:39 INFO 140466583250752] Epoch[54] Batch[10] avg_epoch_loss=-4.112940\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:30:39 INFO 140466583250752] #quality_metric: host=algo-1, epoch=54, batch=10 train loss <loss>=-4.0050696373\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:30:39 INFO 140466583250752] Epoch[54] Batch [10]#011Speed: 1094.73 samples/sec#011loss=-4.005070\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:30:39 INFO 140466583250752] processed a total of 1286 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52410.82191467285, \"sum\": 52410.82191467285, \"min\": 52410.82191467285}}, \"EndTime\": 1593880239.871538, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593880187.460141}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:30:39 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=24.5368554069 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:30:39 INFO 140466583250752] #progress_metric: host=algo-1, completed 27 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:30:39 INFO 140466583250752] #quality_metric: host=algo-1, epoch=54, train loss <loss>=-4.11294026808\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:30:39 INFO 140466583250752] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:31:31 INFO 140466583250752] Epoch[55] Batch[0] avg_epoch_loss=-2.530028\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:31:31 INFO 140466583250752] #quality_metric: host=algo-1, epoch=55, batch=0 train loss <loss>=-2.53002786636\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:31:32 INFO 140466583250752] Epoch[55] Batch[5] avg_epoch_loss=-3.527230\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:31:32 INFO 140466583250752] #quality_metric: host=algo-1, epoch=55, batch=5 train loss <loss>=-3.52722958724\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:31:32 INFO 140466583250752] Epoch[55] Batch [5]#011Speed: 1753.41 samples/sec#011loss=-3.527230\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:31:32 INFO 140466583250752] processed a total of 1278 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52660.720109939575, \"sum\": 52660.720109939575, \"min\": 52660.720109939575}}, \"EndTime\": 1593880292.532851, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593880239.871627}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:31:32 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=24.2685095324 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:31:32 INFO 140466583250752] #progress_metric: host=algo-1, completed 28 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:31:32 INFO 140466583250752] #quality_metric: host=algo-1, epoch=55, train loss <loss>=-3.67360246181\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:31:32 INFO 140466583250752] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:32:23 INFO 140466583250752] Epoch[56] Batch[0] avg_epoch_loss=-3.694667\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:32:23 INFO 140466583250752] #quality_metric: host=algo-1, epoch=56, batch=0 train loss <loss>=-3.69466710091\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:32:24 INFO 140466583250752] Epoch[56] Batch[5] avg_epoch_loss=-3.887826\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:32:24 INFO 140466583250752] #quality_metric: host=algo-1, epoch=56, batch=5 train loss <loss>=-3.88782560825\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:32:24 INFO 140466583250752] Epoch[56] Batch [5]#011Speed: 1825.38 samples/sec#011loss=-3.887826\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:32:24 INFO 140466583250752] processed a total of 1220 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52109.971046447754, \"sum\": 52109.971046447754, \"min\": 52109.971046447754}}, \"EndTime\": 1593880344.643436, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593880292.532928}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:32:24 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=23.411965858 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:32:24 INFO 140466583250752] #progress_metric: host=algo-1, completed 28 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:32:24 INFO 140466583250752] #quality_metric: host=algo-1, epoch=56, train loss <loss>=-3.90279903412\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:32:24 INFO 140466583250752] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:33:16 INFO 140466583250752] Epoch[57] Batch[0] avg_epoch_loss=-4.163558\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:33:16 INFO 140466583250752] #quality_metric: host=algo-1, epoch=57, batch=0 train loss <loss>=-4.16355800629\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:33:16 INFO 140466583250752] Epoch[57] Batch[5] avg_epoch_loss=-4.045337\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:33:16 INFO 140466583250752] #quality_metric: host=algo-1, epoch=57, batch=5 train loss <loss>=-4.04533684254\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:33:16 INFO 140466583250752] Epoch[57] Batch [5]#011Speed: 1779.84 samples/sec#011loss=-4.045337\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:33:17 INFO 140466583250752] Epoch[57] Batch[10] avg_epoch_loss=-4.060760\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:33:17 INFO 140466583250752] #quality_metric: host=algo-1, epoch=57, batch=10 train loss <loss>=-4.07926855087\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:33:17 INFO 140466583250752] Epoch[57] Batch [10]#011Speed: 1118.98 samples/sec#011loss=-4.079269\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:33:17 INFO 140466583250752] processed a total of 1303 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52608.82091522217, \"sum\": 52608.82091522217, \"min\": 52608.82091522217}}, \"EndTime\": 1593880397.252926, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593880344.643526}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:33:17 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=24.7676347551 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:33:17 INFO 140466583250752] #progress_metric: host=algo-1, completed 29 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:33:17 INFO 140466583250752] #quality_metric: host=algo-1, epoch=57, train loss <loss>=-4.06076034633\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:33:17 INFO 140466583250752] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:34:08 INFO 140466583250752] Epoch[58] Batch[0] avg_epoch_loss=-3.990984\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:34:08 INFO 140466583250752] #quality_metric: host=algo-1, epoch=58, batch=0 train loss <loss>=-3.99098443985\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:34:08 INFO 140466583250752] Epoch[58] Batch[5] avg_epoch_loss=-4.110006\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:34:08 INFO 140466583250752] #quality_metric: host=algo-1, epoch=58, batch=5 train loss <loss>=-4.11000649134\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:34:08 INFO 140466583250752] Epoch[58] Batch [5]#011Speed: 1750.83 samples/sec#011loss=-4.110006\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:34:09 INFO 140466583250752] processed a total of 1244 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52050.26197433472, \"sum\": 52050.26197433472, \"min\": 52050.26197433472}}, \"EndTime\": 1593880449.303849, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593880397.253046}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:34:09 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=23.8999132598 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:34:09 INFO 140466583250752] #progress_metric: host=algo-1, completed 29 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:34:09 INFO 140466583250752] #quality_metric: host=algo-1, epoch=58, train loss <loss>=-4.10030975342\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:34:09 INFO 140466583250752] loss did not improve\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[07/04/2020 16:35:01 INFO 140466583250752] Epoch[59] Batch[0] avg_epoch_loss=-4.243622\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:35:01 INFO 140466583250752] #quality_metric: host=algo-1, epoch=59, batch=0 train loss <loss>=-4.24362230301\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:35:01 INFO 140466583250752] Epoch[59] Batch[5] avg_epoch_loss=-4.172838\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:35:01 INFO 140466583250752] #quality_metric: host=algo-1, epoch=59, batch=5 train loss <loss>=-4.17283757528\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:35:01 INFO 140466583250752] Epoch[59] Batch [5]#011Speed: 1738.70 samples/sec#011loss=-4.172838\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:35:01 INFO 140466583250752] processed a total of 1254 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52546.54312133789, \"sum\": 52546.54312133789, \"min\": 52546.54312133789}}, \"EndTime\": 1593880501.851024, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593880449.303942}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:35:01 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=23.8645053139 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:35:01 INFO 140466583250752] #progress_metric: host=algo-1, completed 30 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:35:01 INFO 140466583250752] #quality_metric: host=algo-1, epoch=59, train loss <loss>=-4.21572413445\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:35:01 INFO 140466583250752] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:35:01 INFO 140466583250752] Saved checkpoint to \"/opt/ml/model/state_0cf19cb6-1d3c-41a2-9482-0f360765b86d-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 8.683919906616211, \"sum\": 8.683919906616211, \"min\": 8.683919906616211}}, \"EndTime\": 1593880501.860385, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593880501.851096}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:35:53 INFO 140466583250752] Epoch[60] Batch[0] avg_epoch_loss=-4.212773\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:35:53 INFO 140466583250752] #quality_metric: host=algo-1, epoch=60, batch=0 train loss <loss>=-4.21277332306\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:35:54 INFO 140466583250752] Epoch[60] Batch[5] avg_epoch_loss=-4.238133\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:35:54 INFO 140466583250752] #quality_metric: host=algo-1, epoch=60, batch=5 train loss <loss>=-4.23813311259\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:35:54 INFO 140466583250752] Epoch[60] Batch [5]#011Speed: 1810.74 samples/sec#011loss=-4.238133\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:35:54 INFO 140466583250752] Epoch[60] Batch[10] avg_epoch_loss=-4.228701\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:35:54 INFO 140466583250752] #quality_metric: host=algo-1, epoch=60, batch=10 train loss <loss>=-4.2173833847\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:35:54 INFO 140466583250752] Epoch[60] Batch [10]#011Speed: 1143.56 samples/sec#011loss=-4.217383\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:35:54 INFO 140466583250752] processed a total of 1306 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52781.17299079895, \"sum\": 52781.17299079895, \"min\": 52781.17299079895}}, \"EndTime\": 1593880554.641699, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593880501.860464}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:35:54 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=24.7436156157 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:35:54 INFO 140466583250752] #progress_metric: host=algo-1, completed 30 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:35:54 INFO 140466583250752] #quality_metric: host=algo-1, epoch=60, train loss <loss>=-4.2287014181\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:35:54 INFO 140466583250752] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:35:54 INFO 140466583250752] Saved checkpoint to \"/opt/ml/model/state_ed834b74-da7d-4ecc-ba55-fce653dbe93e-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 7.949113845825195, \"sum\": 7.949113845825195, \"min\": 7.949113845825195}}, \"EndTime\": 1593880554.650346, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593880554.641782}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:36:46 INFO 140466583250752] Epoch[61] Batch[0] avg_epoch_loss=-4.492440\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:36:46 INFO 140466583250752] #quality_metric: host=algo-1, epoch=61, batch=0 train loss <loss>=-4.49244022369\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:36:46 INFO 140466583250752] Epoch[61] Batch[5] avg_epoch_loss=-4.222762\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:36:46 INFO 140466583250752] #quality_metric: host=algo-1, epoch=61, batch=5 train loss <loss>=-4.2227619489\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:36:46 INFO 140466583250752] Epoch[61] Batch [5]#011Speed: 1795.46 samples/sec#011loss=-4.222762\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:36:46 INFO 140466583250752] processed a total of 1257 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52326.50804519653, \"sum\": 52326.50804519653, \"min\": 52326.50804519653}}, \"EndTime\": 1593880606.977002, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593880554.650425}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:36:46 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=24.0221765622 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:36:46 INFO 140466583250752] #progress_metric: host=algo-1, completed 31 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:36:46 INFO 140466583250752] #quality_metric: host=algo-1, epoch=61, train loss <loss>=-4.24275565147\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:36:46 INFO 140466583250752] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:36:46 INFO 140466583250752] Saved checkpoint to \"/opt/ml/model/state_66f0bbc2-3dcb-481d-b43f-0339f4e004ef-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 8.023977279663086, \"sum\": 8.023977279663086, \"min\": 8.023977279663086}}, \"EndTime\": 1593880606.985772, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593880606.977099}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:37:38 INFO 140466583250752] Epoch[62] Batch[0] avg_epoch_loss=-4.398005\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:37:38 INFO 140466583250752] #quality_metric: host=algo-1, epoch=62, batch=0 train loss <loss>=-4.39800453186\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:37:38 INFO 140466583250752] Epoch[62] Batch[5] avg_epoch_loss=-4.191213\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:37:38 INFO 140466583250752] #quality_metric: host=algo-1, epoch=62, batch=5 train loss <loss>=-4.19121344884\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:37:38 INFO 140466583250752] Epoch[62] Batch [5]#011Speed: 1808.64 samples/sec#011loss=-4.191213\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:37:39 INFO 140466583250752] Epoch[62] Batch[10] avg_epoch_loss=-4.223121\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:37:39 INFO 140466583250752] #quality_metric: host=algo-1, epoch=62, batch=10 train loss <loss>=-4.26141004562\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:37:39 INFO 140466583250752] Epoch[62] Batch [10]#011Speed: 1118.86 samples/sec#011loss=-4.261410\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:37:39 INFO 140466583250752] processed a total of 1302 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52545.66812515259, \"sum\": 52545.66812515259, \"min\": 52545.66812515259}}, \"EndTime\": 1593880659.531572, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593880606.985845}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:37:39 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=24.7783946169 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:37:39 INFO 140466583250752] #progress_metric: host=algo-1, completed 31 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:37:39 INFO 140466583250752] #quality_metric: host=algo-1, epoch=62, train loss <loss>=-4.22312099283\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:37:39 INFO 140466583250752] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:38:31 INFO 140466583250752] Epoch[63] Batch[0] avg_epoch_loss=-4.223205\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:38:31 INFO 140466583250752] #quality_metric: host=algo-1, epoch=63, batch=0 train loss <loss>=-4.22320461273\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:38:31 INFO 140466583250752] Epoch[63] Batch[5] avg_epoch_loss=-4.229718\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:38:31 INFO 140466583250752] #quality_metric: host=algo-1, epoch=63, batch=5 train loss <loss>=-4.22971844673\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:38:31 INFO 140466583250752] Epoch[63] Batch [5]#011Speed: 1806.04 samples/sec#011loss=-4.229718\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:38:31 INFO 140466583250752] processed a total of 1253 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52319.03696060181, \"sum\": 52319.03696060181, \"min\": 52319.03696060181}}, \"EndTime\": 1593880711.851248, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593880659.531644}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:38:31 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=23.9491602881 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:38:31 INFO 140466583250752] #progress_metric: host=algo-1, completed 32 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:38:31 INFO 140466583250752] #quality_metric: host=algo-1, epoch=63, train loss <loss>=-4.21491880417\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:38:31 INFO 140466583250752] loss did not improve\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[07/04/2020 16:39:23 INFO 140466583250752] Epoch[64] Batch[0] avg_epoch_loss=-4.105927\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:39:23 INFO 140466583250752] #quality_metric: host=algo-1, epoch=64, batch=0 train loss <loss>=-4.10592746735\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:39:23 INFO 140466583250752] Epoch[64] Batch[5] avg_epoch_loss=-4.214569\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:39:23 INFO 140466583250752] #quality_metric: host=algo-1, epoch=64, batch=5 train loss <loss>=-4.21456917127\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:39:23 INFO 140466583250752] Epoch[64] Batch [5]#011Speed: 1827.95 samples/sec#011loss=-4.214569\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:39:24 INFO 140466583250752] Epoch[64] Batch[10] avg_epoch_loss=-4.266317\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:39:24 INFO 140466583250752] #quality_metric: host=algo-1, epoch=64, batch=10 train loss <loss>=-4.32841539383\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:39:24 INFO 140466583250752] Epoch[64] Batch [10]#011Speed: 1119.79 samples/sec#011loss=-4.328415\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:39:24 INFO 140466583250752] processed a total of 1297 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52707.326889038086, \"sum\": 52707.326889038086, \"min\": 52707.326889038086}}, \"EndTime\": 1593880764.559269, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593880711.851328}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:39:24 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=24.6075275053 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:39:24 INFO 140466583250752] #progress_metric: host=algo-1, completed 32 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:39:24 INFO 140466583250752] #quality_metric: host=algo-1, epoch=64, train loss <loss>=-4.26631745425\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:39:24 INFO 140466583250752] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:39:24 INFO 140466583250752] Saved checkpoint to \"/opt/ml/model/state_3a029420-5cc9-4aef-a14f-0b29964f20bf-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 8.069992065429688, \"sum\": 8.069992065429688, \"min\": 8.069992065429688}}, \"EndTime\": 1593880764.567994, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593880764.559352}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:40:16 INFO 140466583250752] Epoch[65] Batch[0] avg_epoch_loss=-3.557078\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:40:16 INFO 140466583250752] #quality_metric: host=algo-1, epoch=65, batch=0 train loss <loss>=-3.55707812309\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:40:16 INFO 140466583250752] Epoch[65] Batch[5] avg_epoch_loss=-3.856206\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:40:16 INFO 140466583250752] #quality_metric: host=algo-1, epoch=65, batch=5 train loss <loss>=-3.85620629787\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:40:16 INFO 140466583250752] Epoch[65] Batch [5]#011Speed: 1802.42 samples/sec#011loss=-3.856206\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:40:17 INFO 140466583250752] Epoch[65] Batch[10] avg_epoch_loss=-3.923714\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:40:17 INFO 140466583250752] #quality_metric: host=algo-1, epoch=65, batch=10 train loss <loss>=-4.00472316742\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:40:17 INFO 140466583250752] Epoch[65] Batch [10]#011Speed: 1248.02 samples/sec#011loss=-4.004723\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:40:17 INFO 140466583250752] processed a total of 1361 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52476.83095932007, \"sum\": 52476.83095932007, \"min\": 52476.83095932007}}, \"EndTime\": 1593880817.044959, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593880764.568058}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:40:17 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=25.9351968158 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:40:17 INFO 140466583250752] #progress_metric: host=algo-1, completed 33 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:40:17 INFO 140466583250752] #quality_metric: host=algo-1, epoch=65, train loss <loss>=-3.92371396585\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:40:17 INFO 140466583250752] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:41:09 INFO 140466583250752] Epoch[66] Batch[0] avg_epoch_loss=-4.201944\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:41:09 INFO 140466583250752] #quality_metric: host=algo-1, epoch=66, batch=0 train loss <loss>=-4.2019443512\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:41:09 INFO 140466583250752] Epoch[66] Batch[5] avg_epoch_loss=-4.138503\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:41:09 INFO 140466583250752] #quality_metric: host=algo-1, epoch=66, batch=5 train loss <loss>=-4.13850251834\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:41:09 INFO 140466583250752] Epoch[66] Batch [5]#011Speed: 1820.34 samples/sec#011loss=-4.138503\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:41:09 INFO 140466583250752] Epoch[66] Batch[10] avg_epoch_loss=-4.185055\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:41:09 INFO 140466583250752] #quality_metric: host=algo-1, epoch=66, batch=10 train loss <loss>=-4.24091844559\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:41:09 INFO 140466583250752] Epoch[66] Batch [10]#011Speed: 1173.03 samples/sec#011loss=-4.240918\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:41:09 INFO 140466583250752] processed a total of 1323 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52904.620885849, \"sum\": 52904.620885849, \"min\": 52904.620885849}}, \"EndTime\": 1593880869.950163, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593880817.045034}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:41:09 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=25.0072025769 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:41:09 INFO 140466583250752] #progress_metric: host=algo-1, completed 33 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:41:09 INFO 140466583250752] #quality_metric: host=algo-1, epoch=66, train loss <loss>=-4.18505521254\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:41:09 INFO 140466583250752] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:42:01 INFO 140466583250752] Epoch[67] Batch[0] avg_epoch_loss=-3.876819\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:42:01 INFO 140466583250752] #quality_metric: host=algo-1, epoch=67, batch=0 train loss <loss>=-3.87681865692\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:42:01 INFO 140466583250752] Epoch[67] Batch[5] avg_epoch_loss=-4.099971\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:42:01 INFO 140466583250752] #quality_metric: host=algo-1, epoch=67, batch=5 train loss <loss>=-4.09997065862\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:42:01 INFO 140466583250752] Epoch[67] Batch [5]#011Speed: 1834.12 samples/sec#011loss=-4.099971\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:42:02 INFO 140466583250752] processed a total of 1277 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52313.67588043213, \"sum\": 52313.67588043213, \"min\": 52313.67588043213}}, \"EndTime\": 1593880922.26452, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593880869.95026}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:42:02 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=24.4103830864 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:42:02 INFO 140466583250752] #progress_metric: host=algo-1, completed 34 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:42:02 INFO 140466583250752] #quality_metric: host=algo-1, epoch=67, train loss <loss>=-4.14815626144\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:42:02 INFO 140466583250752] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:42:53 INFO 140466583250752] Epoch[68] Batch[0] avg_epoch_loss=-4.165171\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:42:53 INFO 140466583250752] #quality_metric: host=algo-1, epoch=68, batch=0 train loss <loss>=-4.16517066956\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:42:54 INFO 140466583250752] Epoch[68] Batch[5] avg_epoch_loss=-4.226042\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:42:54 INFO 140466583250752] #quality_metric: host=algo-1, epoch=68, batch=5 train loss <loss>=-4.2260418733\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:42:54 INFO 140466583250752] Epoch[68] Batch [5]#011Speed: 1821.26 samples/sec#011loss=-4.226042\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:42:54 INFO 140466583250752] processed a total of 1202 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52353.64389419556, \"sum\": 52353.64389419556, \"min\": 52353.64389419556}}, \"EndTime\": 1593880974.618814, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593880922.264606}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:42:54 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=22.9591696158 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:42:54 INFO 140466583250752] #progress_metric: host=algo-1, completed 34 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:42:54 INFO 140466583250752] #quality_metric: host=algo-1, epoch=68, train loss <loss>=-4.25521492958\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:42:54 INFO 140466583250752] loss did not improve\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[07/04/2020 16:43:45 INFO 140466583250752] Epoch[69] Batch[0] avg_epoch_loss=-4.153598\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:43:45 INFO 140466583250752] #quality_metric: host=algo-1, epoch=69, batch=0 train loss <loss>=-4.15359783173\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:43:46 INFO 140466583250752] Epoch[69] Batch[5] avg_epoch_loss=-4.242741\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:43:46 INFO 140466583250752] #quality_metric: host=algo-1, epoch=69, batch=5 train loss <loss>=-4.24274110794\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:43:46 INFO 140466583250752] Epoch[69] Batch [5]#011Speed: 1811.64 samples/sec#011loss=-4.242741\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:43:46 INFO 140466583250752] processed a total of 1246 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52086.498975753784, \"sum\": 52086.498975753784, \"min\": 52086.498975753784}}, \"EndTime\": 1593881026.706016, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593880974.618937}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:43:46 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=23.9216895272 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:43:46 INFO 140466583250752] #progress_metric: host=algo-1, completed 35 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:43:46 INFO 140466583250752] #quality_metric: host=algo-1, epoch=69, train loss <loss>=-4.2582942009\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:43:46 INFO 140466583250752] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:44:38 INFO 140466583250752] Epoch[70] Batch[0] avg_epoch_loss=-4.246548\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:44:38 INFO 140466583250752] #quality_metric: host=algo-1, epoch=70, batch=0 train loss <loss>=-4.24654817581\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:44:38 INFO 140466583250752] Epoch[70] Batch[5] avg_epoch_loss=-4.252617\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:44:38 INFO 140466583250752] #quality_metric: host=algo-1, epoch=70, batch=5 train loss <loss>=-4.25261727969\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:44:38 INFO 140466583250752] Epoch[70] Batch [5]#011Speed: 1817.61 samples/sec#011loss=-4.252617\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:44:39 INFO 140466583250752] Epoch[70] Batch[10] avg_epoch_loss=-4.293093\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:44:39 INFO 140466583250752] #quality_metric: host=algo-1, epoch=70, batch=10 train loss <loss>=-4.34166326523\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:44:39 INFO 140466583250752] Epoch[70] Batch [10]#011Speed: 1067.77 samples/sec#011loss=-4.341663\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:44:39 INFO 140466583250752] processed a total of 1287 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52348.149061203, \"sum\": 52348.149061203, \"min\": 52348.149061203}}, \"EndTime\": 1593881079.054839, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593881026.706094}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:44:39 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=24.5853322137 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:44:39 INFO 140466583250752] #progress_metric: host=algo-1, completed 35 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:44:39 INFO 140466583250752] #quality_metric: host=algo-1, epoch=70, train loss <loss>=-4.29309272766\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:44:39 INFO 140466583250752] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:44:39 INFO 140466583250752] Saved checkpoint to \"/opt/ml/model/state_7b346d98-6239-43a5-945e-29d9973e9c82-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 8.203983306884766, \"sum\": 8.203983306884766, \"min\": 8.203983306884766}}, \"EndTime\": 1593881079.063742, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593881079.054933}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:45:30 INFO 140466583250752] Epoch[71] Batch[0] avg_epoch_loss=-4.056395\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:45:30 INFO 140466583250752] #quality_metric: host=algo-1, epoch=71, batch=0 train loss <loss>=-4.05639457703\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:45:31 INFO 140466583250752] Epoch[71] Batch[5] avg_epoch_loss=-4.179523\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:45:31 INFO 140466583250752] #quality_metric: host=algo-1, epoch=71, batch=5 train loss <loss>=-4.17952283223\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:45:31 INFO 140466583250752] Epoch[71] Batch [5]#011Speed: 1800.97 samples/sec#011loss=-4.179523\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:45:31 INFO 140466583250752] processed a total of 1253 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52379.558801651, \"sum\": 52379.558801651, \"min\": 52379.558801651}}, \"EndTime\": 1593881131.44344, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593881079.063815}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:45:31 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=23.9214896585 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:45:31 INFO 140466583250752] #progress_metric: host=algo-1, completed 36 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:45:31 INFO 140466583250752] #quality_metric: host=algo-1, epoch=71, train loss <loss>=-4.18055796623\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:45:31 INFO 140466583250752] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:46:23 INFO 140466583250752] Epoch[72] Batch[0] avg_epoch_loss=-4.148842\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:46:23 INFO 140466583250752] #quality_metric: host=algo-1, epoch=72, batch=0 train loss <loss>=-4.14884233475\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:46:23 INFO 140466583250752] Epoch[72] Batch[5] avg_epoch_loss=-4.089063\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:46:23 INFO 140466583250752] #quality_metric: host=algo-1, epoch=72, batch=5 train loss <loss>=-4.08906316757\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:46:23 INFO 140466583250752] Epoch[72] Batch [5]#011Speed: 1810.18 samples/sec#011loss=-4.089063\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:46:24 INFO 140466583250752] processed a total of 1273 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52679.5699596405, \"sum\": 52679.5699596405, \"min\": 52679.5699596405}}, \"EndTime\": 1593881184.123703, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593881131.443518}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:46:24 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=24.1649102292 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:46:24 INFO 140466583250752] #progress_metric: host=algo-1, completed 36 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:46:24 INFO 140466583250752] #quality_metric: host=algo-1, epoch=72, train loss <loss>=-4.0807135582\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:46:24 INFO 140466583250752] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:47:15 INFO 140466583250752] Epoch[73] Batch[0] avg_epoch_loss=-4.198950\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:47:15 INFO 140466583250752] #quality_metric: host=algo-1, epoch=73, batch=0 train loss <loss>=-4.19895029068\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:47:15 INFO 140466583250752] Epoch[73] Batch[5] avg_epoch_loss=-4.211623\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:47:15 INFO 140466583250752] #quality_metric: host=algo-1, epoch=73, batch=5 train loss <loss>=-4.211622715\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:47:15 INFO 140466583250752] Epoch[73] Batch [5]#011Speed: 1810.96 samples/sec#011loss=-4.211623\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:47:16 INFO 140466583250752] Epoch[73] Batch[10] avg_epoch_loss=-4.189528\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:47:16 INFO 140466583250752] #quality_metric: host=algo-1, epoch=73, batch=10 train loss <loss>=-4.16301460266\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:47:16 INFO 140466583250752] Epoch[73] Batch [10]#011Speed: 1100.52 samples/sec#011loss=-4.163015\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:47:16 INFO 140466583250752] processed a total of 1296 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52430.147886276245, \"sum\": 52430.147886276245, \"min\": 52430.147886276245}}, \"EndTime\": 1593881236.554518, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593881184.123781}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:47:16 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=24.7185428821 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:47:16 INFO 140466583250752] #progress_metric: host=algo-1, completed 37 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:47:16 INFO 140466583250752] #quality_metric: host=algo-1, epoch=73, train loss <loss>=-4.18952811848\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:47:16 INFO 140466583250752] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:48:08 INFO 140466583250752] Epoch[74] Batch[0] avg_epoch_loss=-4.051128\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:48:08 INFO 140466583250752] #quality_metric: host=algo-1, epoch=74, batch=0 train loss <loss>=-4.05112838745\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:48:08 INFO 140466583250752] Epoch[74] Batch[5] avg_epoch_loss=-4.157990\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:48:08 INFO 140466583250752] #quality_metric: host=algo-1, epoch=74, batch=5 train loss <loss>=-4.15799045563\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:48:08 INFO 140466583250752] Epoch[74] Batch [5]#011Speed: 1821.21 samples/sec#011loss=-4.157990\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:48:09 INFO 140466583250752] processed a total of 1256 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52631.58202171326, \"sum\": 52631.58202171326, \"min\": 52631.58202171326}}, \"EndTime\": 1593881289.186732, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593881236.554601}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:48:09 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=23.8639365552 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:48:09 INFO 140466583250752] #progress_metric: host=algo-1, completed 37 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:48:09 INFO 140466583250752] #quality_metric: host=algo-1, epoch=74, train loss <loss>=-4.20420117378\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:48:09 INFO 140466583250752] loss did not improve\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[07/04/2020 16:49:00 INFO 140466583250752] Epoch[75] Batch[0] avg_epoch_loss=-4.269294\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:49:00 INFO 140466583250752] #quality_metric: host=algo-1, epoch=75, batch=0 train loss <loss>=-4.2692937851\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:49:01 INFO 140466583250752] Epoch[75] Batch[5] avg_epoch_loss=-4.306075\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:49:01 INFO 140466583250752] #quality_metric: host=algo-1, epoch=75, batch=5 train loss <loss>=-4.30607461929\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:49:01 INFO 140466583250752] Epoch[75] Batch [5]#011Speed: 1808.85 samples/sec#011loss=-4.306075\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:49:01 INFO 140466583250752] processed a total of 1240 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52306.779861450195, \"sum\": 52306.779861450195, \"min\": 52306.779861450195}}, \"EndTime\": 1593881341.494165, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593881289.186824}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:49:01 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=23.7062445553 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:49:01 INFO 140466583250752] #progress_metric: host=algo-1, completed 38 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:49:01 INFO 140466583250752] #quality_metric: host=algo-1, epoch=75, train loss <loss>=-4.27689275742\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:49:01 INFO 140466583250752] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:49:53 INFO 140466583250752] Epoch[76] Batch[0] avg_epoch_loss=-4.119075\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:49:53 INFO 140466583250752] #quality_metric: host=algo-1, epoch=76, batch=0 train loss <loss>=-4.11907529831\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:49:53 INFO 140466583250752] Epoch[76] Batch[5] avg_epoch_loss=-4.252340\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:49:53 INFO 140466583250752] #quality_metric: host=algo-1, epoch=76, batch=5 train loss <loss>=-4.25233952204\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:49:53 INFO 140466583250752] Epoch[76] Batch [5]#011Speed: 1805.28 samples/sec#011loss=-4.252340\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:49:53 INFO 140466583250752] processed a total of 1267 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52340.75999259949, \"sum\": 52340.75999259949, \"min\": 52340.75999259949}}, \"EndTime\": 1593881393.835573, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593881341.494234}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:49:53 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=24.2066943579 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:49:53 INFO 140466583250752] #progress_metric: host=algo-1, completed 38 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:49:53 INFO 140466583250752] #quality_metric: host=algo-1, epoch=76, train loss <loss>=-4.25801143646\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:49:53 INFO 140466583250752] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:50:45 INFO 140466583250752] Epoch[77] Batch[0] avg_epoch_loss=-4.276871\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:50:45 INFO 140466583250752] #quality_metric: host=algo-1, epoch=77, batch=0 train loss <loss>=-4.27687072754\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:50:45 INFO 140466583250752] Epoch[77] Batch[5] avg_epoch_loss=-4.334605\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:50:45 INFO 140466583250752] #quality_metric: host=algo-1, epoch=77, batch=5 train loss <loss>=-4.33460513751\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:50:45 INFO 140466583250752] Epoch[77] Batch [5]#011Speed: 1804.00 samples/sec#011loss=-4.334605\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:50:46 INFO 140466583250752] processed a total of 1237 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52352.169036865234, \"sum\": 52352.169036865234, \"min\": 52352.169036865234}}, \"EndTime\": 1593881446.18847, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593881393.835663}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:50:46 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=23.628384251 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:50:46 INFO 140466583250752] #progress_metric: host=algo-1, completed 39 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:50:46 INFO 140466583250752] #quality_metric: host=algo-1, epoch=77, train loss <loss>=-4.25822014809\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:50:46 INFO 140466583250752] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:51:37 INFO 140466583250752] Epoch[78] Batch[0] avg_epoch_loss=-3.928120\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:51:37 INFO 140466583250752] #quality_metric: host=algo-1, epoch=78, batch=0 train loss <loss>=-3.92812013626\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:51:38 INFO 140466583250752] Epoch[78] Batch[5] avg_epoch_loss=-4.204053\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:51:38 INFO 140466583250752] #quality_metric: host=algo-1, epoch=78, batch=5 train loss <loss>=-4.20405276616\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:51:38 INFO 140466583250752] Epoch[78] Batch [5]#011Speed: 1798.65 samples/sec#011loss=-4.204053\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:51:38 INFO 140466583250752] processed a total of 1262 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52376.035928726196, \"sum\": 52376.035928726196, \"min\": 52376.035928726196}}, \"EndTime\": 1593881498.565127, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593881446.188548}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:51:38 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=24.0949395704 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:51:38 INFO 140466583250752] #progress_metric: host=algo-1, completed 39 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:51:38 INFO 140466583250752] #quality_metric: host=algo-1, epoch=78, train loss <loss>=-4.23701486588\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:51:38 INFO 140466583250752] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:52:30 INFO 140466583250752] Epoch[79] Batch[0] avg_epoch_loss=-4.127981\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:52:30 INFO 140466583250752] #quality_metric: host=algo-1, epoch=79, batch=0 train loss <loss>=-4.12798070908\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:52:30 INFO 140466583250752] Epoch[79] Batch[5] avg_epoch_loss=-4.282633\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:52:30 INFO 140466583250752] #quality_metric: host=algo-1, epoch=79, batch=5 train loss <loss>=-4.28263266881\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:52:30 INFO 140466583250752] Epoch[79] Batch [5]#011Speed: 1812.09 samples/sec#011loss=-4.282633\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:52:30 INFO 140466583250752] processed a total of 1256 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52430.91893196106, \"sum\": 52430.91893196106, \"min\": 52430.91893196106}}, \"EndTime\": 1593881550.99669, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593881498.565199}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:52:30 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=23.9552711867 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:52:30 INFO 140466583250752] #progress_metric: host=algo-1, completed 40 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:52:30 INFO 140466583250752] #quality_metric: host=algo-1, epoch=79, train loss <loss>=-4.2772649765\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:52:30 INFO 140466583250752] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:53:22 INFO 140466583250752] Epoch[80] Batch[0] avg_epoch_loss=-4.189991\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:53:22 INFO 140466583250752] #quality_metric: host=algo-1, epoch=80, batch=0 train loss <loss>=-4.18999147415\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:53:22 INFO 140466583250752] Epoch[80] Batch[5] avg_epoch_loss=-4.302704\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:53:22 INFO 140466583250752] #quality_metric: host=algo-1, epoch=80, batch=5 train loss <loss>=-4.30270377795\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:53:22 INFO 140466583250752] Epoch[80] Batch [5]#011Speed: 1813.83 samples/sec#011loss=-4.302704\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:53:22 INFO 140466583250752] processed a total of 1268 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 51805.54914474487, \"sum\": 51805.54914474487, \"min\": 51805.54914474487}}, \"EndTime\": 1593881602.80281, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593881550.996779}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:53:22 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=24.4760900599 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:53:22 INFO 140466583250752] #progress_metric: host=algo-1, completed 40 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:53:22 INFO 140466583250752] #quality_metric: host=algo-1, epoch=80, train loss <loss>=-4.31916179657\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:53:22 INFO 140466583250752] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:53:22 INFO 140466583250752] Saved checkpoint to \"/opt/ml/model/state_fd221572-d633-405b-8b8a-9e35a6e3fca3-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 7.660150527954102, \"sum\": 7.660150527954102, \"min\": 7.660150527954102}}, \"EndTime\": 1593881602.811121, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593881602.802879}\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[07/04/2020 16:54:14 INFO 140466583250752] Epoch[81] Batch[0] avg_epoch_loss=-4.211822\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:54:14 INFO 140466583250752] #quality_metric: host=algo-1, epoch=81, batch=0 train loss <loss>=-4.21182155609\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:54:14 INFO 140466583250752] Epoch[81] Batch[5] avg_epoch_loss=-4.304887\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:54:14 INFO 140466583250752] #quality_metric: host=algo-1, epoch=81, batch=5 train loss <loss>=-4.30488697688\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:54:14 INFO 140466583250752] Epoch[81] Batch [5]#011Speed: 1805.34 samples/sec#011loss=-4.304887\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:54:15 INFO 140466583250752] Epoch[81] Batch[10] avg_epoch_loss=-4.145523\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:54:15 INFO 140466583250752] #quality_metric: host=algo-1, epoch=81, batch=10 train loss <loss>=-3.95428676605\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:54:15 INFO 140466583250752] Epoch[81] Batch [10]#011Speed: 1127.90 samples/sec#011loss=-3.954287\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:54:15 INFO 140466583250752] processed a total of 1307 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52393.11099052429, \"sum\": 52393.11099052429, \"min\": 52393.11099052429}}, \"EndTime\": 1593881655.204367, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593881602.81119}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:54:15 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=24.9459672317 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:54:15 INFO 140466583250752] #progress_metric: host=algo-1, completed 41 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:54:15 INFO 140466583250752] #quality_metric: host=algo-1, epoch=81, train loss <loss>=-4.14552324468\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:54:15 INFO 140466583250752] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:55:07 INFO 140466583250752] Epoch[82] Batch[0] avg_epoch_loss=-2.764450\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:55:07 INFO 140466583250752] #quality_metric: host=algo-1, epoch=82, batch=0 train loss <loss>=-2.76444959641\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:55:07 INFO 140466583250752] Epoch[82] Batch[5] avg_epoch_loss=-3.574988\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:55:07 INFO 140466583250752] #quality_metric: host=algo-1, epoch=82, batch=5 train loss <loss>=-3.57498820623\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:55:07 INFO 140466583250752] Epoch[82] Batch [5]#011Speed: 1805.44 samples/sec#011loss=-3.574988\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:55:08 INFO 140466583250752] Epoch[82] Batch[10] avg_epoch_loss=-3.692773\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:55:08 INFO 140466583250752] #quality_metric: host=algo-1, epoch=82, batch=10 train loss <loss>=-3.83411507607\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:55:08 INFO 140466583250752] Epoch[82] Batch [10]#011Speed: 1225.38 samples/sec#011loss=-3.834115\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:55:08 INFO 140466583250752] processed a total of 1327 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52867.57802963257, \"sum\": 52867.57802963257, \"min\": 52867.57802963257}}, \"EndTime\": 1593881708.072577, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593881655.204452}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:55:08 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=25.1003853849 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:55:08 INFO 140466583250752] #progress_metric: host=algo-1, completed 41 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:55:08 INFO 140466583250752] #quality_metric: host=algo-1, epoch=82, train loss <loss>=-3.69277314706\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:55:08 INFO 140466583250752] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:55:59 INFO 140466583250752] Epoch[83] Batch[0] avg_epoch_loss=-3.963503\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:55:59 INFO 140466583250752] #quality_metric: host=algo-1, epoch=83, batch=0 train loss <loss>=-3.96350312233\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:56:00 INFO 140466583250752] Epoch[83] Batch[5] avg_epoch_loss=-3.916488\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:56:00 INFO 140466583250752] #quality_metric: host=algo-1, epoch=83, batch=5 train loss <loss>=-3.91648773352\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:56:00 INFO 140466583250752] Epoch[83] Batch [5]#011Speed: 1792.38 samples/sec#011loss=-3.916488\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:56:00 INFO 140466583250752] processed a total of 1260 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52573.365926742554, \"sum\": 52573.365926742554, \"min\": 52573.365926742554}}, \"EndTime\": 1593881760.646539, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593881708.072671}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:56:00 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=23.9664528807 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:56:00 INFO 140466583250752] #progress_metric: host=algo-1, completed 42 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:56:00 INFO 140466583250752] #quality_metric: host=algo-1, epoch=83, train loss <loss>=-3.97965917587\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:56:00 INFO 140466583250752] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:56:52 INFO 140466583250752] Epoch[84] Batch[0] avg_epoch_loss=-4.166241\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:56:52 INFO 140466583250752] #quality_metric: host=algo-1, epoch=84, batch=0 train loss <loss>=-4.16624069214\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:56:52 INFO 140466583250752] Epoch[84] Batch[5] avg_epoch_loss=-4.184083\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:56:52 INFO 140466583250752] #quality_metric: host=algo-1, epoch=84, batch=5 train loss <loss>=-4.18408290545\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:56:52 INFO 140466583250752] Epoch[84] Batch [5]#011Speed: 1812.42 samples/sec#011loss=-4.184083\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:56:52 INFO 140466583250752] processed a total of 1243 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52294.46005821228, \"sum\": 52294.46005821228, \"min\": 52294.46005821228}}, \"EndTime\": 1593881812.941556, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593881760.646621}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:56:52 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=23.7691968222 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:56:52 INFO 140466583250752] #progress_metric: host=algo-1, completed 42 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:56:52 INFO 140466583250752] #quality_metric: host=algo-1, epoch=84, train loss <loss>=-4.2102584362\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:56:52 INFO 140466583250752] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:57:45 INFO 140466583250752] Epoch[85] Batch[0] avg_epoch_loss=-4.152129\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:57:45 INFO 140466583250752] #quality_metric: host=algo-1, epoch=85, batch=0 train loss <loss>=-4.15212917328\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:57:45 INFO 140466583250752] Epoch[85] Batch[5] avg_epoch_loss=-4.126836\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:57:45 INFO 140466583250752] #quality_metric: host=algo-1, epoch=85, batch=5 train loss <loss>=-4.12683606148\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:57:45 INFO 140466583250752] Epoch[85] Batch [5]#011Speed: 1817.54 samples/sec#011loss=-4.126836\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:57:45 INFO 140466583250752] processed a total of 1279 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52856.28414154053, \"sum\": 52856.28414154053, \"min\": 52856.28414154053}}, \"EndTime\": 1593881865.798449, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593881812.941635}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:57:45 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=24.1976383486 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:57:45 INFO 140466583250752] #progress_metric: host=algo-1, completed 43 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:57:45 INFO 140466583250752] #quality_metric: host=algo-1, epoch=85, train loss <loss>=-4.14440574646\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:57:45 INFO 140466583250752] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:58:36 INFO 140466583250752] Epoch[86] Batch[0] avg_epoch_loss=-4.316816\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:58:36 INFO 140466583250752] #quality_metric: host=algo-1, epoch=86, batch=0 train loss <loss>=-4.31681632996\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:58:37 INFO 140466583250752] Epoch[86] Batch[5] avg_epoch_loss=-4.304420\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:58:37 INFO 140466583250752] #quality_metric: host=algo-1, epoch=86, batch=5 train loss <loss>=-4.30442023277\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:58:37 INFO 140466583250752] Epoch[86] Batch [5]#011Speed: 1821.75 samples/sec#011loss=-4.304420\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:58:37 INFO 140466583250752] processed a total of 1179 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52027.53806114197, \"sum\": 52027.53806114197, \"min\": 52027.53806114197}}, \"EndTime\": 1593881917.82667, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593881865.798522}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:58:37 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=22.6610282408 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:58:37 INFO 140466583250752] #progress_metric: host=algo-1, completed 43 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:58:37 INFO 140466583250752] #quality_metric: host=algo-1, epoch=86, train loss <loss>=-4.33818278313\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:58:37 INFO 140466583250752] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:58:37 INFO 140466583250752] Saved checkpoint to \"/opt/ml/model/state_753e69aa-a0e7-443f-af7f-daba0319ec3e-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 23.164987564086914, \"sum\": 23.164987564086914, \"min\": 23.164987564086914}}, \"EndTime\": 1593881917.85049, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593881917.826739}\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[07/04/2020 16:59:29 INFO 140466583250752] Epoch[87] Batch[0] avg_epoch_loss=-4.162812\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:59:29 INFO 140466583250752] #quality_metric: host=algo-1, epoch=87, batch=0 train loss <loss>=-4.16281223297\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:59:29 INFO 140466583250752] Epoch[87] Batch[5] avg_epoch_loss=-4.263187\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:59:29 INFO 140466583250752] #quality_metric: host=algo-1, epoch=87, batch=5 train loss <loss>=-4.26318693161\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:59:29 INFO 140466583250752] Epoch[87] Batch [5]#011Speed: 1803.42 samples/sec#011loss=-4.263187\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:59:29 INFO 140466583250752] processed a total of 1241 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52040.87996482849, \"sum\": 52040.87996482849, \"min\": 52040.87996482849}}, \"EndTime\": 1593881969.89152, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593881917.850569}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:59:29 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=23.846573813 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:59:29 INFO 140466583250752] #progress_metric: host=algo-1, completed 44 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:59:29 INFO 140466583250752] #quality_metric: host=algo-1, epoch=87, train loss <loss>=-4.21557657719\u001b[0m\n",
      "\u001b[34m[07/04/2020 16:59:29 INFO 140466583250752] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:00:21 INFO 140466583250752] Epoch[88] Batch[0] avg_epoch_loss=-4.319481\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:00:21 INFO 140466583250752] #quality_metric: host=algo-1, epoch=88, batch=0 train loss <loss>=-4.31948137283\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:00:22 INFO 140466583250752] Epoch[88] Batch[5] avg_epoch_loss=-4.262474\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:00:22 INFO 140466583250752] #quality_metric: host=algo-1, epoch=88, batch=5 train loss <loss>=-4.262474219\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:00:22 INFO 140466583250752] Epoch[88] Batch [5]#011Speed: 1752.22 samples/sec#011loss=-4.262474\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:00:22 INFO 140466583250752] processed a total of 1189 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52693.108797073364, \"sum\": 52693.108797073364, \"min\": 52693.108797073364}}, \"EndTime\": 1593882022.585293, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593881969.891612}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:00:22 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=22.5645689867 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:00:22 INFO 140466583250752] #progress_metric: host=algo-1, completed 44 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:00:22 INFO 140466583250752] #quality_metric: host=algo-1, epoch=88, train loss <loss>=-4.22139906883\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:00:22 INFO 140466583250752] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:01:13 INFO 140466583250752] Epoch[89] Batch[0] avg_epoch_loss=-3.751705\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:01:13 INFO 140466583250752] #quality_metric: host=algo-1, epoch=89, batch=0 train loss <loss>=-3.75170516968\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:01:14 INFO 140466583250752] Epoch[89] Batch[5] avg_epoch_loss=-4.168740\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:01:14 INFO 140466583250752] #quality_metric: host=algo-1, epoch=89, batch=5 train loss <loss>=-4.16873979568\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:01:14 INFO 140466583250752] Epoch[89] Batch [5]#011Speed: 1781.07 samples/sec#011loss=-4.168740\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:01:14 INFO 140466583250752] Epoch[89] Batch[10] avg_epoch_loss=-4.310547\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:01:14 INFO 140466583250752] #quality_metric: host=algo-1, epoch=89, batch=10 train loss <loss>=-4.48071622849\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:01:14 INFO 140466583250752] Epoch[89] Batch [10]#011Speed: 1166.71 samples/sec#011loss=-4.480716\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:01:14 INFO 140466583250752] processed a total of 1302 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52133.94498825073, \"sum\": 52133.94498825073, \"min\": 52133.94498825073}}, \"EndTime\": 1593882074.719902, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593882022.585367}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:01:14 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=24.9740735238 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:01:14 INFO 140466583250752] #progress_metric: host=algo-1, completed 45 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:01:14 INFO 140466583250752] #quality_metric: host=algo-1, epoch=89, train loss <loss>=-4.31054726514\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:01:14 INFO 140466583250752] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:02:06 INFO 140466583250752] Epoch[90] Batch[0] avg_epoch_loss=-4.172597\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:02:06 INFO 140466583250752] #quality_metric: host=algo-1, epoch=90, batch=0 train loss <loss>=-4.17259740829\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:02:06 INFO 140466583250752] Epoch[90] Batch[5] avg_epoch_loss=-4.254956\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:02:06 INFO 140466583250752] #quality_metric: host=algo-1, epoch=90, batch=5 train loss <loss>=-4.25495608648\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:02:06 INFO 140466583250752] Epoch[90] Batch [5]#011Speed: 1801.18 samples/sec#011loss=-4.254956\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:02:07 INFO 140466583250752] Epoch[90] Batch[10] avg_epoch_loss=-4.329778\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:02:07 INFO 140466583250752] #quality_metric: host=algo-1, epoch=90, batch=10 train loss <loss>=-4.4195643425\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:02:07 INFO 140466583250752] Epoch[90] Batch [10]#011Speed: 1122.90 samples/sec#011loss=-4.419564\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:02:07 INFO 140466583250752] processed a total of 1282 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52354.65097427368, \"sum\": 52354.65097427368, \"min\": 52354.65097427368}}, \"EndTime\": 1593882127.075175, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593882074.719988}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:02:07 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=24.4867556497 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:02:07 INFO 140466583250752] #progress_metric: host=algo-1, completed 45 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:02:07 INFO 140466583250752] #quality_metric: host=algo-1, epoch=90, train loss <loss>=-4.32977802103\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:02:07 INFO 140466583250752] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:02:58 INFO 140466583250752] Epoch[91] Batch[0] avg_epoch_loss=-4.259561\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:02:58 INFO 140466583250752] #quality_metric: host=algo-1, epoch=91, batch=0 train loss <loss>=-4.25956106186\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:02:59 INFO 140466583250752] Epoch[91] Batch[5] avg_epoch_loss=-4.235323\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:02:59 INFO 140466583250752] #quality_metric: host=algo-1, epoch=91, batch=5 train loss <loss>=-4.23532334963\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:02:59 INFO 140466583250752] Epoch[91] Batch [5]#011Speed: 1806.68 samples/sec#011loss=-4.235323\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:02:59 INFO 140466583250752] Epoch[91] Batch[10] avg_epoch_loss=-4.221637\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:02:59 INFO 140466583250752] #quality_metric: host=algo-1, epoch=91, batch=10 train loss <loss>=-4.20521287918\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:02:59 INFO 140466583250752] Epoch[91] Batch [10]#011Speed: 1141.85 samples/sec#011loss=-4.205213\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:02:59 INFO 140466583250752] processed a total of 1298 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52743.10803413391, \"sum\": 52743.10803413391, \"min\": 52743.10803413391}}, \"EndTime\": 1593882179.818942, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593882127.075313}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:02:59 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=24.6097879113 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:02:59 INFO 140466583250752] #progress_metric: host=algo-1, completed 46 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:02:59 INFO 140466583250752] #quality_metric: host=algo-1, epoch=91, train loss <loss>=-4.22163677216\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:02:59 INFO 140466583250752] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:03:51 INFO 140466583250752] Epoch[92] Batch[0] avg_epoch_loss=-4.006458\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:03:51 INFO 140466583250752] #quality_metric: host=algo-1, epoch=92, batch=0 train loss <loss>=-4.00645828247\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:03:51 INFO 140466583250752] Epoch[92] Batch[5] avg_epoch_loss=-4.198847\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:03:51 INFO 140466583250752] #quality_metric: host=algo-1, epoch=92, batch=5 train loss <loss>=-4.19884705544\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:03:51 INFO 140466583250752] Epoch[92] Batch [5]#011Speed: 1810.36 samples/sec#011loss=-4.198847\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:03:51 INFO 140466583250752] Epoch[92] Batch[10] avg_epoch_loss=-4.258287\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:03:51 INFO 140466583250752] #quality_metric: host=algo-1, epoch=92, batch=10 train loss <loss>=-4.32961511612\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:03:51 INFO 140466583250752] Epoch[92] Batch [10]#011Speed: 1283.91 samples/sec#011loss=-4.329615\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:03:51 INFO 140466583250752] processed a total of 1362 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52154.2649269104, \"sum\": 52154.2649269104, \"min\": 52154.2649269104}}, \"EndTime\": 1593882231.973871, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593882179.819035}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:03:51 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=26.1147740305 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:03:51 INFO 140466583250752] #progress_metric: host=algo-1, completed 46 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:03:51 INFO 140466583250752] #quality_metric: host=algo-1, epoch=92, train loss <loss>=-4.25828708302\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:03:51 INFO 140466583250752] loss did not improve\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[07/04/2020 17:04:43 INFO 140466583250752] Epoch[93] Batch[0] avg_epoch_loss=-4.476624\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:04:43 INFO 140466583250752] #quality_metric: host=algo-1, epoch=93, batch=0 train loss <loss>=-4.47662448883\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:04:44 INFO 140466583250752] Epoch[93] Batch[5] avg_epoch_loss=-4.304698\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:04:44 INFO 140466583250752] #quality_metric: host=algo-1, epoch=93, batch=5 train loss <loss>=-4.30469751358\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:04:44 INFO 140466583250752] Epoch[93] Batch [5]#011Speed: 1730.04 samples/sec#011loss=-4.304698\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:04:44 INFO 140466583250752] Epoch[93] Batch[10] avg_epoch_loss=-4.264870\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:04:44 INFO 140466583250752] #quality_metric: host=algo-1, epoch=93, batch=10 train loss <loss>=-4.21707744598\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:04:44 INFO 140466583250752] Epoch[93] Batch [10]#011Speed: 1097.99 samples/sec#011loss=-4.217077\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:04:44 INFO 140466583250752] processed a total of 1293 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52700.10495185852, \"sum\": 52700.10495185852, \"min\": 52700.10495185852}}, \"EndTime\": 1593882284.674562, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593882231.973949}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:04:44 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=24.5350001149 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:04:44 INFO 140466583250752] #progress_metric: host=algo-1, completed 47 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:04:44 INFO 140466583250752] #quality_metric: host=algo-1, epoch=93, train loss <loss>=-4.26487021013\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:04:44 INFO 140466583250752] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:05:36 INFO 140466583250752] Epoch[94] Batch[0] avg_epoch_loss=-3.323923\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:05:36 INFO 140466583250752] #quality_metric: host=algo-1, epoch=94, batch=0 train loss <loss>=-3.32392311096\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:05:36 INFO 140466583250752] Epoch[94] Batch[5] avg_epoch_loss=-3.793670\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:05:36 INFO 140466583250752] #quality_metric: host=algo-1, epoch=94, batch=5 train loss <loss>=-3.79367029667\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:05:36 INFO 140466583250752] Epoch[94] Batch [5]#011Speed: 1823.73 samples/sec#011loss=-3.793670\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:05:37 INFO 140466583250752] Epoch[94] Batch[10] avg_epoch_loss=-3.884940\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:05:37 INFO 140466583250752] #quality_metric: host=algo-1, epoch=94, batch=10 train loss <loss>=-3.99446296692\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:05:37 INFO 140466583250752] Epoch[94] Batch [10]#011Speed: 1257.34 samples/sec#011loss=-3.994463\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:05:37 INFO 140466583250752] processed a total of 1352 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52825.438022613525, \"sum\": 52825.438022613525, \"min\": 52825.438022613525}}, \"EndTime\": 1593882337.50065, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593882284.674645}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:05:37 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=25.5936723993 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:05:37 INFO 140466583250752] #progress_metric: host=algo-1, completed 47 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:05:37 INFO 140466583250752] #quality_metric: host=algo-1, epoch=94, train loss <loss>=-3.88493969224\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:05:37 INFO 140466583250752] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:06:29 INFO 140466583250752] Epoch[95] Batch[0] avg_epoch_loss=-4.258837\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:06:29 INFO 140466583250752] #quality_metric: host=algo-1, epoch=95, batch=0 train loss <loss>=-4.25883722305\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:06:29 INFO 140466583250752] Epoch[95] Batch[5] avg_epoch_loss=-4.097968\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:06:29 INFO 140466583250752] #quality_metric: host=algo-1, epoch=95, batch=5 train loss <loss>=-4.09796841939\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:06:29 INFO 140466583250752] Epoch[95] Batch [5]#011Speed: 1824.54 samples/sec#011loss=-4.097968\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:06:29 INFO 140466583250752] processed a total of 1207 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52352.72693634033, \"sum\": 52352.72693634033, \"min\": 52352.72693634033}}, \"EndTime\": 1593882389.854038, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593882337.500734}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:06:29 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=23.0550641161 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:06:29 INFO 140466583250752] #progress_metric: host=algo-1, completed 48 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:06:29 INFO 140466583250752] #quality_metric: host=algo-1, epoch=95, train loss <loss>=-4.05222005844\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:06:29 INFO 140466583250752] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:07:21 INFO 140466583250752] Epoch[96] Batch[0] avg_epoch_loss=-4.098558\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:07:21 INFO 140466583250752] #quality_metric: host=algo-1, epoch=96, batch=0 train loss <loss>=-4.09855794907\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:07:21 INFO 140466583250752] Epoch[96] Batch[5] avg_epoch_loss=-4.206948\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:07:21 INFO 140466583250752] #quality_metric: host=algo-1, epoch=96, batch=5 train loss <loss>=-4.20694788297\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:07:21 INFO 140466583250752] Epoch[96] Batch [5]#011Speed: 1812.22 samples/sec#011loss=-4.206948\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:07:22 INFO 140466583250752] processed a total of 1180 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52557.486057281494, \"sum\": 52557.486057281494, \"min\": 52557.486057281494}}, \"EndTime\": 1593882442.412162, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593882389.854112}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:07:22 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=22.4515580025 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:07:22 INFO 140466583250752] #progress_metric: host=algo-1, completed 48 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:07:22 INFO 140466583250752] #quality_metric: host=algo-1, epoch=96, train loss <loss>=-4.30488286018\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:07:22 INFO 140466583250752] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:08:13 INFO 140466583250752] Epoch[97] Batch[0] avg_epoch_loss=-4.323760\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:08:13 INFO 140466583250752] #quality_metric: host=algo-1, epoch=97, batch=0 train loss <loss>=-4.32375955582\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:08:14 INFO 140466583250752] Epoch[97] Batch[5] avg_epoch_loss=-4.279638\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:08:14 INFO 140466583250752] #quality_metric: host=algo-1, epoch=97, batch=5 train loss <loss>=-4.27963757515\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:08:14 INFO 140466583250752] Epoch[97] Batch [5]#011Speed: 1808.07 samples/sec#011loss=-4.279638\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:08:14 INFO 140466583250752] Epoch[97] Batch[10] avg_epoch_loss=-4.288223\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:08:14 INFO 140466583250752] #quality_metric: host=algo-1, epoch=97, batch=10 train loss <loss>=-4.29852457047\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:08:14 INFO 140466583250752] Epoch[97] Batch [10]#011Speed: 1123.64 samples/sec#011loss=-4.298525\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:08:14 INFO 140466583250752] processed a total of 1294 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52397.10998535156, \"sum\": 52397.10998535156, \"min\": 52397.10998535156}}, \"EndTime\": 1593882494.80991, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593882442.412233}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:08:14 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=24.695958203 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:08:14 INFO 140466583250752] #progress_metric: host=algo-1, completed 49 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:08:14 INFO 140466583250752] #quality_metric: host=algo-1, epoch=97, train loss <loss>=-4.28822257302\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:08:14 INFO 140466583250752] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:09:06 INFO 140466583250752] Epoch[98] Batch[0] avg_epoch_loss=-3.766159\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:09:06 INFO 140466583250752] #quality_metric: host=algo-1, epoch=98, batch=0 train loss <loss>=-3.76615858078\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:09:06 INFO 140466583250752] Epoch[98] Batch[5] avg_epoch_loss=-4.110651\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:09:06 INFO 140466583250752] #quality_metric: host=algo-1, epoch=98, batch=5 train loss <loss>=-4.11065061887\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:09:06 INFO 140466583250752] Epoch[98] Batch [5]#011Speed: 1794.29 samples/sec#011loss=-4.110651\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:09:06 INFO 140466583250752] Epoch[98] Batch[10] avg_epoch_loss=-4.152802\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:09:06 INFO 140466583250752] #quality_metric: host=algo-1, epoch=98, batch=10 train loss <loss>=-4.20338439941\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:09:06 INFO 140466583250752] Epoch[98] Batch [10]#011Speed: 1238.86 samples/sec#011loss=-4.203384\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:09:06 INFO 140466583250752] processed a total of 1316 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52135.5619430542, \"sum\": 52135.5619430542, \"min\": 52135.5619430542}}, \"EndTime\": 1593882546.946087, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593882494.809996}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:09:06 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=25.2418281419 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:09:06 INFO 140466583250752] #progress_metric: host=algo-1, completed 49 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:09:06 INFO 140466583250752] #quality_metric: host=algo-1, epoch=98, train loss <loss>=-4.1528023373\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:09:06 INFO 140466583250752] loss did not improve\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[07/04/2020 17:09:58 INFO 140466583250752] Epoch[99] Batch[0] avg_epoch_loss=-4.461358\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:09:58 INFO 140466583250752] #quality_metric: host=algo-1, epoch=99, batch=0 train loss <loss>=-4.46135807037\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:09:58 INFO 140466583250752] Epoch[99] Batch[5] avg_epoch_loss=-4.385100\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:09:58 INFO 140466583250752] #quality_metric: host=algo-1, epoch=99, batch=5 train loss <loss>=-4.3850997289\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:09:58 INFO 140466583250752] Epoch[99] Batch [5]#011Speed: 1792.81 samples/sec#011loss=-4.385100\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:09:59 INFO 140466583250752] Epoch[99] Batch[10] avg_epoch_loss=-4.383811\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:09:59 INFO 140466583250752] #quality_metric: host=algo-1, epoch=99, batch=10 train loss <loss>=-4.38226442337\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:09:59 INFO 140466583250752] Epoch[99] Batch [10]#011Speed: 1138.45 samples/sec#011loss=-4.382264\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:09:59 INFO 140466583250752] processed a total of 1293 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52301.80621147156, \"sum\": 52301.80621147156, \"min\": 52301.80621147156}}, \"EndTime\": 1593882599.24852, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593882546.94617}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:09:59 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=24.7218382546 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:09:59 INFO 140466583250752] #progress_metric: host=algo-1, completed 50 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:09:59 INFO 140466583250752] #quality_metric: host=algo-1, epoch=99, train loss <loss>=-4.38381095366\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:09:59 INFO 140466583250752] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:09:59 INFO 140466583250752] Saved checkpoint to \"/opt/ml/model/state_088b315c-ff21-4d8a-9ddf-d0b79333e87c-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 9.094953536987305, \"sum\": 9.094953536987305, \"min\": 9.094953536987305}}, \"EndTime\": 1593882599.258167, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593882599.248606}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:10:50 INFO 140466583250752] Epoch[100] Batch[0] avg_epoch_loss=-4.092095\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:10:50 INFO 140466583250752] #quality_metric: host=algo-1, epoch=100, batch=0 train loss <loss>=-4.09209537506\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:10:51 INFO 140466583250752] Epoch[100] Batch[5] avg_epoch_loss=-4.279065\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:10:51 INFO 140466583250752] #quality_metric: host=algo-1, epoch=100, batch=5 train loss <loss>=-4.27906505267\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:10:51 INFO 140466583250752] Epoch[100] Batch [5]#011Speed: 1825.83 samples/sec#011loss=-4.279065\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:10:51 INFO 140466583250752] processed a total of 1252 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52454.98204231262, \"sum\": 52454.98204231262, \"min\": 52454.98204231262}}, \"EndTime\": 1593882651.71326, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593882599.258216}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:10:51 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=23.8680355592 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:10:51 INFO 140466583250752] #progress_metric: host=algo-1, completed 50 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:10:51 INFO 140466583250752] #quality_metric: host=algo-1, epoch=100, train loss <loss>=-4.33991117477\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:10:51 INFO 140466583250752] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:11:43 INFO 140466583250752] Epoch[101] Batch[0] avg_epoch_loss=-4.347501\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:11:43 INFO 140466583250752] #quality_metric: host=algo-1, epoch=101, batch=0 train loss <loss>=-4.34750080109\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:11:43 INFO 140466583250752] Epoch[101] Batch[5] avg_epoch_loss=-4.487295\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:11:43 INFO 140466583250752] #quality_metric: host=algo-1, epoch=101, batch=5 train loss <loss>=-4.48729546865\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:11:43 INFO 140466583250752] Epoch[101] Batch [5]#011Speed: 1703.18 samples/sec#011loss=-4.487295\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:11:44 INFO 140466583250752] processed a total of 1252 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52385.63108444214, \"sum\": 52385.63108444214, \"min\": 52385.63108444214}}, \"EndTime\": 1593882704.099549, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593882651.71333}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:11:44 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=23.8996205748 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:11:44 INFO 140466583250752] #progress_metric: host=algo-1, completed 51 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:11:44 INFO 140466583250752] #quality_metric: host=algo-1, epoch=101, train loss <loss>=-4.46780471802\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:11:44 INFO 140466583250752] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:11:44 INFO 140466583250752] Saved checkpoint to \"/opt/ml/model/state_7caf8e90-f8ef-4e77-b21c-e917eba44436-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 7.873058319091797, \"sum\": 7.873058319091797, \"min\": 7.873058319091797}}, \"EndTime\": 1593882704.108147, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593882704.099642}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:12:36 INFO 140466583250752] Epoch[102] Batch[0] avg_epoch_loss=-4.311042\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:12:36 INFO 140466583250752] #quality_metric: host=algo-1, epoch=102, batch=0 train loss <loss>=-4.31104183197\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:12:36 INFO 140466583250752] Epoch[102] Batch[5] avg_epoch_loss=-4.365188\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:12:36 INFO 140466583250752] #quality_metric: host=algo-1, epoch=102, batch=5 train loss <loss>=-4.36518772443\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:12:36 INFO 140466583250752] Epoch[102] Batch [5]#011Speed: 1817.45 samples/sec#011loss=-4.365188\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:12:36 INFO 140466583250752] processed a total of 1268 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52714.09296989441, \"sum\": 52714.09296989441, \"min\": 52714.09296989441}}, \"EndTime\": 1593882756.822378, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593882704.108218}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:12:36 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=24.0542272431 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:12:36 INFO 140466583250752] #progress_metric: host=algo-1, completed 51 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:12:36 INFO 140466583250752] #quality_metric: host=algo-1, epoch=102, train loss <loss>=-4.32991080284\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:12:36 INFO 140466583250752] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:13:28 INFO 140466583250752] Epoch[103] Batch[0] avg_epoch_loss=-4.299445\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:13:28 INFO 140466583250752] #quality_metric: host=algo-1, epoch=103, batch=0 train loss <loss>=-4.29944467545\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:13:28 INFO 140466583250752] Epoch[103] Batch[5] avg_epoch_loss=-4.367992\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:13:28 INFO 140466583250752] #quality_metric: host=algo-1, epoch=103, batch=5 train loss <loss>=-4.36799152692\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:13:28 INFO 140466583250752] Epoch[103] Batch [5]#011Speed: 1817.69 samples/sec#011loss=-4.367992\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:13:29 INFO 140466583250752] processed a total of 1265 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52462.86201477051, \"sum\": 52462.86201477051, \"min\": 52462.86201477051}}, \"EndTime\": 1593882809.285864, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593882756.82247}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:13:29 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=24.1122315598 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:13:29 INFO 140466583250752] #progress_metric: host=algo-1, completed 52 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:13:29 INFO 140466583250752] #quality_metric: host=algo-1, epoch=103, train loss <loss>=-4.38776192665\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:13:29 INFO 140466583250752] loss did not improve\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[07/04/2020 17:14:21 INFO 140466583250752] Epoch[104] Batch[0] avg_epoch_loss=-4.417675\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:14:21 INFO 140466583250752] #quality_metric: host=algo-1, epoch=104, batch=0 train loss <loss>=-4.41767501831\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:14:21 INFO 140466583250752] Epoch[104] Batch[5] avg_epoch_loss=-4.393484\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:14:21 INFO 140466583250752] #quality_metric: host=algo-1, epoch=104, batch=5 train loss <loss>=-4.39348435402\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:14:21 INFO 140466583250752] Epoch[104] Batch [5]#011Speed: 1835.67 samples/sec#011loss=-4.393484\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:14:22 INFO 140466583250752] Epoch[104] Batch[10] avg_epoch_loss=-4.331566\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:14:22 INFO 140466583250752] #quality_metric: host=algo-1, epoch=104, batch=10 train loss <loss>=-4.25726318359\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:14:22 INFO 140466583250752] Epoch[104] Batch [10]#011Speed: 1182.00 samples/sec#011loss=-4.257263\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:14:22 INFO 140466583250752] processed a total of 1310 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52763.829946517944, \"sum\": 52763.829946517944, \"min\": 52763.829946517944}}, \"EndTime\": 1593882862.050364, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593882809.285957}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:14:22 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=24.8275507812 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:14:22 INFO 140466583250752] #progress_metric: host=algo-1, completed 52 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:14:22 INFO 140466583250752] #quality_metric: host=algo-1, epoch=104, train loss <loss>=-4.33156564019\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:14:22 INFO 140466583250752] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:15:13 INFO 140466583250752] Epoch[105] Batch[0] avg_epoch_loss=-3.270826\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:15:13 INFO 140466583250752] #quality_metric: host=algo-1, epoch=105, batch=0 train loss <loss>=-3.27082586288\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:15:14 INFO 140466583250752] Epoch[105] Batch[5] avg_epoch_loss=-3.572603\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:15:14 INFO 140466583250752] #quality_metric: host=algo-1, epoch=105, batch=5 train loss <loss>=-3.5726031065\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:15:14 INFO 140466583250752] Epoch[105] Batch [5]#011Speed: 1799.86 samples/sec#011loss=-3.572603\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:15:14 INFO 140466583250752] Epoch[105] Batch[10] avg_epoch_loss=-3.732699\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:15:14 INFO 140466583250752] #quality_metric: host=algo-1, epoch=105, batch=10 train loss <loss>=-3.92481336594\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:15:14 INFO 140466583250752] Epoch[105] Batch [10]#011Speed: 1089.20 samples/sec#011loss=-3.924813\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:15:14 INFO 140466583250752] processed a total of 1284 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52817.394971847534, \"sum\": 52817.394971847534, \"min\": 52817.394971847534}}, \"EndTime\": 1593882914.868393, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593882862.050451}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:15:14 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=24.3101088501 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:15:14 INFO 140466583250752] #progress_metric: host=algo-1, completed 53 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:15:14 INFO 140466583250752] #quality_metric: host=algo-1, epoch=105, train loss <loss>=-3.73269867897\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:15:14 INFO 140466583250752] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:16:06 INFO 140466583250752] Epoch[106] Batch[0] avg_epoch_loss=-3.963795\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:16:06 INFO 140466583250752] #quality_metric: host=algo-1, epoch=106, batch=0 train loss <loss>=-3.96379470825\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:16:06 INFO 140466583250752] Epoch[106] Batch[5] avg_epoch_loss=-3.894427\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:16:06 INFO 140466583250752] #quality_metric: host=algo-1, epoch=106, batch=5 train loss <loss>=-3.89442722003\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:16:06 INFO 140466583250752] Epoch[106] Batch [5]#011Speed: 1803.43 samples/sec#011loss=-3.894427\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:16:07 INFO 140466583250752] processed a total of 1274 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52467.353105545044, \"sum\": 52467.353105545044, \"min\": 52467.353105545044}}, \"EndTime\": 1593882967.336399, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593882914.868487}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:16:07 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=24.2817024002 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:16:07 INFO 140466583250752] #progress_metric: host=algo-1, completed 53 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:16:07 INFO 140466583250752] #quality_metric: host=algo-1, epoch=106, train loss <loss>=-3.9208615303\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:16:07 INFO 140466583250752] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:16:59 INFO 140466583250752] Epoch[107] Batch[0] avg_epoch_loss=-3.984622\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:16:59 INFO 140466583250752] #quality_metric: host=algo-1, epoch=107, batch=0 train loss <loss>=-3.98462224007\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:16:59 INFO 140466583250752] Epoch[107] Batch[5] avg_epoch_loss=-4.022343\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:16:59 INFO 140466583250752] #quality_metric: host=algo-1, epoch=107, batch=5 train loss <loss>=-4.02234260241\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:16:59 INFO 140466583250752] Epoch[107] Batch [5]#011Speed: 1808.34 samples/sec#011loss=-4.022343\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:17:00 INFO 140466583250752] Epoch[107] Batch[10] avg_epoch_loss=-4.194098\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:17:00 INFO 140466583250752] #quality_metric: host=algo-1, epoch=107, batch=10 train loss <loss>=-4.40020465851\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:17:00 INFO 140466583250752] Epoch[107] Batch [10]#011Speed: 1142.35 samples/sec#011loss=-4.400205\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:17:00 INFO 140466583250752] processed a total of 1309 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52743.39818954468, \"sum\": 52743.39818954468, \"min\": 52743.39818954468}}, \"EndTime\": 1593883020.080437, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593882967.336493}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:17:00 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=24.8182122696 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:17:00 INFO 140466583250752] #progress_metric: host=algo-1, completed 54 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:17:00 INFO 140466583250752] #quality_metric: host=algo-1, epoch=107, train loss <loss>=-4.19409808246\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:17:00 INFO 140466583250752] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:17:51 INFO 140466583250752] Epoch[108] Batch[0] avg_epoch_loss=-4.309373\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:17:51 INFO 140466583250752] #quality_metric: host=algo-1, epoch=108, batch=0 train loss <loss>=-4.30937290192\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:17:51 INFO 140466583250752] Epoch[108] Batch[5] avg_epoch_loss=-4.257129\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:17:51 INFO 140466583250752] #quality_metric: host=algo-1, epoch=108, batch=5 train loss <loss>=-4.25712943077\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:17:51 INFO 140466583250752] Epoch[108] Batch [5]#011Speed: 1827.53 samples/sec#011loss=-4.257129\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:17:52 INFO 140466583250752] processed a total of 1261 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52161.3929271698, \"sum\": 52161.3929271698, \"min\": 52161.3929271698}}, \"EndTime\": 1593883072.242453, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593883020.080523}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:17:52 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=24.1749140943 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:17:52 INFO 140466583250752] #progress_metric: host=algo-1, completed 54 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:17:52 INFO 140466583250752] #quality_metric: host=algo-1, epoch=108, train loss <loss>=-4.27756080627\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:17:52 INFO 140466583250752] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:18:43 INFO 140466583250752] Epoch[109] Batch[0] avg_epoch_loss=-4.095638\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:18:43 INFO 140466583250752] #quality_metric: host=algo-1, epoch=109, batch=0 train loss <loss>=-4.09563827515\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:18:44 INFO 140466583250752] Epoch[109] Batch[5] avg_epoch_loss=-4.313660\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:18:44 INFO 140466583250752] #quality_metric: host=algo-1, epoch=109, batch=5 train loss <loss>=-4.3136604627\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:18:44 INFO 140466583250752] Epoch[109] Batch [5]#011Speed: 1779.66 samples/sec#011loss=-4.313660\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:18:44 INFO 140466583250752] processed a total of 1248 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52520.50709724426, \"sum\": 52520.50709724426, \"min\": 52520.50709724426}}, \"EndTime\": 1593883124.763565, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593883072.242527}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:18:44 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=23.7620908227 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:18:44 INFO 140466583250752] #progress_metric: host=algo-1, completed 55 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:18:44 INFO 140466583250752] #quality_metric: host=algo-1, epoch=109, train loss <loss>=-4.32915802002\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:18:44 INFO 140466583250752] loss did not improve\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[07/04/2020 17:19:36 INFO 140466583250752] Epoch[110] Batch[0] avg_epoch_loss=-4.058572\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:19:36 INFO 140466583250752] #quality_metric: host=algo-1, epoch=110, batch=0 train loss <loss>=-4.05857181549\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:19:36 INFO 140466583250752] Epoch[110] Batch[5] avg_epoch_loss=-4.245801\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:19:36 INFO 140466583250752] #quality_metric: host=algo-1, epoch=110, batch=5 train loss <loss>=-4.24580093225\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:19:36 INFO 140466583250752] Epoch[110] Batch [5]#011Speed: 1820.37 samples/sec#011loss=-4.245801\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:19:37 INFO 140466583250752] processed a total of 1278 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52283.75697135925, \"sum\": 52283.75697135925, \"min\": 52283.75697135925}}, \"EndTime\": 1593883177.047992, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593883124.763643}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:19:37 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=24.4434766886 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:19:37 INFO 140466583250752] #progress_metric: host=algo-1, completed 55 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:19:37 INFO 140466583250752] #quality_metric: host=algo-1, epoch=110, train loss <loss>=-4.28377702236\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:19:37 INFO 140466583250752] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:20:29 INFO 140466583250752] Epoch[111] Batch[0] avg_epoch_loss=-4.411918\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:20:29 INFO 140466583250752] #quality_metric: host=algo-1, epoch=111, batch=0 train loss <loss>=-4.41191768646\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:20:29 INFO 140466583250752] Epoch[111] Batch[5] avg_epoch_loss=-4.334561\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:20:29 INFO 140466583250752] #quality_metric: host=algo-1, epoch=111, batch=5 train loss <loss>=-4.33456118902\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:20:29 INFO 140466583250752] Epoch[111] Batch [5]#011Speed: 1828.77 samples/sec#011loss=-4.334561\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:20:30 INFO 140466583250752] Epoch[111] Batch[10] avg_epoch_loss=-4.384616\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:20:30 INFO 140466583250752] #quality_metric: host=algo-1, epoch=111, batch=10 train loss <loss>=-4.44468269348\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:20:30 INFO 140466583250752] Epoch[111] Batch [10]#011Speed: 1120.56 samples/sec#011loss=-4.444683\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:20:30 INFO 140466583250752] processed a total of 1303 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 53148.90217781067, \"sum\": 53148.90217781067, \"min\": 53148.90217781067}}, \"EndTime\": 1593883230.197578, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593883177.048081}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:20:30 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=24.5159732824 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:20:30 INFO 140466583250752] #progress_metric: host=algo-1, completed 56 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:20:30 INFO 140466583250752] #quality_metric: host=algo-1, epoch=111, train loss <loss>=-4.38461641832\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:20:30 INFO 140466583250752] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:21:21 INFO 140466583250752] Epoch[112] Batch[0] avg_epoch_loss=-4.316948\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:21:21 INFO 140466583250752] #quality_metric: host=algo-1, epoch=112, batch=0 train loss <loss>=-4.31694793701\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:21:22 INFO 140466583250752] Epoch[112] Batch[5] avg_epoch_loss=-4.364997\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:21:22 INFO 140466583250752] #quality_metric: host=algo-1, epoch=112, batch=5 train loss <loss>=-4.36499738693\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:21:22 INFO 140466583250752] Epoch[112] Batch [5]#011Speed: 1810.48 samples/sec#011loss=-4.364997\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:21:22 INFO 140466583250752] processed a total of 1212 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52488.87610435486, \"sum\": 52488.87610435486, \"min\": 52488.87610435486}}, \"EndTime\": 1593883282.686998, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593883230.197661}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:21:22 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=23.0905510115 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:21:22 INFO 140466583250752] #progress_metric: host=algo-1, completed 56 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:21:22 INFO 140466583250752] #quality_metric: host=algo-1, epoch=112, train loss <loss>=-4.2691783905\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:21:22 INFO 140466583250752] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:22:14 INFO 140466583250752] Epoch[113] Batch[0] avg_epoch_loss=-4.430153\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:22:14 INFO 140466583250752] #quality_metric: host=algo-1, epoch=113, batch=0 train loss <loss>=-4.43015289307\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:22:14 INFO 140466583250752] Epoch[113] Batch[5] avg_epoch_loss=-4.328000\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:22:14 INFO 140466583250752] #quality_metric: host=algo-1, epoch=113, batch=5 train loss <loss>=-4.32800046603\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:22:14 INFO 140466583250752] Epoch[113] Batch [5]#011Speed: 1803.71 samples/sec#011loss=-4.328000\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:22:15 INFO 140466583250752] Epoch[113] Batch[10] avg_epoch_loss=-4.407155\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:22:15 INFO 140466583250752] #quality_metric: host=algo-1, epoch=113, batch=10 train loss <loss>=-4.50214033127\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:22:15 INFO 140466583250752] Epoch[113] Batch [10]#011Speed: 1203.70 samples/sec#011loss=-4.502140\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:22:15 INFO 140466583250752] processed a total of 1323 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52439.29696083069, \"sum\": 52439.29696083069, \"min\": 52439.29696083069}}, \"EndTime\": 1593883335.126939, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593883282.68708}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:22:15 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=25.2291157851 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:22:15 INFO 140466583250752] #progress_metric: host=algo-1, completed 57 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:22:15 INFO 140466583250752] #quality_metric: host=algo-1, epoch=113, train loss <loss>=-4.40715495023\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:22:15 INFO 140466583250752] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:23:06 INFO 140466583250752] Epoch[114] Batch[0] avg_epoch_loss=-4.182889\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:23:06 INFO 140466583250752] #quality_metric: host=algo-1, epoch=114, batch=0 train loss <loss>=-4.18288946152\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:23:06 INFO 140466583250752] Epoch[114] Batch[5] avg_epoch_loss=-4.316475\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:23:06 INFO 140466583250752] #quality_metric: host=algo-1, epoch=114, batch=5 train loss <loss>=-4.31647459666\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:23:06 INFO 140466583250752] Epoch[114] Batch [5]#011Speed: 1824.75 samples/sec#011loss=-4.316475\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:23:07 INFO 140466583250752] processed a total of 1247 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52074.305057525635, \"sum\": 52074.305057525635, \"min\": 52074.305057525635}}, \"EndTime\": 1593883387.201844, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593883335.12702}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:23:07 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=23.9464926374 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:23:07 INFO 140466583250752] #progress_metric: host=algo-1, completed 57 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:23:07 INFO 140466583250752] #quality_metric: host=algo-1, epoch=114, train loss <loss>=-4.32146468163\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:23:07 INFO 140466583250752] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:23:58 INFO 140466583250752] Epoch[115] Batch[0] avg_epoch_loss=-4.186905\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:23:58 INFO 140466583250752] #quality_metric: host=algo-1, epoch=115, batch=0 train loss <loss>=-4.18690490723\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:23:58 INFO 140466583250752] Epoch[115] Batch[5] avg_epoch_loss=-4.353666\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:23:58 INFO 140466583250752] #quality_metric: host=algo-1, epoch=115, batch=5 train loss <loss>=-4.3536661466\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:23:58 INFO 140466583250752] Epoch[115] Batch [5]#011Speed: 1704.77 samples/sec#011loss=-4.353666\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:23:59 INFO 140466583250752] processed a total of 1250 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52067.163944244385, \"sum\": 52067.163944244385, \"min\": 52067.163944244385}}, \"EndTime\": 1593883439.269652, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593883387.201925}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:23:59 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=24.0073899579 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:23:59 INFO 140466583250752] #progress_metric: host=algo-1, completed 58 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:23:59 INFO 140466583250752] #quality_metric: host=algo-1, epoch=115, train loss <loss>=-4.39741210937\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:23:59 INFO 140466583250752] loss did not improve\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[07/04/2020 17:24:50 INFO 140466583250752] Epoch[116] Batch[0] avg_epoch_loss=-4.372274\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:24:50 INFO 140466583250752] #quality_metric: host=algo-1, epoch=116, batch=0 train loss <loss>=-4.3722743988\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:24:51 INFO 140466583250752] Epoch[116] Batch[5] avg_epoch_loss=-4.301935\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:24:51 INFO 140466583250752] #quality_metric: host=algo-1, epoch=116, batch=5 train loss <loss>=-4.30193471909\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:24:51 INFO 140466583250752] Epoch[116] Batch [5]#011Speed: 1818.31 samples/sec#011loss=-4.301935\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:24:51 INFO 140466583250752] Epoch[116] Batch[10] avg_epoch_loss=-4.390605\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:24:51 INFO 140466583250752] #quality_metric: host=algo-1, epoch=116, batch=10 train loss <loss>=-4.49701013565\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:24:51 INFO 140466583250752] Epoch[116] Batch [10]#011Speed: 1176.05 samples/sec#011loss=-4.497010\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:24:51 INFO 140466583250752] processed a total of 1314 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52610.6071472168, \"sum\": 52610.6071472168, \"min\": 52610.6071472168}}, \"EndTime\": 1593883491.880877, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593883439.269745}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:24:51 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=24.9758913558 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:24:51 INFO 140466583250752] #progress_metric: host=algo-1, completed 58 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:24:51 INFO 140466583250752] #quality_metric: host=algo-1, epoch=116, train loss <loss>=-4.39060536298\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:24:51 INFO 140466583250752] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:25:43 INFO 140466583250752] Epoch[117] Batch[0] avg_epoch_loss=-4.429557\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:25:43 INFO 140466583250752] #quality_metric: host=algo-1, epoch=117, batch=0 train loss <loss>=-4.42955684662\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:25:43 INFO 140466583250752] Epoch[117] Batch[5] avg_epoch_loss=-4.429624\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:25:43 INFO 140466583250752] #quality_metric: host=algo-1, epoch=117, batch=5 train loss <loss>=-4.42962392171\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:25:43 INFO 140466583250752] Epoch[117] Batch [5]#011Speed: 1829.17 samples/sec#011loss=-4.429624\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:25:44 INFO 140466583250752] processed a total of 1277 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52383.054971694946, \"sum\": 52383.054971694946, \"min\": 52383.054971694946}}, \"EndTime\": 1593883544.264555, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593883491.880969}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:25:44 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=24.3780551097 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:25:44 INFO 140466583250752] #progress_metric: host=algo-1, completed 59 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:25:44 INFO 140466583250752] #quality_metric: host=algo-1, epoch=117, train loss <loss>=-4.46610155106\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:25:44 INFO 140466583250752] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:26:35 INFO 140466583250752] Epoch[118] Batch[0] avg_epoch_loss=-4.489592\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:26:35 INFO 140466583250752] #quality_metric: host=algo-1, epoch=118, batch=0 train loss <loss>=-4.48959159851\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:26:35 INFO 140466583250752] Epoch[118] Batch[5] avg_epoch_loss=-4.397510\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:26:35 INFO 140466583250752] #quality_metric: host=algo-1, epoch=118, batch=5 train loss <loss>=-4.39750957489\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:26:35 INFO 140466583250752] Epoch[118] Batch [5]#011Speed: 1840.48 samples/sec#011loss=-4.397510\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:26:36 INFO 140466583250752] processed a total of 1270 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52026.169776916504, \"sum\": 52026.169776916504, \"min\": 52026.169776916504}}, \"EndTime\": 1593883596.291384, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593883544.264632}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:26:36 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=24.4107382434 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:26:36 INFO 140466583250752] #progress_metric: host=algo-1, completed 59 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:26:36 INFO 140466583250752] #quality_metric: host=algo-1, epoch=118, train loss <loss>=-4.28940834999\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:26:36 INFO 140466583250752] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:27:28 INFO 140466583250752] Epoch[119] Batch[0] avg_epoch_loss=-4.390212\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:27:28 INFO 140466583250752] #quality_metric: host=algo-1, epoch=119, batch=0 train loss <loss>=-4.39021158218\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:27:28 INFO 140466583250752] Epoch[119] Batch[5] avg_epoch_loss=-4.230199\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:27:28 INFO 140466583250752] #quality_metric: host=algo-1, epoch=119, batch=5 train loss <loss>=-4.23019925753\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:27:28 INFO 140466583250752] Epoch[119] Batch [5]#011Speed: 1818.55 samples/sec#011loss=-4.230199\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:27:29 INFO 140466583250752] Epoch[119] Batch[10] avg_epoch_loss=-4.270770\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:27:29 INFO 140466583250752] #quality_metric: host=algo-1, epoch=119, batch=10 train loss <loss>=-4.31945476532\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:27:29 INFO 140466583250752] Epoch[119] Batch [10]#011Speed: 1250.82 samples/sec#011loss=-4.319455\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:27:29 INFO 140466583250752] processed a total of 1345 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52844.70582008362, \"sum\": 52844.70582008362, \"min\": 52844.70582008362}}, \"EndTime\": 1593883649.136742, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593883596.291454}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:27:29 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=25.4518700167 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:27:29 INFO 140466583250752] #progress_metric: host=algo-1, completed 60 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:27:29 INFO 140466583250752] #quality_metric: host=algo-1, epoch=119, train loss <loss>=-4.27076994289\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:27:29 INFO 140466583250752] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:28:21 INFO 140466583250752] Epoch[120] Batch[0] avg_epoch_loss=-3.916943\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:28:21 INFO 140466583250752] #quality_metric: host=algo-1, epoch=120, batch=0 train loss <loss>=-3.91694283485\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:28:21 INFO 140466583250752] Epoch[120] Batch[5] avg_epoch_loss=-4.148851\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:28:21 INFO 140466583250752] #quality_metric: host=algo-1, epoch=120, batch=5 train loss <loss>=-4.1488506794\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:28:21 INFO 140466583250752] Epoch[120] Batch [5]#011Speed: 1781.32 samples/sec#011loss=-4.148851\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:28:22 INFO 140466583250752] Epoch[120] Batch[10] avg_epoch_loss=-4.163874\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:28:22 INFO 140466583250752] #quality_metric: host=algo-1, epoch=120, batch=10 train loss <loss>=-4.18190193176\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:28:22 INFO 140466583250752] Epoch[120] Batch [10]#011Speed: 1084.10 samples/sec#011loss=-4.181902\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:28:22 INFO 140466583250752] processed a total of 1281 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 53108.234167099, \"sum\": 53108.234167099, \"min\": 53108.234167099}}, \"EndTime\": 1593883702.245601, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593883649.136831}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:28:22 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=24.1204931196 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:28:22 INFO 140466583250752] #progress_metric: host=algo-1, completed 60 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:28:22 INFO 140466583250752] #quality_metric: host=algo-1, epoch=120, train loss <loss>=-4.16387397593\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:28:22 INFO 140466583250752] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:29:13 INFO 140466583250752] Epoch[121] Batch[0] avg_epoch_loss=-4.422266\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:29:13 INFO 140466583250752] #quality_metric: host=algo-1, epoch=121, batch=0 train loss <loss>=-4.42226648331\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:29:13 INFO 140466583250752] Epoch[121] Batch[5] avg_epoch_loss=-4.323470\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:29:13 INFO 140466583250752] #quality_metric: host=algo-1, epoch=121, batch=5 train loss <loss>=-4.32346995672\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:29:13 INFO 140466583250752] Epoch[121] Batch [5]#011Speed: 1813.94 samples/sec#011loss=-4.323470\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:29:14 INFO 140466583250752] processed a total of 1269 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 51873.68106842041, \"sum\": 51873.68106842041, \"min\": 51873.68106842041}}, \"EndTime\": 1593883754.119948, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593883702.245691}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:29:14 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=24.46321648 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:29:14 INFO 140466583250752] #progress_metric: host=algo-1, completed 61 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:29:14 INFO 140466583250752] #quality_metric: host=algo-1, epoch=121, train loss <loss>=-4.33265995979\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:29:14 INFO 140466583250752] loss did not improve\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[07/04/2020 17:30:06 INFO 140466583250752] Epoch[122] Batch[0] avg_epoch_loss=-4.329181\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:30:06 INFO 140466583250752] #quality_metric: host=algo-1, epoch=122, batch=0 train loss <loss>=-4.32918071747\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:30:06 INFO 140466583250752] Epoch[122] Batch[5] avg_epoch_loss=-4.372017\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:30:06 INFO 140466583250752] #quality_metric: host=algo-1, epoch=122, batch=5 train loss <loss>=-4.37201674779\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:30:06 INFO 140466583250752] Epoch[122] Batch [5]#011Speed: 1749.00 samples/sec#011loss=-4.372017\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:30:06 INFO 140466583250752] processed a total of 1241 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52691.547870635986, \"sum\": 52691.547870635986, \"min\": 52691.547870635986}}, \"EndTime\": 1593883806.812107, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593883754.120023}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:30:06 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=23.5521152044 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:30:06 INFO 140466583250752] #progress_metric: host=algo-1, completed 61 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:30:06 INFO 140466583250752] #quality_metric: host=algo-1, epoch=122, train loss <loss>=-4.40501265526\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:30:06 INFO 140466583250752] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:30:58 INFO 140466583250752] Epoch[123] Batch[0] avg_epoch_loss=-4.548426\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:30:58 INFO 140466583250752] #quality_metric: host=algo-1, epoch=123, batch=0 train loss <loss>=-4.54842567444\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:30:58 INFO 140466583250752] Epoch[123] Batch[5] avg_epoch_loss=-4.376937\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:30:58 INFO 140466583250752] #quality_metric: host=algo-1, epoch=123, batch=5 train loss <loss>=-4.37693675359\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:30:58 INFO 140466583250752] Epoch[123] Batch [5]#011Speed: 1799.41 samples/sec#011loss=-4.376937\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:30:59 INFO 140466583250752] Epoch[123] Batch[10] avg_epoch_loss=-4.409586\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:30:59 INFO 140466583250752] #quality_metric: host=algo-1, epoch=123, batch=10 train loss <loss>=-4.44876613617\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:30:59 INFO 140466583250752] Epoch[123] Batch [10]#011Speed: 1110.60 samples/sec#011loss=-4.448766\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:30:59 INFO 140466583250752] processed a total of 1293 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52359.33494567871, \"sum\": 52359.33494567871, \"min\": 52359.33494567871}}, \"EndTime\": 1593883859.172057, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593883806.812176}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:30:59 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=24.6946844853 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:30:59 INFO 140466583250752] #progress_metric: host=algo-1, completed 62 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:30:59 INFO 140466583250752] #quality_metric: host=algo-1, epoch=123, train loss <loss>=-4.40958647294\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:30:59 INFO 140466583250752] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:31:50 INFO 140466583250752] Epoch[124] Batch[0] avg_epoch_loss=-4.466763\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:31:50 INFO 140466583250752] #quality_metric: host=algo-1, epoch=124, batch=0 train loss <loss>=-4.4667634964\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:31:51 INFO 140466583250752] Epoch[124] Batch[5] avg_epoch_loss=-4.522967\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:31:51 INFO 140466583250752] #quality_metric: host=algo-1, epoch=124, batch=5 train loss <loss>=-4.52296717962\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:31:51 INFO 140466583250752] Epoch[124] Batch [5]#011Speed: 1802.49 samples/sec#011loss=-4.522967\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:31:51 INFO 140466583250752] Epoch[124] Batch[10] avg_epoch_loss=-4.424806\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:31:51 INFO 140466583250752] #quality_metric: host=algo-1, epoch=124, batch=10 train loss <loss>=-4.30701360703\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:31:51 INFO 140466583250752] Epoch[124] Batch [10]#011Speed: 1117.14 samples/sec#011loss=-4.307014\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:31:51 INFO 140466583250752] processed a total of 1299 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52549.39794540405, \"sum\": 52549.39794540405, \"min\": 52549.39794540405}}, \"EndTime\": 1593883911.722041, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593883859.172127}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:31:51 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=24.7195360125 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:31:51 INFO 140466583250752] #progress_metric: host=algo-1, completed 62 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:31:51 INFO 140466583250752] #quality_metric: host=algo-1, epoch=124, train loss <loss>=-4.4248064648\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:31:51 INFO 140466583250752] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:32:43 INFO 140466583250752] Epoch[125] Batch[0] avg_epoch_loss=-3.965540\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:32:43 INFO 140466583250752] #quality_metric: host=algo-1, epoch=125, batch=0 train loss <loss>=-3.96553969383\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:32:43 INFO 140466583250752] Epoch[125] Batch[5] avg_epoch_loss=-4.211252\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:32:43 INFO 140466583250752] #quality_metric: host=algo-1, epoch=125, batch=5 train loss <loss>=-4.21125153701\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:32:43 INFO 140466583250752] Epoch[125] Batch [5]#011Speed: 1785.06 samples/sec#011loss=-4.211252\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:32:44 INFO 140466583250752] Epoch[125] Batch[10] avg_epoch_loss=-4.383337\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:32:44 INFO 140466583250752] #quality_metric: host=algo-1, epoch=125, batch=10 train loss <loss>=-4.58983945847\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:32:44 INFO 140466583250752] Epoch[125] Batch [10]#011Speed: 1195.53 samples/sec#011loss=-4.589839\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:32:44 INFO 140466583250752] processed a total of 1315 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52753.23700904846, \"sum\": 52753.23700904846, \"min\": 52753.23700904846}}, \"EndTime\": 1593883964.475933, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593883911.722126}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:32:44 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=24.9272933086 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:32:44 INFO 140466583250752] #progress_metric: host=algo-1, completed 63 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:32:44 INFO 140466583250752] #quality_metric: host=algo-1, epoch=125, train loss <loss>=-4.38333695585\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:32:44 INFO 140466583250752] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:33:36 INFO 140466583250752] Epoch[126] Batch[0] avg_epoch_loss=-4.297237\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:33:36 INFO 140466583250752] #quality_metric: host=algo-1, epoch=126, batch=0 train loss <loss>=-4.2972369194\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:33:36 INFO 140466583250752] Epoch[126] Batch[5] avg_epoch_loss=-4.444611\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:33:36 INFO 140466583250752] #quality_metric: host=algo-1, epoch=126, batch=5 train loss <loss>=-4.44461051623\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:33:36 INFO 140466583250752] Epoch[126] Batch [5]#011Speed: 1804.68 samples/sec#011loss=-4.444611\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:33:36 INFO 140466583250752] processed a total of 1261 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52327.747106552124, \"sum\": 52327.747106552124, \"min\": 52327.747106552124}}, \"EndTime\": 1593884016.804256, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593883964.476019}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:33:36 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=24.0980594711 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:33:36 INFO 140466583250752] #progress_metric: host=algo-1, completed 63 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:33:36 INFO 140466583250752] #quality_metric: host=algo-1, epoch=126, train loss <loss>=-4.3898859024\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:33:36 INFO 140466583250752] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:34:28 INFO 140466583250752] Epoch[127] Batch[0] avg_epoch_loss=-4.398912\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:34:28 INFO 140466583250752] #quality_metric: host=algo-1, epoch=127, batch=0 train loss <loss>=-4.39891242981\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:34:28 INFO 140466583250752] Epoch[127] Batch[5] avg_epoch_loss=-4.405146\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:34:28 INFO 140466583250752] #quality_metric: host=algo-1, epoch=127, batch=5 train loss <loss>=-4.40514564514\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:34:28 INFO 140466583250752] Epoch[127] Batch [5]#011Speed: 1725.02 samples/sec#011loss=-4.405146\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:34:29 INFO 140466583250752] processed a total of 1263 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52325.62017440796, \"sum\": 52325.62017440796, \"min\": 52325.62017440796}}, \"EndTime\": 1593884069.13052, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593884016.804338}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:34:29 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=24.1372583653 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:34:29 INFO 140466583250752] #progress_metric: host=algo-1, completed 64 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:34:29 INFO 140466583250752] #quality_metric: host=algo-1, epoch=127, train loss <loss>=-4.41453175545\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:34:29 INFO 140466583250752] loss did not improve\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[07/04/2020 17:35:20 INFO 140466583250752] Epoch[128] Batch[0] avg_epoch_loss=-4.387909\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:35:20 INFO 140466583250752] #quality_metric: host=algo-1, epoch=128, batch=0 train loss <loss>=-4.38790893555\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:35:21 INFO 140466583250752] Epoch[128] Batch[5] avg_epoch_loss=-4.424069\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:35:21 INFO 140466583250752] #quality_metric: host=algo-1, epoch=128, batch=5 train loss <loss>=-4.42406868935\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:35:21 INFO 140466583250752] Epoch[128] Batch [5]#011Speed: 1826.23 samples/sec#011loss=-4.424069\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:35:21 INFO 140466583250752] Epoch[128] Batch[10] avg_epoch_loss=-4.403231\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:35:21 INFO 140466583250752] #quality_metric: host=algo-1, epoch=128, batch=10 train loss <loss>=-4.37822647095\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:35:21 INFO 140466583250752] Epoch[128] Batch [10]#011Speed: 1125.34 samples/sec#011loss=-4.378226\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:35:21 INFO 140466583250752] processed a total of 1284 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52686.00106239319, \"sum\": 52686.00106239319, \"min\": 52686.00106239319}}, \"EndTime\": 1593884121.817182, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593884069.1306}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:35:21 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=24.370740437 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:35:21 INFO 140466583250752] #progress_metric: host=algo-1, completed 64 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:35:21 INFO 140466583250752] #quality_metric: host=algo-1, epoch=128, train loss <loss>=-4.40323131735\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:35:21 INFO 140466583250752] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:36:13 INFO 140466583250752] Epoch[129] Batch[0] avg_epoch_loss=-3.770127\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:36:13 INFO 140466583250752] #quality_metric: host=algo-1, epoch=129, batch=0 train loss <loss>=-3.77012729645\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:36:13 INFO 140466583250752] Epoch[129] Batch[5] avg_epoch_loss=-4.192903\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:36:13 INFO 140466583250752] #quality_metric: host=algo-1, epoch=129, batch=5 train loss <loss>=-4.19290308158\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:36:13 INFO 140466583250752] Epoch[129] Batch [5]#011Speed: 1807.41 samples/sec#011loss=-4.192903\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:36:14 INFO 140466583250752] Epoch[129] Batch[10] avg_epoch_loss=-4.356415\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:36:14 INFO 140466583250752] #quality_metric: host=algo-1, epoch=129, batch=10 train loss <loss>=-4.55262889862\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:36:14 INFO 140466583250752] Epoch[129] Batch [10]#011Speed: 1171.81 samples/sec#011loss=-4.552629\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:36:14 INFO 140466583250752] processed a total of 1332 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52475.55208206177, \"sum\": 52475.55208206177, \"min\": 52475.55208206177}}, \"EndTime\": 1593884174.293367, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593884121.817269}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:36:14 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=25.3831681779 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:36:14 INFO 140466583250752] #progress_metric: host=algo-1, completed 65 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:36:14 INFO 140466583250752] #quality_metric: host=algo-1, epoch=129, train loss <loss>=-4.3564148166\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:36:14 INFO 140466583250752] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:37:06 INFO 140466583250752] Epoch[130] Batch[0] avg_epoch_loss=-4.168766\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:37:06 INFO 140466583250752] #quality_metric: host=algo-1, epoch=130, batch=0 train loss <loss>=-4.16876602173\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:37:06 INFO 140466583250752] Epoch[130] Batch[5] avg_epoch_loss=-4.360809\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:37:06 INFO 140466583250752] #quality_metric: host=algo-1, epoch=130, batch=5 train loss <loss>=-4.36080884933\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:37:06 INFO 140466583250752] Epoch[130] Batch [5]#011Speed: 1816.24 samples/sec#011loss=-4.360809\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:37:07 INFO 140466583250752] processed a total of 1268 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52764.41311836243, \"sum\": 52764.41311836243, \"min\": 52764.41311836243}}, \"EndTime\": 1593884227.058835, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593884174.293464}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:37:07 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=24.0312957261 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:37:07 INFO 140466583250752] #progress_metric: host=algo-1, completed 65 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:37:07 INFO 140466583250752] #quality_metric: host=algo-1, epoch=130, train loss <loss>=-4.39007935524\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:37:07 INFO 140466583250752] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:37:59 INFO 140466583250752] Epoch[131] Batch[0] avg_epoch_loss=-4.048913\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:37:59 INFO 140466583250752] #quality_metric: host=algo-1, epoch=131, batch=0 train loss <loss>=-4.04891300201\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:37:59 INFO 140466583250752] Epoch[131] Batch[5] avg_epoch_loss=-4.257189\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:37:59 INFO 140466583250752] #quality_metric: host=algo-1, epoch=131, batch=5 train loss <loss>=-4.25718895594\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:37:59 INFO 140466583250752] Epoch[131] Batch [5]#011Speed: 1813.05 samples/sec#011loss=-4.257189\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:37:59 INFO 140466583250752] Epoch[131] Batch[10] avg_epoch_loss=-4.302798\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:37:59 INFO 140466583250752] #quality_metric: host=algo-1, epoch=131, batch=10 train loss <loss>=-4.35752906799\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:37:59 INFO 140466583250752] Epoch[131] Batch [10]#011Speed: 1166.93 samples/sec#011loss=-4.357529\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:37:59 INFO 140466583250752] processed a total of 1305 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52882.87115097046, \"sum\": 52882.87115097046, \"min\": 52882.87115097046}}, \"EndTime\": 1593884279.942347, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593884227.058905}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:37:59 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=24.6771211296 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:37:59 INFO 140466583250752] #progress_metric: host=algo-1, completed 66 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:37:59 INFO 140466583250752] #quality_metric: host=algo-1, epoch=131, train loss <loss>=-4.30279809778\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:37:59 INFO 140466583250752] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:38:51 INFO 140466583250752] Epoch[132] Batch[0] avg_epoch_loss=-3.444062\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:38:51 INFO 140466583250752] #quality_metric: host=algo-1, epoch=132, batch=0 train loss <loss>=-3.44406247139\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:38:51 INFO 140466583250752] Epoch[132] Batch[5] avg_epoch_loss=-3.839381\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:38:51 INFO 140466583250752] #quality_metric: host=algo-1, epoch=132, batch=5 train loss <loss>=-3.83938141664\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:38:51 INFO 140466583250752] Epoch[132] Batch [5]#011Speed: 1818.98 samples/sec#011loss=-3.839381\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:38:52 INFO 140466583250752] processed a total of 1267 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52352.78916358948, \"sum\": 52352.78916358948, \"min\": 52352.78916358948}}, \"EndTime\": 1593884332.295795, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593884279.942425}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:38:52 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=24.2011213459 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:38:52 INFO 140466583250752] #progress_metric: host=algo-1, completed 66 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:38:52 INFO 140466583250752] #quality_metric: host=algo-1, epoch=132, train loss <loss>=-3.96670284271\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:38:52 INFO 140466583250752] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:39:44 INFO 140466583250752] Epoch[133] Batch[0] avg_epoch_loss=-3.923444\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:39:44 INFO 140466583250752] #quality_metric: host=algo-1, epoch=133, batch=0 train loss <loss>=-3.92344427109\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:39:44 INFO 140466583250752] Epoch[133] Batch[5] avg_epoch_loss=-4.140900\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:39:44 INFO 140466583250752] #quality_metric: host=algo-1, epoch=133, batch=5 train loss <loss>=-4.14090037346\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:39:44 INFO 140466583250752] Epoch[133] Batch [5]#011Speed: 1767.90 samples/sec#011loss=-4.140900\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:39:44 INFO 140466583250752] Epoch[133] Batch[10] avg_epoch_loss=-4.190762\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:39:44 INFO 140466583250752] #quality_metric: host=algo-1, epoch=133, batch=10 train loss <loss>=-4.25059499741\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:39:44 INFO 140466583250752] Epoch[133] Batch [10]#011Speed: 1201.05 samples/sec#011loss=-4.250595\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:39:44 INFO 140466583250752] processed a total of 1326 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52698.8799571991, \"sum\": 52698.8799571991, \"min\": 52698.8799571991}}, \"EndTime\": 1593884384.995379, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593884332.295908}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:39:44 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=25.1617648722 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:39:44 INFO 140466583250752] #progress_metric: host=algo-1, completed 67 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:39:44 INFO 140466583250752] #quality_metric: host=algo-1, epoch=133, train loss <loss>=-4.19076156616\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:39:44 INFO 140466583250752] loss did not improve\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[07/04/2020 17:40:37 INFO 140466583250752] Epoch[134] Batch[0] avg_epoch_loss=-4.319220\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:40:37 INFO 140466583250752] #quality_metric: host=algo-1, epoch=134, batch=0 train loss <loss>=-4.31921958923\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:40:37 INFO 140466583250752] Epoch[134] Batch[5] avg_epoch_loss=-4.347351\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:40:37 INFO 140466583250752] #quality_metric: host=algo-1, epoch=134, batch=5 train loss <loss>=-4.34735075633\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:40:37 INFO 140466583250752] Epoch[134] Batch [5]#011Speed: 1793.17 samples/sec#011loss=-4.347351\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:40:38 INFO 140466583250752] Epoch[134] Batch[10] avg_epoch_loss=-4.503284\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:40:38 INFO 140466583250752] #quality_metric: host=algo-1, epoch=134, batch=10 train loss <loss>=-4.69040384293\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:40:38 INFO 140466583250752] Epoch[134] Batch [10]#011Speed: 1101.63 samples/sec#011loss=-4.690404\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:40:38 INFO 140466583250752] processed a total of 1290 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 53069.61703300476, \"sum\": 53069.61703300476, \"min\": 53069.61703300476}}, \"EndTime\": 1593884438.065592, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593884384.995463}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:40:38 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=24.3076351403 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:40:38 INFO 140466583250752] #progress_metric: host=algo-1, completed 67 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:40:38 INFO 140466583250752] #quality_metric: host=algo-1, epoch=134, train loss <loss>=-4.50328397751\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:40:38 INFO 140466583250752] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:40:38 INFO 140466583250752] Saved checkpoint to \"/opt/ml/model/state_48f46181-7bdb-456f-a78f-bad6d09a2a42-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 7.662057876586914, \"sum\": 7.662057876586914, \"min\": 7.662057876586914}}, \"EndTime\": 1593884438.073902, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593884438.065676}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:41:29 INFO 140466583250752] Epoch[135] Batch[0] avg_epoch_loss=-3.655078\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:41:29 INFO 140466583250752] #quality_metric: host=algo-1, epoch=135, batch=0 train loss <loss>=-3.65507793427\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:41:30 INFO 140466583250752] Epoch[135] Batch[5] avg_epoch_loss=-4.111000\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:41:30 INFO 140466583250752] #quality_metric: host=algo-1, epoch=135, batch=5 train loss <loss>=-4.11100037893\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:41:30 INFO 140466583250752] Epoch[135] Batch [5]#011Speed: 1807.62 samples/sec#011loss=-4.111000\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:41:30 INFO 140466583250752] processed a total of 1262 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52414.17098045349, \"sum\": 52414.17098045349, \"min\": 52414.17098045349}}, \"EndTime\": 1593884490.488207, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593884438.07397}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:41:30 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=24.0773950175 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:41:30 INFO 140466583250752] #progress_metric: host=algo-1, completed 68 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:41:30 INFO 140466583250752] #quality_metric: host=algo-1, epoch=135, train loss <loss>=-4.19731035233\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:41:30 INFO 140466583250752] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:42:22 INFO 140466583250752] Epoch[136] Batch[0] avg_epoch_loss=-4.540296\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:42:22 INFO 140466583250752] #quality_metric: host=algo-1, epoch=136, batch=0 train loss <loss>=-4.54029560089\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:42:22 INFO 140466583250752] Epoch[136] Batch[5] avg_epoch_loss=-4.408052\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:42:22 INFO 140466583250752] #quality_metric: host=algo-1, epoch=136, batch=5 train loss <loss>=-4.40805164973\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:42:22 INFO 140466583250752] Epoch[136] Batch [5]#011Speed: 1681.53 samples/sec#011loss=-4.408052\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:42:22 INFO 140466583250752] processed a total of 1277 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52449.095010757446, \"sum\": 52449.095010757446, \"min\": 52449.095010757446}}, \"EndTime\": 1593884542.937939, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593884490.488301}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:42:22 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=24.3473675447 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:42:22 INFO 140466583250752] #progress_metric: host=algo-1, completed 68 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:42:22 INFO 140466583250752] #quality_metric: host=algo-1, epoch=136, train loss <loss>=-4.39094238281\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:42:22 INFO 140466583250752] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:43:14 INFO 140466583250752] Epoch[137] Batch[0] avg_epoch_loss=-4.510291\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:43:14 INFO 140466583250752] #quality_metric: host=algo-1, epoch=137, batch=0 train loss <loss>=-4.51029109955\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:43:15 INFO 140466583250752] Epoch[137] Batch[5] avg_epoch_loss=-4.443055\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:43:15 INFO 140466583250752] #quality_metric: host=algo-1, epoch=137, batch=5 train loss <loss>=-4.44305467606\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:43:15 INFO 140466583250752] Epoch[137] Batch [5]#011Speed: 1817.37 samples/sec#011loss=-4.443055\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:43:15 INFO 140466583250752] Epoch[137] Batch[10] avg_epoch_loss=-4.418454\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:43:15 INFO 140466583250752] #quality_metric: host=algo-1, epoch=137, batch=10 train loss <loss>=-4.38893241882\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:43:15 INFO 140466583250752] Epoch[137] Batch [10]#011Speed: 1103.01 samples/sec#011loss=-4.388932\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:43:15 INFO 140466583250752] processed a total of 1289 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52745.909214019775, \"sum\": 52745.909214019775, \"min\": 52745.909214019775}}, \"EndTime\": 1593884595.684486, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593884542.938012}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:43:15 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=24.4378572403 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:43:15 INFO 140466583250752] #progress_metric: host=algo-1, completed 69 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:43:15 INFO 140466583250752] #quality_metric: host=algo-1, epoch=137, train loss <loss>=-4.41845365004\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:43:15 INFO 140466583250752] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:44:07 INFO 140466583250752] Epoch[138] Batch[0] avg_epoch_loss=-4.082047\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:44:07 INFO 140466583250752] #quality_metric: host=algo-1, epoch=138, batch=0 train loss <loss>=-4.08204698563\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:44:07 INFO 140466583250752] Epoch[138] Batch[5] avg_epoch_loss=-4.174116\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:44:07 INFO 140466583250752] #quality_metric: host=algo-1, epoch=138, batch=5 train loss <loss>=-4.17411581675\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:44:07 INFO 140466583250752] Epoch[138] Batch [5]#011Speed: 1797.70 samples/sec#011loss=-4.174116\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:44:08 INFO 140466583250752] processed a total of 1239 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52491.36996269226, \"sum\": 52491.36996269226, \"min\": 52491.36996269226}}, \"EndTime\": 1593884648.176475, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593884595.684569}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:44:08 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=23.6038300853 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:44:08 INFO 140466583250752] #progress_metric: host=algo-1, completed 69 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:44:08 INFO 140466583250752] #quality_metric: host=algo-1, epoch=138, train loss <loss>=-4.22756810188\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:44:08 INFO 140466583250752] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:45:00 INFO 140466583250752] Epoch[139] Batch[0] avg_epoch_loss=-4.036995\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:45:00 INFO 140466583250752] #quality_metric: host=algo-1, epoch=139, batch=0 train loss <loss>=-4.03699541092\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:45:00 INFO 140466583250752] Epoch[139] Batch[5] avg_epoch_loss=-4.172830\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:45:00 INFO 140466583250752] #quality_metric: host=algo-1, epoch=139, batch=5 train loss <loss>=-4.17282998562\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:45:00 INFO 140466583250752] Epoch[139] Batch [5]#011Speed: 1793.69 samples/sec#011loss=-4.172830\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:45:00 INFO 140466583250752] processed a total of 1277 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52772.62306213379, \"sum\": 52772.62306213379, \"min\": 52772.62306213379}}, \"EndTime\": 1593884700.949744, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593884648.176545}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:45:00 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=24.1980938315 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:45:00 INFO 140466583250752] #progress_metric: host=algo-1, completed 70 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:45:00 INFO 140466583250752] #quality_metric: host=algo-1, epoch=139, train loss <loss>=-4.20235164165\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:45:00 INFO 140466583250752] loss did not improve\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[07/04/2020 17:45:52 INFO 140466583250752] Epoch[140] Batch[0] avg_epoch_loss=-4.389905\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:45:52 INFO 140466583250752] #quality_metric: host=algo-1, epoch=140, batch=0 train loss <loss>=-4.38990497589\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:45:52 INFO 140466583250752] Epoch[140] Batch[5] avg_epoch_loss=-4.307956\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:45:52 INFO 140466583250752] #quality_metric: host=algo-1, epoch=140, batch=5 train loss <loss>=-4.30795558294\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:45:52 INFO 140466583250752] Epoch[140] Batch [5]#011Speed: 1818.98 samples/sec#011loss=-4.307956\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:45:53 INFO 140466583250752] Epoch[140] Batch[10] avg_epoch_loss=-4.443791\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:45:53 INFO 140466583250752] #quality_metric: host=algo-1, epoch=140, batch=10 train loss <loss>=-4.60679368973\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:45:53 INFO 140466583250752] Epoch[140] Batch [10]#011Speed: 1123.25 samples/sec#011loss=-4.606794\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:45:53 INFO 140466583250752] processed a total of 1294 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52574.965953826904, \"sum\": 52574.965953826904, \"min\": 52574.965953826904}}, \"EndTime\": 1593884753.525403, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593884700.949826}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:45:53 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=24.6124138368 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:45:53 INFO 140466583250752] #progress_metric: host=algo-1, completed 70 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:45:53 INFO 140466583250752] #quality_metric: host=algo-1, epoch=140, train loss <loss>=-4.44379108602\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:45:53 INFO 140466583250752] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:46:45 INFO 140466583250752] Epoch[141] Batch[0] avg_epoch_loss=-4.077628\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:46:45 INFO 140466583250752] #quality_metric: host=algo-1, epoch=141, batch=0 train loss <loss>=-4.07762813568\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:46:45 INFO 140466583250752] Epoch[141] Batch[5] avg_epoch_loss=-4.251918\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:46:45 INFO 140466583250752] #quality_metric: host=algo-1, epoch=141, batch=5 train loss <loss>=-4.251917998\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:46:45 INFO 140466583250752] Epoch[141] Batch [5]#011Speed: 1811.22 samples/sec#011loss=-4.251918\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:46:45 INFO 140466583250752] processed a total of 1274 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52439.77904319763, \"sum\": 52439.77904319763, \"min\": 52439.77904319763}}, \"EndTime\": 1593884805.965836, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593884753.525493}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:46:45 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=24.2944692596 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:46:45 INFO 140466583250752] #progress_metric: host=algo-1, completed 71 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:46:45 INFO 140466583250752] #quality_metric: host=algo-1, epoch=141, train loss <loss>=-4.25465245247\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:46:45 INFO 140466583250752] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:47:37 INFO 140466583250752] Epoch[142] Batch[0] avg_epoch_loss=-4.476560\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:47:37 INFO 140466583250752] #quality_metric: host=algo-1, epoch=142, batch=0 train loss <loss>=-4.47655963898\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:47:38 INFO 140466583250752] Epoch[142] Batch[5] avg_epoch_loss=-4.494180\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:47:38 INFO 140466583250752] #quality_metric: host=algo-1, epoch=142, batch=5 train loss <loss>=-4.49417996407\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:47:38 INFO 140466583250752] Epoch[142] Batch [5]#011Speed: 1731.17 samples/sec#011loss=-4.494180\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:47:38 INFO 140466583250752] processed a total of 1279 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52664.07799720764, \"sum\": 52664.07799720764, \"min\": 52664.07799720764}}, \"EndTime\": 1593884858.630585, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593884805.965931}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:47:38 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=24.2859499574 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:47:38 INFO 140466583250752] #progress_metric: host=algo-1, completed 71 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:47:38 INFO 140466583250752] #quality_metric: host=algo-1, epoch=142, train loss <loss>=-4.48486623764\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:47:38 INFO 140466583250752] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:48:30 INFO 140466583250752] Epoch[143] Batch[0] avg_epoch_loss=-4.516252\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:48:30 INFO 140466583250752] #quality_metric: host=algo-1, epoch=143, batch=0 train loss <loss>=-4.51625204086\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:48:31 INFO 140466583250752] Epoch[143] Batch[5] avg_epoch_loss=-4.503846\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:48:31 INFO 140466583250752] #quality_metric: host=algo-1, epoch=143, batch=5 train loss <loss>=-4.50384561221\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:48:31 INFO 140466583250752] Epoch[143] Batch [5]#011Speed: 1817.36 samples/sec#011loss=-4.503846\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:48:31 INFO 140466583250752] processed a total of 1266 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52781.01706504822, \"sum\": 52781.01706504822, \"min\": 52781.01706504822}}, \"EndTime\": 1593884911.412247, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593884858.63066}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:48:31 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=23.9858340722 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:48:31 INFO 140466583250752] #progress_metric: host=algo-1, completed 72 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:48:31 INFO 140466583250752] #quality_metric: host=algo-1, epoch=143, train loss <loss>=-4.51758112907\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:48:31 INFO 140466583250752] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:48:31 INFO 140466583250752] Saved checkpoint to \"/opt/ml/model/state_d21addcb-50e4-45b5-b70c-e27cddb84e28-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 8.037090301513672, \"sum\": 8.037090301513672, \"min\": 8.037090301513672}}, \"EndTime\": 1593884911.420954, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593884911.412338}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:49:22 INFO 140466583250752] Epoch[144] Batch[0] avg_epoch_loss=-4.629403\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:49:22 INFO 140466583250752] #quality_metric: host=algo-1, epoch=144, batch=0 train loss <loss>=-4.62940311432\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:49:23 INFO 140466583250752] Epoch[144] Batch[5] avg_epoch_loss=-4.549345\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:49:23 INFO 140466583250752] #quality_metric: host=algo-1, epoch=144, batch=5 train loss <loss>=-4.54934501648\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:49:23 INFO 140466583250752] Epoch[144] Batch [5]#011Speed: 1813.65 samples/sec#011loss=-4.549345\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:49:23 INFO 140466583250752] Epoch[144] Batch[10] avg_epoch_loss=-4.515980\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:49:23 INFO 140466583250752] #quality_metric: host=algo-1, epoch=144, batch=10 train loss <loss>=-4.47594146729\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:49:23 INFO 140466583250752] Epoch[144] Batch [10]#011Speed: 1180.72 samples/sec#011loss=-4.475941\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:49:23 INFO 140466583250752] processed a total of 1312 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52319.653034210205, \"sum\": 52319.653034210205, \"min\": 52319.653034210205}}, \"EndTime\": 1593884963.740741, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593884911.421021}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:49:23 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=25.0765632468 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:49:23 INFO 140466583250752] #progress_metric: host=algo-1, completed 72 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:49:23 INFO 140466583250752] #quality_metric: host=algo-1, epoch=144, train loss <loss>=-4.51597976685\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:49:23 INFO 140466583250752] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:50:15 INFO 140466583250752] Epoch[145] Batch[0] avg_epoch_loss=-4.161691\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:50:15 INFO 140466583250752] #quality_metric: host=algo-1, epoch=145, batch=0 train loss <loss>=-4.16169071198\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:50:16 INFO 140466583250752] Epoch[145] Batch[5] avg_epoch_loss=-4.477775\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:50:16 INFO 140466583250752] #quality_metric: host=algo-1, epoch=145, batch=5 train loss <loss>=-4.47777501742\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:50:16 INFO 140466583250752] Epoch[145] Batch [5]#011Speed: 1773.55 samples/sec#011loss=-4.477775\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:50:16 INFO 140466583250752] Epoch[145] Batch[10] avg_epoch_loss=-4.436921\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:50:16 INFO 140466583250752] #quality_metric: host=algo-1, epoch=145, batch=10 train loss <loss>=-4.38789567947\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:50:16 INFO 140466583250752] Epoch[145] Batch [10]#011Speed: 1100.08 samples/sec#011loss=-4.387896\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:50:16 INFO 140466583250752] processed a total of 1291 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 53128.36313247681, \"sum\": 53128.36313247681, \"min\": 53128.36313247681}}, \"EndTime\": 1593885016.869734, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593884963.740818}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:50:16 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=24.2995691306 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:50:16 INFO 140466583250752] #progress_metric: host=algo-1, completed 73 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:50:16 INFO 140466583250752] #quality_metric: host=algo-1, epoch=145, train loss <loss>=-4.4369207729\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:50:16 INFO 140466583250752] loss did not improve\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[07/04/2020 17:51:08 INFO 140466583250752] Epoch[146] Batch[0] avg_epoch_loss=-3.441674\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:51:08 INFO 140466583250752] #quality_metric: host=algo-1, epoch=146, batch=0 train loss <loss>=-3.44167375565\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:51:09 INFO 140466583250752] Epoch[146] Batch[5] avg_epoch_loss=-4.038611\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:51:09 INFO 140466583250752] #quality_metric: host=algo-1, epoch=146, batch=5 train loss <loss>=-4.03861061732\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:51:09 INFO 140466583250752] Epoch[146] Batch [5]#011Speed: 1774.26 samples/sec#011loss=-4.038611\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:51:09 INFO 140466583250752] processed a total of 1276 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52746.55103683472, \"sum\": 52746.55103683472, \"min\": 52746.55103683472}}, \"EndTime\": 1593885069.616885, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593885016.869846}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:51:09 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=24.1911001537 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:51:09 INFO 140466583250752] #progress_metric: host=algo-1, completed 73 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:51:09 INFO 140466583250752] #quality_metric: host=algo-1, epoch=146, train loss <loss>=-4.10545525551\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:51:09 INFO 140466583250752] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:52:01 INFO 140466583250752] Epoch[147] Batch[0] avg_epoch_loss=-3.936881\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:52:01 INFO 140466583250752] #quality_metric: host=algo-1, epoch=147, batch=0 train loss <loss>=-3.93688082695\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:52:01 INFO 140466583250752] Epoch[147] Batch[5] avg_epoch_loss=-4.252661\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:52:01 INFO 140466583250752] #quality_metric: host=algo-1, epoch=147, batch=5 train loss <loss>=-4.25266118844\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:52:01 INFO 140466583250752] Epoch[147] Batch [5]#011Speed: 1819.94 samples/sec#011loss=-4.252661\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:52:02 INFO 140466583250752] Epoch[147] Batch[10] avg_epoch_loss=-4.297319\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:52:02 INFO 140466583250752] #quality_metric: host=algo-1, epoch=147, batch=10 train loss <loss>=-4.35090942383\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:52:02 INFO 140466583250752] Epoch[147] Batch [10]#011Speed: 1171.74 samples/sec#011loss=-4.350909\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:52:02 INFO 140466583250752] processed a total of 1331 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52712.99386024475, \"sum\": 52712.99386024475, \"min\": 52712.99386024475}}, \"EndTime\": 1593885122.33053, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593885069.616966}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:52:02 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=25.2498826997 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:52:02 INFO 140466583250752] #progress_metric: host=algo-1, completed 74 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:52:02 INFO 140466583250752] #quality_metric: host=algo-1, epoch=147, train loss <loss>=-4.29731947725\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:52:02 INFO 140466583250752] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:52:53 INFO 140466583250752] Epoch[148] Batch[0] avg_epoch_loss=-4.263632\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:52:53 INFO 140466583250752] #quality_metric: host=algo-1, epoch=148, batch=0 train loss <loss>=-4.26363182068\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:52:54 INFO 140466583250752] Epoch[148] Batch[5] avg_epoch_loss=-4.368259\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:52:54 INFO 140466583250752] #quality_metric: host=algo-1, epoch=148, batch=5 train loss <loss>=-4.36825919151\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:52:54 INFO 140466583250752] Epoch[148] Batch [5]#011Speed: 1793.52 samples/sec#011loss=-4.368259\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:52:54 INFO 140466583250752] processed a total of 1271 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52170.244216918945, \"sum\": 52170.244216918945, \"min\": 52170.244216918945}}, \"EndTime\": 1593885174.501381, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593885122.330613}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:52:54 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=24.3624938252 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:52:54 INFO 140466583250752] #progress_metric: host=algo-1, completed 74 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:52:54 INFO 140466583250752] #quality_metric: host=algo-1, epoch=148, train loss <loss>=-4.39976587296\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:52:54 INFO 140466583250752] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:53:45 INFO 140466583250752] Epoch[149] Batch[0] avg_epoch_loss=-4.452449\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:53:45 INFO 140466583250752] #quality_metric: host=algo-1, epoch=149, batch=0 train loss <loss>=-4.45244884491\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:53:45 INFO 140466583250752] Epoch[149] Batch[5] avg_epoch_loss=-4.487392\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:53:45 INFO 140466583250752] #quality_metric: host=algo-1, epoch=149, batch=5 train loss <loss>=-4.48739155134\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:53:45 INFO 140466583250752] Epoch[149] Batch [5]#011Speed: 1803.66 samples/sec#011loss=-4.487392\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:53:46 INFO 140466583250752] Epoch[149] Batch[10] avg_epoch_loss=-4.457128\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:53:46 INFO 140466583250752] #quality_metric: host=algo-1, epoch=149, batch=10 train loss <loss>=-4.42081203461\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:53:46 INFO 140466583250752] Epoch[149] Batch [10]#011Speed: 1098.60 samples/sec#011loss=-4.420812\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:53:46 INFO 140466583250752] processed a total of 1291 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52056.83088302612, \"sum\": 52056.83088302612, \"min\": 52056.83088302612}}, \"EndTime\": 1593885226.558803, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593885174.501451}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:53:46 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=24.799757299 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:53:46 INFO 140466583250752] #progress_metric: host=algo-1, completed 75 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:53:46 INFO 140466583250752] #quality_metric: host=algo-1, epoch=149, train loss <loss>=-4.45712813464\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:53:46 INFO 140466583250752] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:54:38 INFO 140466583250752] Epoch[150] Batch[0] avg_epoch_loss=-4.503994\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:54:38 INFO 140466583250752] #quality_metric: host=algo-1, epoch=150, batch=0 train loss <loss>=-4.50399398804\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:54:38 INFO 140466583250752] Epoch[150] Batch[5] avg_epoch_loss=-4.356845\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:54:38 INFO 140466583250752] #quality_metric: host=algo-1, epoch=150, batch=5 train loss <loss>=-4.3568452994\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:54:38 INFO 140466583250752] Epoch[150] Batch [5]#011Speed: 1796.53 samples/sec#011loss=-4.356845\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:54:39 INFO 140466583250752] Epoch[150] Batch[10] avg_epoch_loss=-4.336510\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:54:39 INFO 140466583250752] #quality_metric: host=algo-1, epoch=150, batch=10 train loss <loss>=-4.31210699081\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:54:39 INFO 140466583250752] Epoch[150] Batch [10]#011Speed: 1082.63 samples/sec#011loss=-4.312107\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:54:39 INFO 140466583250752] processed a total of 1281 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52567.333936691284, \"sum\": 52567.333936691284, \"min\": 52567.333936691284}}, \"EndTime\": 1593885279.126769, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593885226.558892}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:54:39 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=24.3686878113 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:54:39 INFO 140466583250752] #progress_metric: host=algo-1, completed 75 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:54:39 INFO 140466583250752] #quality_metric: host=algo-1, epoch=150, train loss <loss>=-4.33650970459\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:54:39 INFO 140466583250752] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:55:30 INFO 140466583250752] Epoch[151] Batch[0] avg_epoch_loss=-3.692448\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:55:30 INFO 140466583250752] #quality_metric: host=algo-1, epoch=151, batch=0 train loss <loss>=-3.69244790077\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:55:30 INFO 140466583250752] Epoch[151] Batch[5] avg_epoch_loss=-4.178618\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:55:30 INFO 140466583250752] #quality_metric: host=algo-1, epoch=151, batch=5 train loss <loss>=-4.17861791452\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:55:30 INFO 140466583250752] Epoch[151] Batch [5]#011Speed: 1789.49 samples/sec#011loss=-4.178618\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:55:31 INFO 140466583250752] Epoch[151] Batch[10] avg_epoch_loss=-4.266290\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:55:31 INFO 140466583250752] #quality_metric: host=algo-1, epoch=151, batch=10 train loss <loss>=-4.37149753571\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:55:31 INFO 140466583250752] Epoch[151] Batch [10]#011Speed: 1111.26 samples/sec#011loss=-4.371498\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:55:31 INFO 140466583250752] processed a total of 1299 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52246.62804603577, \"sum\": 52246.62804603577, \"min\": 52246.62804603577}}, \"EndTime\": 1593885331.374019, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593885279.126851}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:55:31 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=24.8627952183 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:55:31 INFO 140466583250752] #progress_metric: host=algo-1, completed 76 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:55:31 INFO 140466583250752] #quality_metric: host=algo-1, epoch=151, train loss <loss>=-4.2662904696\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:55:31 INFO 140466583250752] loss did not improve\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[07/04/2020 17:56:23 INFO 140466583250752] Epoch[152] Batch[0] avg_epoch_loss=-4.352229\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:56:23 INFO 140466583250752] #quality_metric: host=algo-1, epoch=152, batch=0 train loss <loss>=-4.35222911835\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:56:23 INFO 140466583250752] Epoch[152] Batch[5] avg_epoch_loss=-4.357634\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:56:23 INFO 140466583250752] #quality_metric: host=algo-1, epoch=152, batch=5 train loss <loss>=-4.35763374964\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:56:23 INFO 140466583250752] Epoch[152] Batch [5]#011Speed: 1794.28 samples/sec#011loss=-4.357634\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:56:24 INFO 140466583250752] Epoch[152] Batch[10] avg_epoch_loss=-4.392376\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:56:24 INFO 140466583250752] #quality_metric: host=algo-1, epoch=152, batch=10 train loss <loss>=-4.43406667709\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:56:24 INFO 140466583250752] Epoch[152] Batch [10]#011Speed: 1112.73 samples/sec#011loss=-4.434067\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:56:24 INFO 140466583250752] processed a total of 1307 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52841.667890548706, \"sum\": 52841.667890548706, \"min\": 52841.667890548706}}, \"EndTime\": 1593885384.216246, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593885331.37409}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:56:24 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=24.7342104382 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:56:24 INFO 140466583250752] #progress_metric: host=algo-1, completed 76 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:56:24 INFO 140466583250752] #quality_metric: host=algo-1, epoch=152, train loss <loss>=-4.39237598939\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:56:24 INFO 140466583250752] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:57:15 INFO 140466583250752] Epoch[153] Batch[0] avg_epoch_loss=-4.319960\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:57:15 INFO 140466583250752] #quality_metric: host=algo-1, epoch=153, batch=0 train loss <loss>=-4.3199596405\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:57:15 INFO 140466583250752] Epoch[153] Batch[5] avg_epoch_loss=-4.428166\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:57:15 INFO 140466583250752] #quality_metric: host=algo-1, epoch=153, batch=5 train loss <loss>=-4.42816575368\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:57:15 INFO 140466583250752] Epoch[153] Batch [5]#011Speed: 1816.78 samples/sec#011loss=-4.428166\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:57:16 INFO 140466583250752] Epoch[153] Batch[10] avg_epoch_loss=-4.528587\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:57:16 INFO 140466583250752] #quality_metric: host=algo-1, epoch=153, batch=10 train loss <loss>=-4.64909324646\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:57:16 INFO 140466583250752] Epoch[153] Batch [10]#011Speed: 1224.69 samples/sec#011loss=-4.649093\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:57:16 INFO 140466583250752] processed a total of 1332 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52188.652992248535, \"sum\": 52188.652992248535, \"min\": 52188.652992248535}}, \"EndTime\": 1593885436.405534, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593885384.216329}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:57:16 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=25.5227203857 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:57:16 INFO 140466583250752] #progress_metric: host=algo-1, completed 77 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:57:16 INFO 140466583250752] #quality_metric: host=algo-1, epoch=153, train loss <loss>=-4.52858734131\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:57:16 INFO 140466583250752] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:57:16 INFO 140466583250752] Saved checkpoint to \"/opt/ml/model/state_5c56c6fa-c67d-4771-ad0a-6f81fa245c98-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 8.249998092651367, \"sum\": 8.249998092651367, \"min\": 8.249998092651367}}, \"EndTime\": 1593885436.4144, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593885436.405629}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:58:08 INFO 140466583250752] Epoch[154] Batch[0] avg_epoch_loss=-4.561828\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:58:08 INFO 140466583250752] #quality_metric: host=algo-1, epoch=154, batch=0 train loss <loss>=-4.56182765961\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:58:08 INFO 140466583250752] Epoch[154] Batch[5] avg_epoch_loss=-4.414473\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:58:08 INFO 140466583250752] #quality_metric: host=algo-1, epoch=154, batch=5 train loss <loss>=-4.41447321574\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:58:08 INFO 140466583250752] Epoch[154] Batch [5]#011Speed: 1786.20 samples/sec#011loss=-4.414473\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:58:09 INFO 140466583250752] Epoch[154] Batch[10] avg_epoch_loss=-4.439908\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:58:09 INFO 140466583250752] #quality_metric: host=algo-1, epoch=154, batch=10 train loss <loss>=-4.47042922974\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:58:09 INFO 140466583250752] Epoch[154] Batch [10]#011Speed: 1101.33 samples/sec#011loss=-4.470429\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:58:09 INFO 140466583250752] processed a total of 1292 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52745.124101638794, \"sum\": 52745.124101638794, \"min\": 52745.124101638794}}, \"EndTime\": 1593885489.159656, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593885436.414471}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:58:09 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=24.4950818843 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:58:09 INFO 140466583250752] #progress_metric: host=algo-1, completed 77 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:58:09 INFO 140466583250752] #quality_metric: host=algo-1, epoch=154, train loss <loss>=-4.43990776756\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:58:09 INFO 140466583250752] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:59:00 INFO 140466583250752] Epoch[155] Batch[0] avg_epoch_loss=-4.255273\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:59:00 INFO 140466583250752] #quality_metric: host=algo-1, epoch=155, batch=0 train loss <loss>=-4.2552728653\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:59:00 INFO 140466583250752] Epoch[155] Batch[5] avg_epoch_loss=-4.318684\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:59:00 INFO 140466583250752] #quality_metric: host=algo-1, epoch=155, batch=5 train loss <loss>=-4.31868402163\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:59:00 INFO 140466583250752] Epoch[155] Batch [5]#011Speed: 1807.81 samples/sec#011loss=-4.318684\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:59:01 INFO 140466583250752] Epoch[155] Batch[10] avg_epoch_loss=-4.296417\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:59:01 INFO 140466583250752] #quality_metric: host=algo-1, epoch=155, batch=10 train loss <loss>=-4.26969566345\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:59:01 INFO 140466583250752] Epoch[155] Batch [10]#011Speed: 1231.48 samples/sec#011loss=-4.269696\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:59:01 INFO 140466583250752] processed a total of 1359 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52125.1699924469, \"sum\": 52125.1699924469, \"min\": 52125.1699924469}}, \"EndTime\": 1593885541.285535, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593885489.159776}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:59:01 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=26.0717717969 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:59:01 INFO 140466583250752] #progress_metric: host=algo-1, completed 78 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:59:01 INFO 140466583250752] #quality_metric: host=algo-1, epoch=155, train loss <loss>=-4.2964165861\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:59:01 INFO 140466583250752] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:59:52 INFO 140466583250752] Epoch[156] Batch[0] avg_epoch_loss=-3.579370\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:59:52 INFO 140466583250752] #quality_metric: host=algo-1, epoch=156, batch=0 train loss <loss>=-3.57936954498\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:59:53 INFO 140466583250752] Epoch[156] Batch[5] avg_epoch_loss=-3.898864\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:59:53 INFO 140466583250752] #quality_metric: host=algo-1, epoch=156, batch=5 train loss <loss>=-3.89886422952\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:59:53 INFO 140466583250752] Epoch[156] Batch [5]#011Speed: 1802.83 samples/sec#011loss=-3.898864\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:59:53 INFO 140466583250752] processed a total of 1247 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52469.072103500366, \"sum\": 52469.072103500366, \"min\": 52469.072103500366}}, \"EndTime\": 1593885593.755267, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593885541.285663}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:59:53 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=23.7663288773 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:59:53 INFO 140466583250752] #progress_metric: host=algo-1, completed 78 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:59:53 INFO 140466583250752] #quality_metric: host=algo-1, epoch=156, train loss <loss>=-3.93798086643\u001b[0m\n",
      "\u001b[34m[07/04/2020 17:59:53 INFO 140466583250752] loss did not improve\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[07/04/2020 18:00:45 INFO 140466583250752] Epoch[157] Batch[0] avg_epoch_loss=-4.238723\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:00:45 INFO 140466583250752] #quality_metric: host=algo-1, epoch=157, batch=0 train loss <loss>=-4.23872327805\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:00:46 INFO 140466583250752] Epoch[157] Batch[5] avg_epoch_loss=-4.251945\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:00:46 INFO 140466583250752] #quality_metric: host=algo-1, epoch=157, batch=5 train loss <loss>=-4.25194541613\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:00:46 INFO 140466583250752] Epoch[157] Batch [5]#011Speed: 1708.14 samples/sec#011loss=-4.251945\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:00:46 INFO 140466583250752] Epoch[157] Batch[10] avg_epoch_loss=-4.356063\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:00:46 INFO 140466583250752] #quality_metric: host=algo-1, epoch=157, batch=10 train loss <loss>=-4.48100442886\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:00:46 INFO 140466583250752] Epoch[157] Batch [10]#011Speed: 1104.70 samples/sec#011loss=-4.481004\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:00:46 INFO 140466583250752] processed a total of 1307 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 53063.82894515991, \"sum\": 53063.82894515991, \"min\": 53063.82894515991}}, \"EndTime\": 1593885646.819752, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593885593.75534}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:00:46 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=24.6306282264 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:00:46 INFO 140466583250752] #progress_metric: host=algo-1, completed 79 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:00:46 INFO 140466583250752] #quality_metric: host=algo-1, epoch=157, train loss <loss>=-4.35606314919\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:00:46 INFO 140466583250752] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:01:38 INFO 140466583250752] Epoch[158] Batch[0] avg_epoch_loss=-4.372535\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:01:38 INFO 140466583250752] #quality_metric: host=algo-1, epoch=158, batch=0 train loss <loss>=-4.37253475189\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:01:39 INFO 140466583250752] Epoch[158] Batch[5] avg_epoch_loss=-4.355464\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:01:39 INFO 140466583250752] #quality_metric: host=algo-1, epoch=158, batch=5 train loss <loss>=-4.35546414057\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:01:39 INFO 140466583250752] Epoch[158] Batch [5]#011Speed: 1813.15 samples/sec#011loss=-4.355464\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:01:39 INFO 140466583250752] Epoch[158] Batch[10] avg_epoch_loss=-4.478914\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:01:39 INFO 140466583250752] #quality_metric: host=algo-1, epoch=158, batch=10 train loss <loss>=-4.62705430984\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:01:39 INFO 140466583250752] Epoch[158] Batch [10]#011Speed: 1178.95 samples/sec#011loss=-4.627054\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:01:39 INFO 140466583250752] processed a total of 1308 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52860.156774520874, \"sum\": 52860.156774520874, \"min\": 52860.156774520874}}, \"EndTime\": 1593885699.68063, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593885646.819846}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:01:39 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=24.7444730695 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:01:39 INFO 140466583250752] #progress_metric: host=algo-1, completed 79 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:01:39 INFO 140466583250752] #quality_metric: host=algo-1, epoch=158, train loss <loss>=-4.47891421752\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:01:39 INFO 140466583250752] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:02:31 INFO 140466583250752] Epoch[159] Batch[0] avg_epoch_loss=-4.564879\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:02:31 INFO 140466583250752] #quality_metric: host=algo-1, epoch=159, batch=0 train loss <loss>=-4.56487941742\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:02:31 INFO 140466583250752] Epoch[159] Batch[5] avg_epoch_loss=-4.458272\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:02:31 INFO 140466583250752] #quality_metric: host=algo-1, epoch=159, batch=5 train loss <loss>=-4.45827229818\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:02:31 INFO 140466583250752] Epoch[159] Batch [5]#011Speed: 1788.43 samples/sec#011loss=-4.458272\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:02:32 INFO 140466583250752] processed a total of 1266 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52537.86396980286, \"sum\": 52537.86396980286, \"min\": 52537.86396980286}}, \"EndTime\": 1593885752.21913, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593885699.680718}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:02:32 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=24.0968492795 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:02:32 INFO 140466583250752] #progress_metric: host=algo-1, completed 80 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:02:32 INFO 140466583250752] #quality_metric: host=algo-1, epoch=159, train loss <loss>=-4.42802820206\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:02:32 INFO 140466583250752] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:03:24 INFO 140466583250752] Epoch[160] Batch[0] avg_epoch_loss=-4.481945\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:03:24 INFO 140466583250752] #quality_metric: host=algo-1, epoch=160, batch=0 train loss <loss>=-4.481944561\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:03:24 INFO 140466583250752] Epoch[160] Batch[5] avg_epoch_loss=-4.421628\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:03:24 INFO 140466583250752] #quality_metric: host=algo-1, epoch=160, batch=5 train loss <loss>=-4.42162799835\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:03:24 INFO 140466583250752] Epoch[160] Batch [5]#011Speed: 1809.56 samples/sec#011loss=-4.421628\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:03:24 INFO 140466583250752] processed a total of 1273 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52710.54196357727, \"sum\": 52710.54196357727, \"min\": 52710.54196357727}}, \"EndTime\": 1593885804.930308, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593885752.219212}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:03:24 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=24.1507130341 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:03:24 INFO 140466583250752] #progress_metric: host=algo-1, completed 80 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:03:24 INFO 140466583250752] #quality_metric: host=algo-1, epoch=160, train loss <loss>=-4.41097826958\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:03:24 INFO 140466583250752] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:04:16 INFO 140466583250752] Epoch[161] Batch[0] avg_epoch_loss=-4.552501\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:04:16 INFO 140466583250752] #quality_metric: host=algo-1, epoch=161, batch=0 train loss <loss>=-4.55250120163\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:04:16 INFO 140466583250752] Epoch[161] Batch[5] avg_epoch_loss=-4.426780\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:04:16 INFO 140466583250752] #quality_metric: host=algo-1, epoch=161, batch=5 train loss <loss>=-4.42678014437\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:04:16 INFO 140466583250752] Epoch[161] Batch [5]#011Speed: 1819.88 samples/sec#011loss=-4.426780\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:04:17 INFO 140466583250752] Epoch[161] Batch[10] avg_epoch_loss=-4.497915\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:04:17 INFO 140466583250752] #quality_metric: host=algo-1, epoch=161, batch=10 train loss <loss>=-4.58327655792\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:04:17 INFO 140466583250752] Epoch[161] Batch [10]#011Speed: 1110.17 samples/sec#011loss=-4.583277\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:04:17 INFO 140466583250752] processed a total of 1300 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52328.03297042847, \"sum\": 52328.03297042847, \"min\": 52328.03297042847}}, \"EndTime\": 1593885857.258968, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593885804.930384}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:04:17 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=24.843224902 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:04:17 INFO 140466583250752] #progress_metric: host=algo-1, completed 81 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:04:17 INFO 140466583250752] #quality_metric: host=algo-1, epoch=161, train loss <loss>=-4.4979148778\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:04:17 INFO 140466583250752] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:05:09 INFO 140466583250752] Epoch[162] Batch[0] avg_epoch_loss=-4.327487\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:05:09 INFO 140466583250752] #quality_metric: host=algo-1, epoch=162, batch=0 train loss <loss>=-4.32748746872\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:05:09 INFO 140466583250752] Epoch[162] Batch[5] avg_epoch_loss=-4.462153\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:05:09 INFO 140466583250752] #quality_metric: host=algo-1, epoch=162, batch=5 train loss <loss>=-4.46215327581\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:05:09 INFO 140466583250752] Epoch[162] Batch [5]#011Speed: 1819.09 samples/sec#011loss=-4.462153\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:05:09 INFO 140466583250752] Epoch[162] Batch[10] avg_epoch_loss=-4.571313\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:05:09 INFO 140466583250752] #quality_metric: host=algo-1, epoch=162, batch=10 train loss <loss>=-4.70230512619\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:05:09 INFO 140466583250752] Epoch[162] Batch [10]#011Speed: 1114.95 samples/sec#011loss=-4.702305\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:05:09 INFO 140466583250752] processed a total of 1300 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52701.21788978577, \"sum\": 52701.21788978577, \"min\": 52701.21788978577}}, \"EndTime\": 1593885909.960741, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593885857.259042}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:05:09 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=24.667305386 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:05:09 INFO 140466583250752] #progress_metric: host=algo-1, completed 81 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:05:09 INFO 140466583250752] #quality_metric: host=algo-1, epoch=162, train loss <loss>=-4.5713132078\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:05:09 INFO 140466583250752] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:05:09 INFO 140466583250752] Saved checkpoint to \"/opt/ml/model/state_d60556a3-d4e8-4a09-90a1-dac428a08f29-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 8.173942565917969, \"sum\": 8.173942565917969, \"min\": 8.173942565917969}}, \"EndTime\": 1593885909.969505, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593885909.960816}\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[07/04/2020 18:06:01 INFO 140466583250752] Epoch[163] Batch[0] avg_epoch_loss=-4.555402\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:06:01 INFO 140466583250752] #quality_metric: host=algo-1, epoch=163, batch=0 train loss <loss>=-4.55540180206\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:06:01 INFO 140466583250752] Epoch[163] Batch[5] avg_epoch_loss=-4.567573\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:06:01 INFO 140466583250752] #quality_metric: host=algo-1, epoch=163, batch=5 train loss <loss>=-4.56757259369\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:06:01 INFO 140466583250752] Epoch[163] Batch [5]#011Speed: 1776.56 samples/sec#011loss=-4.567573\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:06:02 INFO 140466583250752] processed a total of 1254 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52381.30497932434, \"sum\": 52381.30497932434, \"min\": 52381.30497932434}}, \"EndTime\": 1593885962.35094, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593885909.969569}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:06:02 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=23.9397862211 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:06:02 INFO 140466583250752] #progress_metric: host=algo-1, completed 82 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:06:02 INFO 140466583250752] #quality_metric: host=algo-1, epoch=163, train loss <loss>=-4.56842169762\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:06:02 INFO 140466583250752] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:06:53 INFO 140466583250752] Epoch[164] Batch[0] avg_epoch_loss=-4.526858\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:06:53 INFO 140466583250752] #quality_metric: host=algo-1, epoch=164, batch=0 train loss <loss>=-4.52685832977\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:06:54 INFO 140466583250752] Epoch[164] Batch[5] avg_epoch_loss=-4.497384\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:06:54 INFO 140466583250752] #quality_metric: host=algo-1, epoch=164, batch=5 train loss <loss>=-4.49738383293\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:06:54 INFO 140466583250752] Epoch[164] Batch [5]#011Speed: 1802.44 samples/sec#011loss=-4.497384\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:06:54 INFO 140466583250752] Epoch[164] Batch[10] avg_epoch_loss=-4.456915\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:06:54 INFO 140466583250752] #quality_metric: host=algo-1, epoch=164, batch=10 train loss <loss>=-4.4083530426\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:06:54 INFO 140466583250752] Epoch[164] Batch [10]#011Speed: 1107.52 samples/sec#011loss=-4.408353\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:06:54 INFO 140466583250752] processed a total of 1298 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52565.49906730652, \"sum\": 52565.49906730652, \"min\": 52565.49906730652}}, \"EndTime\": 1593886014.917065, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593885962.351014}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:06:54 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=24.692946167 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:06:54 INFO 140466583250752] #progress_metric: host=algo-1, completed 82 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:06:54 INFO 140466583250752] #quality_metric: host=algo-1, epoch=164, train loss <loss>=-4.45691529187\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:06:54 INFO 140466583250752] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:07:46 INFO 140466583250752] Epoch[165] Batch[0] avg_epoch_loss=-4.600542\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:07:46 INFO 140466583250752] #quality_metric: host=algo-1, epoch=165, batch=0 train loss <loss>=-4.60054159164\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:07:47 INFO 140466583250752] Epoch[165] Batch[5] avg_epoch_loss=-4.509156\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:07:47 INFO 140466583250752] #quality_metric: host=algo-1, epoch=165, batch=5 train loss <loss>=-4.50915630658\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:07:47 INFO 140466583250752] Epoch[165] Batch [5]#011Speed: 1809.43 samples/sec#011loss=-4.509156\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:07:47 INFO 140466583250752] Epoch[165] Batch[10] avg_epoch_loss=-4.657309\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:07:47 INFO 140466583250752] #quality_metric: host=algo-1, epoch=165, batch=10 train loss <loss>=-4.83509263992\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:07:47 INFO 140466583250752] Epoch[165] Batch [10]#011Speed: 1149.38 samples/sec#011loss=-4.835093\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:07:47 INFO 140466583250752] processed a total of 1313 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52804.59499359131, \"sum\": 52804.59499359131, \"min\": 52804.59499359131}}, \"EndTime\": 1593886067.722289, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593886014.917144}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:07:47 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=24.8652014795 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:07:47 INFO 140466583250752] #progress_metric: host=algo-1, completed 83 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:07:47 INFO 140466583250752] #quality_metric: host=algo-1, epoch=165, train loss <loss>=-4.65730918537\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:07:47 INFO 140466583250752] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:07:47 INFO 140466583250752] Saved checkpoint to \"/opt/ml/model/state_77326b41-003f-4c1a-b524-ed437438499f-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 8.157014846801758, \"sum\": 8.157014846801758, \"min\": 8.157014846801758}}, \"EndTime\": 1593886067.731058, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593886067.722374}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:08:39 INFO 140466583250752] Epoch[166] Batch[0] avg_epoch_loss=-4.040348\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:08:39 INFO 140466583250752] #quality_metric: host=algo-1, epoch=166, batch=0 train loss <loss>=-4.04034805298\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:08:39 INFO 140466583250752] Epoch[166] Batch[5] avg_epoch_loss=-4.129119\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:08:39 INFO 140466583250752] #quality_metric: host=algo-1, epoch=166, batch=5 train loss <loss>=-4.12911899885\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:08:39 INFO 140466583250752] Epoch[166] Batch [5]#011Speed: 1815.47 samples/sec#011loss=-4.129119\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:08:40 INFO 140466583250752] Epoch[166] Batch[10] avg_epoch_loss=-4.064106\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:08:40 INFO 140466583250752] #quality_metric: host=algo-1, epoch=166, batch=10 train loss <loss>=-3.98608942032\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:08:40 INFO 140466583250752] Epoch[166] Batch [10]#011Speed: 1082.45 samples/sec#011loss=-3.986089\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:08:40 INFO 140466583250752] processed a total of 1285 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52826.10106468201, \"sum\": 52826.10106468201, \"min\": 52826.10106468201}}, \"EndTime\": 1593886120.557296, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593886067.731129}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:08:40 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=24.3250392957 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:08:40 INFO 140466583250752] #progress_metric: host=algo-1, completed 83 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:08:40 INFO 140466583250752] #quality_metric: host=algo-1, epoch=166, train loss <loss>=-4.06410555406\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:08:40 INFO 140466583250752] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:09:31 INFO 140466583250752] Epoch[167] Batch[0] avg_epoch_loss=-3.947843\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:09:31 INFO 140466583250752] #quality_metric: host=algo-1, epoch=167, batch=0 train loss <loss>=-3.94784331322\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:09:32 INFO 140466583250752] Epoch[167] Batch[5] avg_epoch_loss=-3.850645\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:09:32 INFO 140466583250752] #quality_metric: host=algo-1, epoch=167, batch=5 train loss <loss>=-3.85064490636\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:09:32 INFO 140466583250752] Epoch[167] Batch [5]#011Speed: 1805.73 samples/sec#011loss=-3.850645\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:09:32 INFO 140466583250752] processed a total of 1229 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52190.950870513916, \"sum\": 52190.950870513916, \"min\": 52190.950870513916}}, \"EndTime\": 1593886172.748867, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593886120.557382}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:09:32 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=23.5480871225 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:09:32 INFO 140466583250752] #progress_metric: host=algo-1, completed 84 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:09:32 INFO 140466583250752] #quality_metric: host=algo-1, epoch=167, train loss <loss>=-3.88285288811\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:09:32 INFO 140466583250752] loss did not improve\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[07/04/2020 18:10:24 INFO 140466583250752] Epoch[168] Batch[0] avg_epoch_loss=-4.131955\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:10:24 INFO 140466583250752] #quality_metric: host=algo-1, epoch=168, batch=0 train loss <loss>=-4.13195514679\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:10:25 INFO 140466583250752] Epoch[168] Batch[5] avg_epoch_loss=-4.113587\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:10:25 INFO 140466583250752] #quality_metric: host=algo-1, epoch=168, batch=5 train loss <loss>=-4.11358706156\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:10:25 INFO 140466583250752] Epoch[168] Batch [5]#011Speed: 1815.41 samples/sec#011loss=-4.113587\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:10:25 INFO 140466583250752] Epoch[168] Batch[10] avg_epoch_loss=-4.186040\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:10:25 INFO 140466583250752] #quality_metric: host=algo-1, epoch=168, batch=10 train loss <loss>=-4.27298316956\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:10:25 INFO 140466583250752] Epoch[168] Batch [10]#011Speed: 1313.11 samples/sec#011loss=-4.272983\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:10:25 INFO 140466583250752] processed a total of 1370 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52816.96915626526, \"sum\": 52816.96915626526, \"min\": 52816.96915626526}}, \"EndTime\": 1593886225.566513, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593886172.74895}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:10:25 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=25.9385651331 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:10:25 INFO 140466583250752] #progress_metric: host=algo-1, completed 84 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:10:25 INFO 140466583250752] #quality_metric: host=algo-1, epoch=168, train loss <loss>=-4.18603983792\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:10:25 INFO 140466583250752] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:11:17 INFO 140466583250752] Epoch[169] Batch[0] avg_epoch_loss=-4.335167\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:11:17 INFO 140466583250752] #quality_metric: host=algo-1, epoch=169, batch=0 train loss <loss>=-4.33516693115\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:11:17 INFO 140466583250752] Epoch[169] Batch[5] avg_epoch_loss=-4.415439\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:11:17 INFO 140466583250752] #quality_metric: host=algo-1, epoch=169, batch=5 train loss <loss>=-4.41543912888\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:11:17 INFO 140466583250752] Epoch[169] Batch [5]#011Speed: 1823.22 samples/sec#011loss=-4.415439\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:11:17 INFO 140466583250752] processed a total of 1255 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52294.07501220703, \"sum\": 52294.07501220703, \"min\": 52294.07501220703}}, \"EndTime\": 1593886277.861233, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593886225.566608}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:11:17 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=23.9988290575 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:11:17 INFO 140466583250752] #progress_metric: host=algo-1, completed 85 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:11:17 INFO 140466583250752] #quality_metric: host=algo-1, epoch=169, train loss <loss>=-4.4465681076\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:11:17 INFO 140466583250752] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:12:09 INFO 140466583250752] Epoch[170] Batch[0] avg_epoch_loss=-4.376182\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:12:09 INFO 140466583250752] #quality_metric: host=algo-1, epoch=170, batch=0 train loss <loss>=-4.37618160248\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:12:10 INFO 140466583250752] Epoch[170] Batch[5] avg_epoch_loss=-4.421766\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:12:10 INFO 140466583250752] #quality_metric: host=algo-1, epoch=170, batch=5 train loss <loss>=-4.42176564535\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:12:10 INFO 140466583250752] Epoch[170] Batch [5]#011Speed: 1806.96 samples/sec#011loss=-4.421766\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:12:10 INFO 140466583250752] Epoch[170] Batch[10] avg_epoch_loss=-4.521592\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:12:10 INFO 140466583250752] #quality_metric: host=algo-1, epoch=170, batch=10 train loss <loss>=-4.64138298035\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:12:10 INFO 140466583250752] Epoch[170] Batch [10]#011Speed: 1209.90 samples/sec#011loss=-4.641383\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:12:10 INFO 140466583250752] processed a total of 1331 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52711.0378742218, \"sum\": 52711.0378742218, \"min\": 52711.0378742218}}, \"EndTime\": 1593886330.572959, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593886277.861332}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:12:10 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=25.2508121249 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:12:10 INFO 140466583250752] #progress_metric: host=algo-1, completed 85 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:12:10 INFO 140466583250752] #quality_metric: host=algo-1, epoch=170, train loss <loss>=-4.52159170671\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:12:10 INFO 140466583250752] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:13:02 INFO 140466583250752] Epoch[171] Batch[0] avg_epoch_loss=-4.358971\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:13:02 INFO 140466583250752] #quality_metric: host=algo-1, epoch=171, batch=0 train loss <loss>=-4.35897111893\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:13:02 INFO 140466583250752] Epoch[171] Batch[5] avg_epoch_loss=-4.462736\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:13:02 INFO 140466583250752] #quality_metric: host=algo-1, epoch=171, batch=5 train loss <loss>=-4.46273644765\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:13:02 INFO 140466583250752] Epoch[171] Batch [5]#011Speed: 1791.77 samples/sec#011loss=-4.462736\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:13:03 INFO 140466583250752] processed a total of 1267 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52740.85307121277, \"sum\": 52740.85307121277, \"min\": 52740.85307121277}}, \"EndTime\": 1593886383.314433, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593886330.573052}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:13:03 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=24.023061525 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:13:03 INFO 140466583250752] #progress_metric: host=algo-1, completed 86 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:13:03 INFO 140466583250752] #quality_metric: host=algo-1, epoch=171, train loss <loss>=-4.50627551079\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:13:03 INFO 140466583250752] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:13:54 INFO 140466583250752] Epoch[172] Batch[0] avg_epoch_loss=-4.246268\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:13:54 INFO 140466583250752] #quality_metric: host=algo-1, epoch=172, batch=0 train loss <loss>=-4.2462682724\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:13:55 INFO 140466583250752] Epoch[172] Batch[5] avg_epoch_loss=-4.463069\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:13:55 INFO 140466583250752] #quality_metric: host=algo-1, epoch=172, batch=5 train loss <loss>=-4.46306864421\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:13:55 INFO 140466583250752] Epoch[172] Batch [5]#011Speed: 1777.23 samples/sec#011loss=-4.463069\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:13:55 INFO 140466583250752] processed a total of 1278 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52242.44689941406, \"sum\": 52242.44689941406, \"min\": 52242.44689941406}}, \"EndTime\": 1593886435.557536, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593886383.314525}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:13:55 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=24.4628124984 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:13:55 INFO 140466583250752] #progress_metric: host=algo-1, completed 86 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:13:55 INFO 140466583250752] #quality_metric: host=algo-1, epoch=172, train loss <loss>=-4.48745532036\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:13:55 INFO 140466583250752] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:14:46 INFO 140466583250752] Epoch[173] Batch[0] avg_epoch_loss=-4.606664\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:14:46 INFO 140466583250752] #quality_metric: host=algo-1, epoch=173, batch=0 train loss <loss>=-4.60666418076\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:14:47 INFO 140466583250752] Epoch[173] Batch[5] avg_epoch_loss=-4.501785\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:14:47 INFO 140466583250752] #quality_metric: host=algo-1, epoch=173, batch=5 train loss <loss>=-4.50178496043\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:14:47 INFO 140466583250752] Epoch[173] Batch [5]#011Speed: 1813.64 samples/sec#011loss=-4.501785\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:14:47 INFO 140466583250752] processed a total of 1210 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52154.63900566101, \"sum\": 52154.63900566101, \"min\": 52154.63900566101}}, \"EndTime\": 1593886487.712781, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593886435.557609}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:14:47 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=23.200176718 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:14:47 INFO 140466583250752] #progress_metric: host=algo-1, completed 87 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:14:47 INFO 140466583250752] #quality_metric: host=algo-1, epoch=173, train loss <loss>=-4.5659992218\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:14:47 INFO 140466583250752] loss did not improve\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[07/04/2020 18:15:39 INFO 140466583250752] Epoch[174] Batch[0] avg_epoch_loss=-4.683774\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:15:39 INFO 140466583250752] #quality_metric: host=algo-1, epoch=174, batch=0 train loss <loss>=-4.68377351761\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:15:39 INFO 140466583250752] Epoch[174] Batch[5] avg_epoch_loss=-4.456165\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:15:39 INFO 140466583250752] #quality_metric: host=algo-1, epoch=174, batch=5 train loss <loss>=-4.45616483688\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:15:39 INFO 140466583250752] Epoch[174] Batch [5]#011Speed: 1819.35 samples/sec#011loss=-4.456165\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:15:40 INFO 140466583250752] processed a total of 1259 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52685.52088737488, \"sum\": 52685.52088737488, \"min\": 52685.52088737488}}, \"EndTime\": 1593886540.398934, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593886487.712874}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:15:40 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=23.8964467961 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:15:40 INFO 140466583250752] #progress_metric: host=algo-1, completed 87 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:15:40 INFO 140466583250752] #quality_metric: host=algo-1, epoch=174, train loss <loss>=-4.46450448036\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:15:40 INFO 140466583250752] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:16:32 INFO 140466583250752] Epoch[175] Batch[0] avg_epoch_loss=-4.535632\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:16:32 INFO 140466583250752] #quality_metric: host=algo-1, epoch=175, batch=0 train loss <loss>=-4.53563165665\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:16:32 INFO 140466583250752] Epoch[175] Batch[5] avg_epoch_loss=-4.463175\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:16:32 INFO 140466583250752] #quality_metric: host=algo-1, epoch=175, batch=5 train loss <loss>=-4.46317497889\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:16:32 INFO 140466583250752] Epoch[175] Batch [5]#011Speed: 1809.83 samples/sec#011loss=-4.463175\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:16:33 INFO 140466583250752] Epoch[175] Batch[10] avg_epoch_loss=-4.460822\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:16:33 INFO 140466583250752] #quality_metric: host=algo-1, epoch=175, batch=10 train loss <loss>=-4.45799856186\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:16:33 INFO 140466583250752] Epoch[175] Batch [10]#011Speed: 1119.08 samples/sec#011loss=-4.457999\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:16:33 INFO 140466583250752] processed a total of 1298 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52931.84995651245, \"sum\": 52931.84995651245, \"min\": 52931.84995651245}}, \"EndTime\": 1593886593.33142, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593886540.399027}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:16:33 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=24.5220444251 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:16:33 INFO 140466583250752] #progress_metric: host=algo-1, completed 88 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:16:33 INFO 140466583250752] #quality_metric: host=algo-1, epoch=175, train loss <loss>=-4.46082206206\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:16:33 INFO 140466583250752] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:17:25 INFO 140466583250752] Epoch[176] Batch[0] avg_epoch_loss=-4.517297\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:17:25 INFO 140466583250752] #quality_metric: host=algo-1, epoch=176, batch=0 train loss <loss>=-4.51729679108\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:17:25 INFO 140466583250752] Epoch[176] Batch[5] avg_epoch_loss=-4.493113\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:17:25 INFO 140466583250752] #quality_metric: host=algo-1, epoch=176, batch=5 train loss <loss>=-4.49311288198\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:17:25 INFO 140466583250752] Epoch[176] Batch [5]#011Speed: 1808.09 samples/sec#011loss=-4.493113\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:17:25 INFO 140466583250752] processed a total of 1249 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52514.75787162781, \"sum\": 52514.75787162781, \"min\": 52514.75787162781}}, \"EndTime\": 1593886645.846821, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593886593.331494}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:17:25 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=23.7837370623 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:17:25 INFO 140466583250752] #progress_metric: host=algo-1, completed 88 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:17:25 INFO 140466583250752] #quality_metric: host=algo-1, epoch=176, train loss <loss>=-4.5325158596\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:17:25 INFO 140466583250752] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:18:17 INFO 140466583250752] Epoch[177] Batch[0] avg_epoch_loss=-4.439602\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:18:17 INFO 140466583250752] #quality_metric: host=algo-1, epoch=177, batch=0 train loss <loss>=-4.43960189819\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:18:17 INFO 140466583250752] Epoch[177] Batch[5] avg_epoch_loss=-4.586987\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:18:17 INFO 140466583250752] #quality_metric: host=algo-1, epoch=177, batch=5 train loss <loss>=-4.58698693911\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:18:17 INFO 140466583250752] Epoch[177] Batch [5]#011Speed: 1791.57 samples/sec#011loss=-4.586987\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:18:18 INFO 140466583250752] Epoch[177] Batch[10] avg_epoch_loss=-4.526932\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:18:18 INFO 140466583250752] #quality_metric: host=algo-1, epoch=177, batch=10 train loss <loss>=-4.45486679077\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:18:18 INFO 140466583250752] Epoch[177] Batch [10]#011Speed: 1098.80 samples/sec#011loss=-4.454867\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:18:18 INFO 140466583250752] processed a total of 1290 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52401.21412277222, \"sum\": 52401.21412277222, \"min\": 52401.21412277222}}, \"EndTime\": 1593886698.248681, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593886645.846904}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:18:18 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=24.6176859183 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:18:18 INFO 140466583250752] #progress_metric: host=algo-1, completed 89 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:18:18 INFO 140466583250752] #quality_metric: host=algo-1, epoch=177, train loss <loss>=-4.52693232623\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:18:18 INFO 140466583250752] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:19:09 INFO 140466583250752] Epoch[178] Batch[0] avg_epoch_loss=-4.229036\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:19:09 INFO 140466583250752] #quality_metric: host=algo-1, epoch=178, batch=0 train loss <loss>=-4.22903633118\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:19:10 INFO 140466583250752] Epoch[178] Batch[5] avg_epoch_loss=-4.472546\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:19:10 INFO 140466583250752] #quality_metric: host=algo-1, epoch=178, batch=5 train loss <loss>=-4.47254602114\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:19:10 INFO 140466583250752] Epoch[178] Batch [5]#011Speed: 1820.74 samples/sec#011loss=-4.472546\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:19:10 INFO 140466583250752] processed a total of 1255 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52334.457874298096, \"sum\": 52334.457874298096, \"min\": 52334.457874298096}}, \"EndTime\": 1593886750.583718, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593886698.248774}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:19:10 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=23.9803121915 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:19:10 INFO 140466583250752] #progress_metric: host=algo-1, completed 89 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:19:10 INFO 140466583250752] #quality_metric: host=algo-1, epoch=178, train loss <loss>=-4.49034290314\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:19:10 INFO 140466583250752] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:20:01 INFO 140466583250752] Epoch[179] Batch[0] avg_epoch_loss=-4.537788\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:20:01 INFO 140466583250752] #quality_metric: host=algo-1, epoch=179, batch=0 train loss <loss>=-4.53778791428\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:20:02 INFO 140466583250752] Epoch[179] Batch[5] avg_epoch_loss=-4.542520\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:20:02 INFO 140466583250752] #quality_metric: host=algo-1, epoch=179, batch=5 train loss <loss>=-4.5425195694\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:20:02 INFO 140466583250752] Epoch[179] Batch [5]#011Speed: 1795.70 samples/sec#011loss=-4.542520\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:20:02 INFO 140466583250752] Epoch[179] Batch[10] avg_epoch_loss=-4.518527\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:20:02 INFO 140466583250752] #quality_metric: host=algo-1, epoch=179, batch=10 train loss <loss>=-4.48973579407\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:20:02 INFO 140466583250752] Epoch[179] Batch [10]#011Speed: 1106.48 samples/sec#011loss=-4.489736\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:20:02 INFO 140466583250752] processed a total of 1299 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52323.30799102783, \"sum\": 52323.30799102783, \"min\": 52323.30799102783}}, \"EndTime\": 1593886802.90775, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593886750.583815}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:20:02 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=24.8263546192 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:20:02 INFO 140466583250752] #progress_metric: host=algo-1, completed 90 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:20:02 INFO 140466583250752] #quality_metric: host=algo-1, epoch=179, train loss <loss>=-4.51852694425\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:20:02 INFO 140466583250752] loss did not improve\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[07/04/2020 18:20:54 INFO 140466583250752] Epoch[180] Batch[0] avg_epoch_loss=-4.602485\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:20:54 INFO 140466583250752] #quality_metric: host=algo-1, epoch=180, batch=0 train loss <loss>=-4.60248470306\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:20:55 INFO 140466583250752] Epoch[180] Batch[5] avg_epoch_loss=-4.495529\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:20:55 INFO 140466583250752] #quality_metric: host=algo-1, epoch=180, batch=5 train loss <loss>=-4.49552869797\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:20:55 INFO 140466583250752] Epoch[180] Batch [5]#011Speed: 1814.34 samples/sec#011loss=-4.495529\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:20:55 INFO 140466583250752] processed a total of 1267 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52764.116048812866, \"sum\": 52764.116048812866, \"min\": 52764.116048812866}}, \"EndTime\": 1593886855.672465, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593886802.90783}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:20:55 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=24.0124815053 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:20:55 INFO 140466583250752] #progress_metric: host=algo-1, completed 90 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:20:55 INFO 140466583250752] #quality_metric: host=algo-1, epoch=180, train loss <loss>=-4.51856718063\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:20:55 INFO 140466583250752] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:21:47 INFO 140466583250752] Epoch[181] Batch[0] avg_epoch_loss=-4.363579\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:21:47 INFO 140466583250752] #quality_metric: host=algo-1, epoch=181, batch=0 train loss <loss>=-4.36357879639\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:21:47 INFO 140466583250752] Epoch[181] Batch[5] avg_epoch_loss=-4.506381\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:21:47 INFO 140466583250752] #quality_metric: host=algo-1, epoch=181, batch=5 train loss <loss>=-4.50638055801\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:21:47 INFO 140466583250752] Epoch[181] Batch [5]#011Speed: 1815.89 samples/sec#011loss=-4.506381\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:21:48 INFO 140466583250752] processed a total of 1262 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52547.7499961853, \"sum\": 52547.7499961853, \"min\": 52547.7499961853}}, \"EndTime\": 1593886908.2208, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593886855.672534}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:21:48 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=24.016197513 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:21:48 INFO 140466583250752] #progress_metric: host=algo-1, completed 91 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:21:48 INFO 140466583250752] #quality_metric: host=algo-1, epoch=181, train loss <loss>=-4.52547254562\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:21:48 INFO 140466583250752] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:22:40 INFO 140466583250752] Epoch[182] Batch[0] avg_epoch_loss=-4.538881\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:22:40 INFO 140466583250752] #quality_metric: host=algo-1, epoch=182, batch=0 train loss <loss>=-4.53888130188\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:22:40 INFO 140466583250752] Epoch[182] Batch[5] avg_epoch_loss=-4.542152\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:22:40 INFO 140466583250752] #quality_metric: host=algo-1, epoch=182, batch=5 train loss <loss>=-4.54215208689\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:22:40 INFO 140466583250752] Epoch[182] Batch [5]#011Speed: 1769.22 samples/sec#011loss=-4.542152\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:22:40 INFO 140466583250752] Epoch[182] Batch[10] avg_epoch_loss=-4.562452\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:22:40 INFO 140466583250752] #quality_metric: host=algo-1, epoch=182, batch=10 train loss <loss>=-4.58681240082\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:22:40 INFO 140466583250752] Epoch[182] Batch [10]#011Speed: 1120.20 samples/sec#011loss=-4.586812\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:22:40 INFO 140466583250752] processed a total of 1300 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52772.92203903198, \"sum\": 52772.92203903198, \"min\": 52772.92203903198}}, \"EndTime\": 1593886960.994367, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593886908.220873}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:22:40 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=24.6337917044 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:22:40 INFO 140466583250752] #progress_metric: host=algo-1, completed 91 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:22:40 INFO 140466583250752] #quality_metric: host=algo-1, epoch=182, train loss <loss>=-4.56245222959\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:22:40 INFO 140466583250752] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:23:32 INFO 140466583250752] Epoch[183] Batch[0] avg_epoch_loss=-4.690860\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:23:32 INFO 140466583250752] #quality_metric: host=algo-1, epoch=183, batch=0 train loss <loss>=-4.69086027145\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:23:32 INFO 140466583250752] Epoch[183] Batch[5] avg_epoch_loss=-4.556170\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:23:32 INFO 140466583250752] #quality_metric: host=algo-1, epoch=183, batch=5 train loss <loss>=-4.55617046356\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:23:32 INFO 140466583250752] Epoch[183] Batch [5]#011Speed: 1779.96 samples/sec#011loss=-4.556170\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:23:33 INFO 140466583250752] processed a total of 1271 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52293.15996170044, \"sum\": 52293.15996170044, \"min\": 52293.15996170044}}, \"EndTime\": 1593887013.288142, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593886960.994445}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:23:33 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=24.3052196205 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:23:33 INFO 140466583250752] #progress_metric: host=algo-1, completed 92 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:23:33 INFO 140466583250752] #quality_metric: host=algo-1, epoch=183, train loss <loss>=-4.57634592056\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:23:33 INFO 140466583250752] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:24:24 INFO 140466583250752] Epoch[184] Batch[0] avg_epoch_loss=-4.634574\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:24:24 INFO 140466583250752] #quality_metric: host=algo-1, epoch=184, batch=0 train loss <loss>=-4.63457393646\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:24:24 INFO 140466583250752] Epoch[184] Batch[5] avg_epoch_loss=-4.597583\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:24:24 INFO 140466583250752] #quality_metric: host=algo-1, epoch=184, batch=5 train loss <loss>=-4.59758281708\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:24:24 INFO 140466583250752] Epoch[184] Batch [5]#011Speed: 1789.14 samples/sec#011loss=-4.597583\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:24:25 INFO 140466583250752] processed a total of 1226 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52002.341985702515, \"sum\": 52002.341985702515, \"min\": 52002.341985702515}}, \"EndTime\": 1593887065.291162, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593887013.288234}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:24:25 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=23.5758114336 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:24:25 INFO 140466583250752] #progress_metric: host=algo-1, completed 92 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:24:25 INFO 140466583250752] #quality_metric: host=algo-1, epoch=184, train loss <loss>=-4.57593750954\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:24:25 INFO 140466583250752] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:25:16 INFO 140466583250752] Epoch[185] Batch[0] avg_epoch_loss=-4.251232\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:25:16 INFO 140466583250752] #quality_metric: host=algo-1, epoch=185, batch=0 train loss <loss>=-4.25123167038\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:25:17 INFO 140466583250752] Epoch[185] Batch[5] avg_epoch_loss=-4.291955\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:25:17 INFO 140466583250752] #quality_metric: host=algo-1, epoch=185, batch=5 train loss <loss>=-4.29195475578\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:25:17 INFO 140466583250752] Epoch[185] Batch [5]#011Speed: 1800.18 samples/sec#011loss=-4.291955\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:25:17 INFO 140466583250752] processed a total of 1270 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52381.4492225647, \"sum\": 52381.4492225647, \"min\": 52381.4492225647}}, \"EndTime\": 1593887117.673214, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593887065.291233}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:25:17 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=24.245165426 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:25:17 INFO 140466583250752] #progress_metric: host=algo-1, completed 93 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:25:17 INFO 140466583250752] #quality_metric: host=algo-1, epoch=185, train loss <loss>=-4.31219372749\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:25:17 INFO 140466583250752] loss did not improve\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[07/04/2020 18:26:09 INFO 140466583250752] Epoch[186] Batch[0] avg_epoch_loss=-4.420472\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:26:09 INFO 140466583250752] #quality_metric: host=algo-1, epoch=186, batch=0 train loss <loss>=-4.42047166824\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:26:09 INFO 140466583250752] Epoch[186] Batch[5] avg_epoch_loss=-4.495818\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:26:09 INFO 140466583250752] #quality_metric: host=algo-1, epoch=186, batch=5 train loss <loss>=-4.4958178997\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:26:09 INFO 140466583250752] Epoch[186] Batch [5]#011Speed: 1803.96 samples/sec#011loss=-4.495818\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:26:09 INFO 140466583250752] processed a total of 1275 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52178.12895774841, \"sum\": 52178.12895774841, \"min\": 52178.12895774841}}, \"EndTime\": 1593887169.851948, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593887117.673298}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:26:09 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=24.4354688785 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:26:09 INFO 140466583250752] #progress_metric: host=algo-1, completed 93 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:26:09 INFO 140466583250752] #quality_metric: host=algo-1, epoch=186, train loss <loss>=-4.48644275665\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:26:09 INFO 140466583250752] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:27:00 INFO 140466583250752] Epoch[187] Batch[0] avg_epoch_loss=-4.239805\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:27:00 INFO 140466583250752] #quality_metric: host=algo-1, epoch=187, batch=0 train loss <loss>=-4.23980474472\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:27:01 INFO 140466583250752] Epoch[187] Batch[5] avg_epoch_loss=-4.478384\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:27:01 INFO 140466583250752] #quality_metric: host=algo-1, epoch=187, batch=5 train loss <loss>=-4.478383859\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:27:01 INFO 140466583250752] Epoch[187] Batch [5]#011Speed: 1808.71 samples/sec#011loss=-4.478384\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:27:01 INFO 140466583250752] processed a total of 1218 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 51934.338092803955, \"sum\": 51934.338092803955, \"min\": 51934.338092803955}}, \"EndTime\": 1593887221.786897, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593887169.852025}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:27:01 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=23.4526376987 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:27:01 INFO 140466583250752] #progress_metric: host=algo-1, completed 94 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:27:01 INFO 140466583250752] #quality_metric: host=algo-1, epoch=187, train loss <loss>=-4.44257097244\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:27:01 INFO 140466583250752] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:27:53 INFO 140466583250752] Epoch[188] Batch[0] avg_epoch_loss=-4.563381\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:27:53 INFO 140466583250752] #quality_metric: host=algo-1, epoch=188, batch=0 train loss <loss>=-4.56338119507\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:27:53 INFO 140466583250752] Epoch[188] Batch[5] avg_epoch_loss=-4.522297\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:27:53 INFO 140466583250752] #quality_metric: host=algo-1, epoch=188, batch=5 train loss <loss>=-4.52229650815\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:27:53 INFO 140466583250752] Epoch[188] Batch [5]#011Speed: 1827.05 samples/sec#011loss=-4.522297\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:27:54 INFO 140466583250752] Epoch[188] Batch[10] avg_epoch_loss=-4.509375\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:27:54 INFO 140466583250752] #quality_metric: host=algo-1, epoch=188, batch=10 train loss <loss>=-4.49386911392\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:27:54 INFO 140466583250752] Epoch[188] Batch [10]#011Speed: 1096.77 samples/sec#011loss=-4.493869\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:27:54 INFO 140466583250752] processed a total of 1285 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52530.50398826599, \"sum\": 52530.50398826599, \"min\": 52530.50398826599}}, \"EndTime\": 1593887274.318043, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593887221.786972}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:27:54 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=24.4619214175 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:27:54 INFO 140466583250752] #progress_metric: host=algo-1, completed 94 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:27:54 INFO 140466583250752] #quality_metric: host=algo-1, epoch=188, train loss <loss>=-4.50937496532\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:27:54 INFO 140466583250752] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:28:46 INFO 140466583250752] Epoch[189] Batch[0] avg_epoch_loss=-3.915896\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:28:46 INFO 140466583250752] #quality_metric: host=algo-1, epoch=189, batch=0 train loss <loss>=-3.91589641571\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:28:46 INFO 140466583250752] Epoch[189] Batch[5] avg_epoch_loss=-4.340706\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:28:46 INFO 140466583250752] #quality_metric: host=algo-1, epoch=189, batch=5 train loss <loss>=-4.34070587158\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:28:46 INFO 140466583250752] Epoch[189] Batch [5]#011Speed: 1813.65 samples/sec#011loss=-4.340706\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:28:47 INFO 140466583250752] Epoch[189] Batch[10] avg_epoch_loss=-4.385736\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:28:47 INFO 140466583250752] #quality_metric: host=algo-1, epoch=189, batch=10 train loss <loss>=-4.43977222443\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:28:47 INFO 140466583250752] Epoch[189] Batch [10]#011Speed: 1233.25 samples/sec#011loss=-4.439772\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:28:47 INFO 140466583250752] processed a total of 1333 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52704.18906211853, \"sum\": 52704.18906211853, \"min\": 52704.18906211853}}, \"EndTime\": 1593887327.022814, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593887274.318121}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:28:47 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=25.2920425628 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:28:47 INFO 140466583250752] #progress_metric: host=algo-1, completed 95 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:28:47 INFO 140466583250752] #quality_metric: host=algo-1, epoch=189, train loss <loss>=-4.38573603197\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:28:47 INFO 140466583250752] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:29:38 INFO 140466583250752] Epoch[190] Batch[0] avg_epoch_loss=-4.360591\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:29:38 INFO 140466583250752] #quality_metric: host=algo-1, epoch=190, batch=0 train loss <loss>=-4.36059093475\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:29:38 INFO 140466583250752] Epoch[190] Batch[5] avg_epoch_loss=-4.460496\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:29:38 INFO 140466583250752] #quality_metric: host=algo-1, epoch=190, batch=5 train loss <loss>=-4.46049626668\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:29:38 INFO 140466583250752] Epoch[190] Batch [5]#011Speed: 1804.68 samples/sec#011loss=-4.460496\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:29:39 INFO 140466583250752] processed a total of 1262 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52224.271059036255, \"sum\": 52224.271059036255, \"min\": 52224.271059036255}}, \"EndTime\": 1593887379.247693, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593887327.022908}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:29:39 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=24.1649533374 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:29:39 INFO 140466583250752] #progress_metric: host=algo-1, completed 95 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:29:39 INFO 140466583250752] #quality_metric: host=algo-1, epoch=190, train loss <loss>=-4.46653456688\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:29:39 INFO 140466583250752] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:30:30 INFO 140466583250752] Epoch[191] Batch[0] avg_epoch_loss=-4.549672\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:30:30 INFO 140466583250752] #quality_metric: host=algo-1, epoch=191, batch=0 train loss <loss>=-4.54967212677\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:30:31 INFO 140466583250752] Epoch[191] Batch[5] avg_epoch_loss=-4.390247\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:30:31 INFO 140466583250752] #quality_metric: host=algo-1, epoch=191, batch=5 train loss <loss>=-4.39024718602\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:30:31 INFO 140466583250752] Epoch[191] Batch [5]#011Speed: 1807.94 samples/sec#011loss=-4.390247\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:30:31 INFO 140466583250752] processed a total of 1247 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52340.39402008057, \"sum\": 52340.39402008057, \"min\": 52340.39402008057}}, \"EndTime\": 1593887431.588739, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593887379.247767}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:30:31 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=23.8247478215 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:30:31 INFO 140466583250752] #progress_metric: host=algo-1, completed 96 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:30:31 INFO 140466583250752] #quality_metric: host=algo-1, epoch=191, train loss <loss>=-4.48669428825\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:30:31 INFO 140466583250752] loss did not improve\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[07/04/2020 18:31:23 INFO 140466583250752] Epoch[192] Batch[0] avg_epoch_loss=-4.491228\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:31:23 INFO 140466583250752] #quality_metric: host=algo-1, epoch=192, batch=0 train loss <loss>=-4.49122810364\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:31:23 INFO 140466583250752] Epoch[192] Batch[5] avg_epoch_loss=-4.530313\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:31:23 INFO 140466583250752] #quality_metric: host=algo-1, epoch=192, batch=5 train loss <loss>=-4.53031269709\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:31:23 INFO 140466583250752] Epoch[192] Batch [5]#011Speed: 1818.93 samples/sec#011loss=-4.530313\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:31:24 INFO 140466583250752] Epoch[192] Batch[10] avg_epoch_loss=-4.546350\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:31:24 INFO 140466583250752] #quality_metric: host=algo-1, epoch=192, batch=10 train loss <loss>=-4.56559391022\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:31:24 INFO 140466583250752] Epoch[192] Batch [10]#011Speed: 1228.28 samples/sec#011loss=-4.565594\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:31:24 INFO 140466583250752] processed a total of 1338 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52735.694885253906, \"sum\": 52735.694885253906, \"min\": 52735.694885253906}}, \"EndTime\": 1593887484.325093, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593887431.588833}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:31:24 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=25.3717424268 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:31:24 INFO 140466583250752] #progress_metric: host=algo-1, completed 96 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:31:24 INFO 140466583250752] #quality_metric: host=algo-1, epoch=192, train loss <loss>=-4.54634961215\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:31:24 INFO 140466583250752] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:32:15 INFO 140466583250752] Epoch[193] Batch[0] avg_epoch_loss=-4.705844\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:32:15 INFO 140466583250752] #quality_metric: host=algo-1, epoch=193, batch=0 train loss <loss>=-4.70584392548\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:32:16 INFO 140466583250752] Epoch[193] Batch[5] avg_epoch_loss=-4.672621\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:32:16 INFO 140466583250752] #quality_metric: host=algo-1, epoch=193, batch=5 train loss <loss>=-4.67262132963\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:32:16 INFO 140466583250752] Epoch[193] Batch [5]#011Speed: 1825.18 samples/sec#011loss=-4.672621\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:32:16 INFO 140466583250752] processed a total of 1242 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52429.3098449707, \"sum\": 52429.3098449707, \"min\": 52429.3098449707}}, \"EndTime\": 1593887536.755061, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593887484.325188}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:32:16 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=23.6889753438 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:32:16 INFO 140466583250752] #progress_metric: host=algo-1, completed 97 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:32:16 INFO 140466583250752] #quality_metric: host=algo-1, epoch=193, train loss <loss>=-4.58097267151\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:32:16 INFO 140466583250752] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:33:08 INFO 140466583250752] Epoch[194] Batch[0] avg_epoch_loss=-3.219323\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:33:08 INFO 140466583250752] #quality_metric: host=algo-1, epoch=194, batch=0 train loss <loss>=-3.21932291985\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:33:09 INFO 140466583250752] Epoch[194] Batch[5] avg_epoch_loss=-3.422367\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:33:09 INFO 140466583250752] #quality_metric: host=algo-1, epoch=194, batch=5 train loss <loss>=-3.42236689727\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:33:09 INFO 140466583250752] Epoch[194] Batch [5]#011Speed: 1797.37 samples/sec#011loss=-3.422367\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:33:09 INFO 140466583250752] Epoch[194] Batch[10] avg_epoch_loss=-3.762225\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:33:09 INFO 140466583250752] #quality_metric: host=algo-1, epoch=194, batch=10 train loss <loss>=-4.17005496025\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:33:09 INFO 140466583250752] Epoch[194] Batch [10]#011Speed: 1084.55 samples/sec#011loss=-4.170055\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:33:09 INFO 140466583250752] processed a total of 1284 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52982.98692703247, \"sum\": 52982.98692703247, \"min\": 52982.98692703247}}, \"EndTime\": 1593887589.738682, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593887536.755158}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:33:09 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=24.2341315995 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:33:09 INFO 140466583250752] #progress_metric: host=algo-1, completed 97 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:33:09 INFO 140466583250752] #quality_metric: host=algo-1, epoch=194, train loss <loss>=-3.76222510771\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:33:09 INFO 140466583250752] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:34:01 INFO 140466583250752] Epoch[195] Batch[0] avg_epoch_loss=-3.945612\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:34:01 INFO 140466583250752] #quality_metric: host=algo-1, epoch=195, batch=0 train loss <loss>=-3.94561243057\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:34:01 INFO 140466583250752] Epoch[195] Batch[5] avg_epoch_loss=-4.010056\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:34:01 INFO 140466583250752] #quality_metric: host=algo-1, epoch=195, batch=5 train loss <loss>=-4.01005593936\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:34:01 INFO 140466583250752] Epoch[195] Batch [5]#011Speed: 1802.73 samples/sec#011loss=-4.010056\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:34:01 INFO 140466583250752] processed a total of 1276 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52216.126918792725, \"sum\": 52216.126918792725, \"min\": 52216.126918792725}}, \"EndTime\": 1593887641.955445, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593887589.738776}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:34:01 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=24.4368390035 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:34:01 INFO 140466583250752] #progress_metric: host=algo-1, completed 98 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:34:01 INFO 140466583250752] #quality_metric: host=algo-1, epoch=195, train loss <loss>=-4.03232011795\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:34:01 INFO 140466583250752] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:34:53 INFO 140466583250752] Epoch[196] Batch[0] avg_epoch_loss=-4.098527\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:34:53 INFO 140466583250752] #quality_metric: host=algo-1, epoch=196, batch=0 train loss <loss>=-4.09852695465\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:34:53 INFO 140466583250752] Epoch[196] Batch[5] avg_epoch_loss=-4.105530\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:34:53 INFO 140466583250752] #quality_metric: host=algo-1, epoch=196, batch=5 train loss <loss>=-4.10553026199\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:34:53 INFO 140466583250752] Epoch[196] Batch [5]#011Speed: 1832.74 samples/sec#011loss=-4.105530\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:34:54 INFO 140466583250752] Epoch[196] Batch[10] avg_epoch_loss=-4.168017\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:34:54 INFO 140466583250752] #quality_metric: host=algo-1, epoch=196, batch=10 train loss <loss>=-4.24300127029\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:34:54 INFO 140466583250752] Epoch[196] Batch [10]#011Speed: 1118.36 samples/sec#011loss=-4.243001\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:34:54 INFO 140466583250752] processed a total of 1294 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52208.950996398926, \"sum\": 52208.950996398926, \"min\": 52208.950996398926}}, \"EndTime\": 1593887694.16504, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593887641.95552}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:34:54 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=24.7849612388 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:34:54 INFO 140466583250752] #progress_metric: host=algo-1, completed 98 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:34:54 INFO 140466583250752] #quality_metric: host=algo-1, epoch=196, train loss <loss>=-4.16801708395\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:34:54 INFO 140466583250752] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:35:45 INFO 140466583250752] Epoch[197] Batch[0] avg_epoch_loss=-4.343217\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:35:45 INFO 140466583250752] #quality_metric: host=algo-1, epoch=197, batch=0 train loss <loss>=-4.34321689606\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:35:46 INFO 140466583250752] Epoch[197] Batch[5] avg_epoch_loss=-4.414532\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:35:46 INFO 140466583250752] #quality_metric: host=algo-1, epoch=197, batch=5 train loss <loss>=-4.41453194618\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:35:46 INFO 140466583250752] Epoch[197] Batch [5]#011Speed: 1798.39 samples/sec#011loss=-4.414532\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:35:46 INFO 140466583250752] processed a total of 1274 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52501.888036727905, \"sum\": 52501.888036727905, \"min\": 52501.888036727905}}, \"EndTime\": 1593887746.667549, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593887694.165127}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:35:46 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=24.2657417735 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:35:46 INFO 140466583250752] #progress_metric: host=algo-1, completed 99 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:35:46 INFO 140466583250752] #quality_metric: host=algo-1, epoch=197, train loss <loss>=-4.42686476707\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:35:46 INFO 140466583250752] loss did not improve\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[07/04/2020 18:36:38 INFO 140466583250752] Epoch[198] Batch[0] avg_epoch_loss=-4.315952\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:36:38 INFO 140466583250752] #quality_metric: host=algo-1, epoch=198, batch=0 train loss <loss>=-4.31595230103\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:36:38 INFO 140466583250752] Epoch[198] Batch[5] avg_epoch_loss=-4.498737\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:36:38 INFO 140466583250752] #quality_metric: host=algo-1, epoch=198, batch=5 train loss <loss>=-4.49873725573\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:36:38 INFO 140466583250752] Epoch[198] Batch [5]#011Speed: 1810.37 samples/sec#011loss=-4.498737\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:36:39 INFO 140466583250752] Epoch[198] Batch[10] avg_epoch_loss=-3.679280\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:36:39 INFO 140466583250752] #quality_metric: host=algo-1, epoch=198, batch=10 train loss <loss>=-2.69593200684\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:36:39 INFO 140466583250752] Epoch[198] Batch [10]#011Speed: 1198.98 samples/sec#011loss=-2.695932\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:36:39 INFO 140466583250752] processed a total of 1321 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52731.06408119202, \"sum\": 52731.06408119202, \"min\": 52731.06408119202}}, \"EndTime\": 1593887799.399229, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593887746.66762}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:36:39 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=25.0515823745 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:36:39 INFO 140466583250752] #progress_metric: host=algo-1, completed 99 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:36:39 INFO 140466583250752] #quality_metric: host=algo-1, epoch=198, train loss <loss>=-3.67928032442\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:36:39 INFO 140466583250752] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:37:31 INFO 140466583250752] Epoch[199] Batch[0] avg_epoch_loss=-2.748653\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:37:31 INFO 140466583250752] #quality_metric: host=algo-1, epoch=199, batch=0 train loss <loss>=-2.74865293503\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:37:31 INFO 140466583250752] Epoch[199] Batch[5] avg_epoch_loss=-3.313119\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:37:31 INFO 140466583250752] #quality_metric: host=algo-1, epoch=199, batch=5 train loss <loss>=-3.31311861674\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:37:31 INFO 140466583250752] Epoch[199] Batch [5]#011Speed: 1833.91 samples/sec#011loss=-3.313119\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:37:32 INFO 140466583250752] Epoch[199] Batch[10] avg_epoch_loss=-3.263386\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:37:32 INFO 140466583250752] #quality_metric: host=algo-1, epoch=199, batch=10 train loss <loss>=-3.20370769501\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:37:32 INFO 140466583250752] Epoch[199] Batch [10]#011Speed: 1083.21 samples/sec#011loss=-3.203708\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:37:32 INFO 140466583250752] processed a total of 1283 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 52922.17302322388, \"sum\": 52922.17302322388, \"min\": 52922.17302322388}}, \"EndTime\": 1593887852.322094, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593887799.399321}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:37:32 INFO 140466583250752] #throughput_metric: host=algo-1, train throughput=24.243079669 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:37:32 INFO 140466583250752] #progress_metric: host=algo-1, completed 100 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:37:32 INFO 140466583250752] #quality_metric: host=algo-1, epoch=199, train loss <loss>=-3.26338637959\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:37:32 INFO 140466583250752] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:37:32 INFO 140466583250752] Final loss: -4.65730918537 (occurred at epoch 165)\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:37:32 INFO 140466583250752] #quality_metric: host=algo-1, train final_loss <loss>=-4.65730918537\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:37:32 INFO 140466583250752] Worker algo-1 finished training.\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:37:32 WARNING 140466583250752] wait_for_all_workers will not sync workers since the kv store is not running distributed\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:37:32 INFO 140466583250752] All workers finished. Serializing model for prediction.\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"get_graph.time\": {\"count\": 1, \"max\": 80.45196533203125, \"sum\": 80.45196533203125, \"min\": 80.45196533203125}}, \"EndTime\": 1593887852.403715, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593887852.322198}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:37:32 INFO 140466583250752] Number of GPUs being used: 1\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"finalize.time\": {\"count\": 1, \"max\": 115.81802368164062, \"sum\": 115.81802368164062, \"min\": 115.81802368164062}}, \"EndTime\": 1593887852.439037, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593887852.403797}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:37:32 INFO 140466583250752] Serializing to /opt/ml/model/model_algo-1\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:37:32 INFO 140466583250752] Saved checkpoint to \"/opt/ml/model/model_algo-1-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"model.serialize.time\": {\"count\": 1, \"max\": 4.108190536499023, \"sum\": 4.108190536499023, \"min\": 4.108190536499023}}, \"EndTime\": 1593887852.443277, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593887852.439125}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:37:32 INFO 140466583250752] Successfully serialized the model for prediction.\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:37:32 INFO 140466583250752] Evaluating model accuracy on testset using 100 samples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"model.bind.time\": {\"count\": 1, \"max\": 0.04601478576660156, \"sum\": 0.04601478576660156, \"min\": 0.04601478576660156}}, \"EndTime\": 1593887852.444133, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593887852.443327}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"model.score.time\": {\"count\": 1, \"max\": 378.8580894470215, \"sum\": 378.8580894470215, \"min\": 378.8580894470215}}, \"EndTime\": 1593887852.82294, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593887852.444196}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:37:32 INFO 140466583250752] #test_score (algo-1, RMSE): 0.0393535601381\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:37:32 INFO 140466583250752] #test_score (algo-1, mean_absolute_QuantileLoss): 0.175435115231408\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:37:32 INFO 140466583250752] #test_score (algo-1, mean_wQuantileLoss): 0.03388766754274254\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:37:32 INFO 140466583250752] #test_score (algo-1, wQuantileLoss[0.1]): 0.006066528087783656\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:37:32 INFO 140466583250752] #test_score (algo-1, wQuantileLoss[0.2]): 0.009093723395309129\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:37:32 INFO 140466583250752] #test_score (algo-1, wQuantileLoss[0.3]): 0.024513066515561156\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:37:32 INFO 140466583250752] #test_score (algo-1, wQuantileLoss[0.4]): 0.03917886393255222\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:37:32 INFO 140466583250752] #test_score (algo-1, wQuantileLoss[0.5]): 0.048613030658090965\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:37:32 INFO 140466583250752] #test_score (algo-1, wQuantileLoss[0.6]): 0.054154967088589884\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:37:32 INFO 140466583250752] #test_score (algo-1, wQuantileLoss[0.7]): 0.053225228848122294\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:37:32 INFO 140466583250752] #test_score (algo-1, wQuantileLoss[0.8]): 0.04299787332871996\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:37:32 INFO 140466583250752] #test_score (algo-1, wQuantileLoss[0.9]): 0.027145726029953555\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:37:32 INFO 140466583250752] #quality_metric: host=algo-1, test mean_wQuantileLoss <loss>=0.0338876675427\u001b[0m\n",
      "\u001b[34m[07/04/2020 18:37:32 INFO 140466583250752] #quality_metric: host=algo-1, test RMSE <loss>=0.0393535601381\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"totaltime\": {\"count\": 1, \"max\": 10459023.447036743, \"sum\": 10459023.447036743, \"min\": 10459023.447036743}, \"setuptime\": {\"count\": 1, \"max\": 10.048151016235352, \"sum\": 10.048151016235352, \"min\": 10.048151016235352}}, \"EndTime\": 1593887852.836699, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1593887852.823013}\n",
      "\u001b[0m\n",
      "\n",
      "2020-07-04 18:37:40 Uploading - Uploading generated training model\n",
      "2020-07-04 18:37:40 Completed - Training job completed\n",
      "Training seconds: 10527\n",
      "Billable seconds: 10527\n"
     ]
    }
   ],
   "source": [
    "# Specify data channels\n",
    "data_channels = {\n",
    "    \"train\": 's3://{}/{}'.format(s3_bucket, train_channel),\n",
    "    \"test\": 's3://{}/{}'.format(s3_bucket, valid_channel)\n",
    "}\n",
    "\n",
    "estimator = sagemaker.estimator.Estimator(\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    image_name=image_name,\n",
    "    role=role,\n",
    "    train_instance_count=1,\n",
    "    train_instance_type='ml.p2.xlarge',\n",
    "    base_job_name='deepar-stock-pred-job',\n",
    "    output_path=s3_output_path\n",
    ")\n",
    "\n",
    "# Set hyperparameters\n",
    "estimator.set_hyperparameters(**hyperparameters)\n",
    "\n",
    "# Train the model\n",
    "estimator.fit(inputs=data_channels, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two elements from the output above are worth noting once training is done:\n",
    "- the final RMSE test loss obtained after 200 training epochs is 0.0393535601381, which gives the SD-RMSE of 1.36 below. This is not very good compared to the linear regression and kernel ridge models trained previously - respectively 0.126 and 0.159 for d+1\n",
    "\n",
    "\n",
    "- the learning phase took 3 hours. This is not really acceptable, as we want our training phase to be much quicker than this so that our model can return predictions for a new stock as fast as possible once training is over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3618174275914596"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the SD-RMSE from the output above\n",
    "0.0393535601381 / np.std(target_list[train_size : train_size+valid_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For both reasons above, DeepAR will be dropped and I will keep working on LinearRegression and KernelRidge models for next steps."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
